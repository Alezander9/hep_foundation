{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union, Any, Optional, Tuple\n",
    "import uuid\n",
    "import uproot\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from abc import ABC, abstractmethod\n",
    "from tensorflow import keras\n",
    "from qkeras import QDense, QActivation, quantized_bits, quantized_relu\n",
    "import platform\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRegistry:\n",
    "    \"\"\"\n",
    "    Enhanced central registry for managing ML experiments, models, and metrics\n",
    "    Tracks detailed dataset configurations and training metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, base_path: str):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.db_path = self.base_path / \"registry.db\"\n",
    "        self.model_store = self.base_path / \"model_store\"\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        self.model_store.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize database\n",
    "        self._initialize_db()\n",
    "        \n",
    "    def _initialize_db(self):\n",
    "        \"\"\"Create database tables if they don't exist\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            # Main experiments table\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS experiments (\n",
    "                    experiment_id TEXT PRIMARY KEY,\n",
    "                    timestamp DATETIME,\n",
    "                    name TEXT,\n",
    "                    description TEXT,\n",
    "                    status TEXT,\n",
    "                    environment_info JSON\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            # Dataset configuration table\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS dataset_configs (\n",
    "                    experiment_id TEXT PRIMARY KEY,\n",
    "                    run_numbers JSON,\n",
    "                    selections JSON,\n",
    "                    normalization_params JSON,\n",
    "                    train_fraction FLOAT,\n",
    "                    validation_fraction FLOAT,\n",
    "                    test_fraction FLOAT,\n",
    "                    batch_size INTEGER,\n",
    "                    shuffle_buffer INTEGER,\n",
    "                    data_quality_metrics JSON,\n",
    "                    FOREIGN KEY(experiment_id) REFERENCES experiments(experiment_id)\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            # Model configuration table\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS model_configs (\n",
    "                    experiment_id TEXT PRIMARY KEY,\n",
    "                    model_type TEXT,\n",
    "                    architecture JSON,\n",
    "                    hyperparameters JSON,\n",
    "                    FOREIGN KEY(experiment_id) REFERENCES experiments(experiment_id)\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            # Training configuration and results\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS training_info (\n",
    "                    experiment_id TEXT PRIMARY KEY,\n",
    "                    config JSON,\n",
    "                    start_time DATETIME,\n",
    "                    end_time DATETIME,\n",
    "                    epochs_completed INTEGER,\n",
    "                    training_history JSON,\n",
    "                    final_metrics JSON,\n",
    "                    hardware_metrics JSON,\n",
    "                    FOREIGN KEY(experiment_id) REFERENCES experiments(experiment_id)\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            # Checkpoints table\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS checkpoints (\n",
    "                    checkpoint_id TEXT PRIMARY KEY,\n",
    "                    experiment_id TEXT,\n",
    "                    name TEXT,\n",
    "                    timestamp DATETIME,\n",
    "                    metadata JSON,\n",
    "                    FOREIGN KEY(experiment_id) REFERENCES experiments(experiment_id)\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "    def register_experiment(self,\n",
    "                          name: str,\n",
    "                          dataset_config: dict,\n",
    "                          model_config: dict,\n",
    "                          training_config: dict,\n",
    "                          description: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        Register new experiment with enhanced configuration tracking\n",
    "        \n",
    "        Args:\n",
    "            name: Human readable experiment name\n",
    "            dataset_config: Dataset parameters including:\n",
    "                - run_numbers: List of ATLAS run numbers\n",
    "                - selections: Dictionary of applied selections\n",
    "                - normalization_params: Dictionary of normalization parameters\n",
    "                - train_fraction: Fraction of data for training\n",
    "                - validation_fraction: Fraction for validation\n",
    "                - test_fraction: Fraction for testing\n",
    "                - batch_size: Batch size used\n",
    "                - shuffle_buffer: Shuffle buffer size\n",
    "                - data_quality_metrics: Results of data validation\n",
    "            model_config: Model configuration including:\n",
    "                - model_type: Type of model (e.g., \"autoencoder\")\n",
    "                - architecture: Network architecture details\n",
    "                - hyperparameters: Model hyperparameters\n",
    "            training_config: Training parameters\n",
    "            description: Optional experiment description\n",
    "        \"\"\"\n",
    "        experiment_id = str(uuid.uuid4())\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        # Get environment info\n",
    "        environment_info = {\n",
    "            \"python_version\": platform.python_version(),\n",
    "            \"tensorflow_version\": tf.__version__,\n",
    "            \"platform\": platform.platform(),\n",
    "            \"cpu_count\": os.cpu_count()\n",
    "        }\n",
    "        try:\n",
    "            environment_info[\"gpu_devices\"] = tf.config.list_physical_devices('GPU')\n",
    "        except:\n",
    "            environment_info[\"gpu_devices\"] = []\n",
    "            \n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            # Insert main experiment info\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO experiments \n",
    "                (experiment_id, timestamp, name, description, status, environment_info)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    experiment_id,\n",
    "                    timestamp,\n",
    "                    name,\n",
    "                    description,\n",
    "                    \"registered\",\n",
    "                    json.dumps(environment_info)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Insert dataset configuration\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO dataset_configs\n",
    "                (experiment_id, run_numbers, selections, normalization_params,\n",
    "                 train_fraction, validation_fraction, test_fraction,\n",
    "                 batch_size, shuffle_buffer, data_quality_metrics)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    experiment_id,\n",
    "                    json.dumps(dataset_config['run_numbers']),\n",
    "                    json.dumps(dataset_config['selections']),\n",
    "                    json.dumps(dataset_config.get('normalization_params', {})),\n",
    "                    dataset_config['train_fraction'],\n",
    "                    dataset_config['validation_fraction'],\n",
    "                    dataset_config['test_fraction'],\n",
    "                    dataset_config['batch_size'],\n",
    "                    dataset_config['shuffle_buffer'],\n",
    "                    json.dumps(dataset_config.get('data_quality_metrics', {}))\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Insert model configuration\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO model_configs\n",
    "                (experiment_id, model_type, architecture, hyperparameters)\n",
    "                VALUES (?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    experiment_id,\n",
    "                    model_config['model_type'],\n",
    "                    json.dumps(model_config['architecture']),\n",
    "                    json.dumps(model_config.get('hyperparameters', {}))\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Insert initial training info\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO training_info\n",
    "                (experiment_id, config, start_time, epochs_completed, training_history, final_metrics)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    experiment_id,\n",
    "                    json.dumps(training_config),\n",
    "                    None,\n",
    "                    0,\n",
    "                    \"{}\",\n",
    "                    \"{}\"\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Create experiment directory structure\n",
    "        exp_dir = self.model_store / experiment_id\n",
    "        exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save configurations as YAML for easy reading\n",
    "        configs_dir = exp_dir / \"configs\"\n",
    "        configs_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        with open(configs_dir / \"dataset_config.yaml\", 'w') as f:\n",
    "            yaml.dump(dataset_config, f)\n",
    "        with open(configs_dir / \"model_config.yaml\", 'w') as f:\n",
    "            yaml.dump(model_config, f)\n",
    "        with open(configs_dir / \"training_config.yaml\", 'w') as f:\n",
    "            yaml.dump(training_config, f)\n",
    "            \n",
    "        return experiment_id\n",
    "        \n",
    "    def update_training_progress(self,\n",
    "                               experiment_id: str,\n",
    "                               epoch: int,\n",
    "                               metrics: Dict[str, float],\n",
    "                               hardware_metrics: Optional[Dict] = None):\n",
    "        \"\"\"Update training progress and metrics\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            current = conn.execute(\n",
    "                \"SELECT training_history FROM training_info WHERE experiment_id = ?\",\n",
    "                (experiment_id,)\n",
    "            ).fetchone()\n",
    "            \n",
    "            if current is None:\n",
    "                raise ValueError(f\"No experiment found with id {experiment_id}\")\n",
    "                \n",
    "            history = json.loads(current[0])\n",
    "            \n",
    "            # Update history\n",
    "            if str(epoch) not in history:\n",
    "                history[str(epoch)] = {}\n",
    "            history[str(epoch)].update(metrics)\n",
    "            \n",
    "            # Update training info\n",
    "            updates = {\n",
    "                \"epochs_completed\": epoch,\n",
    "                \"training_history\": json.dumps(history)\n",
    "            }\n",
    "            \n",
    "            if hardware_metrics:\n",
    "                updates[\"hardware_metrics\"] = json.dumps(hardware_metrics)\n",
    "                \n",
    "            update_sql = \"UPDATE training_info SET \" + \\\n",
    "                        \", \".join(f\"{k} = ?\" for k in updates.keys()) + \\\n",
    "                        \" WHERE experiment_id = ?\"\n",
    "            \n",
    "            conn.execute(update_sql, list(updates.values()) + [experiment_id])\n",
    "            \n",
    "    def complete_training(self,\n",
    "                         experiment_id: str,\n",
    "                         final_metrics: Dict[str, float]):\n",
    "        \"\"\"Mark training as complete and save final metrics\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                UPDATE training_info \n",
    "                SET end_time = ?, final_metrics = ?\n",
    "                WHERE experiment_id = ?\n",
    "                \"\"\",\n",
    "                (datetime.now(), json.dumps(final_metrics), experiment_id)\n",
    "            )\n",
    "            \n",
    "            conn.execute(\n",
    "                \"UPDATE experiments SET status = ? WHERE experiment_id = ?\",\n",
    "                (\"completed\", experiment_id)\n",
    "            )\n",
    "\n",
    "    def save_checkpoint(self, \n",
    "                       experiment_id: str,\n",
    "                       models: Dict[str, Any],\n",
    "                       checkpoint_name: str = \"latest\",\n",
    "                       metadata: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Save model checkpoints for an experiment\n",
    "        \n",
    "        Args:\n",
    "            experiment_id: Experiment identifier\n",
    "            models: Dictionary of named models to save\n",
    "            checkpoint_name: Name for this checkpoint\n",
    "            metadata: Optional metadata about the checkpoint\n",
    "        \"\"\"\n",
    "        exp_dir = self.model_store / experiment_id\n",
    "        checkpoint_dir = exp_dir / \"checkpoints\" / checkpoint_name\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        checkpoint_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Save each model\n",
    "        for name, model in models.items():\n",
    "            model_path = checkpoint_dir / name\n",
    "            model.save(model_path)\n",
    "            \n",
    "        # Save checkpoint metadata\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        metadata.update({\n",
    "            \"saved_models\": list(models.keys()),\n",
    "            \"checkpoint_path\": str(checkpoint_dir)\n",
    "        })\n",
    "            \n",
    "        # Record checkpoint in database\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO checkpoints\n",
    "                (checkpoint_id, experiment_id, name, timestamp, metadata)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    checkpoint_id,\n",
    "                    experiment_id,\n",
    "                    checkpoint_name,\n",
    "                    datetime.now(),\n",
    "                    json.dumps(metadata)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Update experiment status\n",
    "            conn.execute(\n",
    "                \"UPDATE experiments SET status = ? WHERE experiment_id = ?\",\n",
    "                (\"checkpoint_saved\", experiment_id)\n",
    "            )\n",
    "            \n",
    "    def load_checkpoint(self, \n",
    "                       experiment_id: str,\n",
    "                       checkpoint_name: str = \"latest\") -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Get paths to saved model checkpoints\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of model names to their saved paths\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            result = conn.execute(\n",
    "                \"\"\"\n",
    "                SELECT metadata FROM checkpoints \n",
    "                WHERE experiment_id = ? AND name = ?\n",
    "                ORDER BY timestamp DESC LIMIT 1\n",
    "                \"\"\",\n",
    "                (experiment_id, checkpoint_name)\n",
    "            ).fetchone()\n",
    "            \n",
    "        if result is None:\n",
    "            raise ValueError(\n",
    "                f\"No checkpoint '{checkpoint_name}' found for experiment {experiment_id}\"\n",
    "            )\n",
    "            \n",
    "        metadata = json.loads(result[0])\n",
    "        checkpoint_path = Path(metadata[\"checkpoint_path\"])\n",
    "        \n",
    "        if not checkpoint_path.exists():\n",
    "            raise ValueError(f\"Checkpoint directory not found: {checkpoint_path}\")\n",
    "            \n",
    "        return {\n",
    "            model_name: str(checkpoint_path / model_name)\n",
    "            for model_name in metadata[\"saved_models\"]\n",
    "            if (checkpoint_path / model_name).exists()\n",
    "        }\n",
    "\n",
    "    def query_experiments(self, \n",
    "                         filters: Optional[Dict] = None,\n",
    "                         metrics: Optional[List[str]] = None,\n",
    "                         sort_by: Optional[str] = None,\n",
    "                         ascending: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query experiments with enhanced filtering and sorting\n",
    "        \n",
    "        Args:\n",
    "            filters: Dictionary of column:value pairs to filter on\n",
    "            metrics: List of specific metrics to include\n",
    "            sort_by: Column to sort by\n",
    "            ascending: Sort order\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame of matching experiments\n",
    "        \"\"\"\n",
    "        # Build base query joining all tables\n",
    "        query = \"\"\"\n",
    "        SELECT e.*, dc.*, mc.*, ti.* \n",
    "        FROM experiments e\n",
    "        LEFT JOIN dataset_configs dc ON e.experiment_id = dc.experiment_id\n",
    "        LEFT JOIN model_configs mc ON e.experiment_id = mc.experiment_id\n",
    "        LEFT JOIN training_info ti ON e.experiment_id = ti.experiment_id\n",
    "        \"\"\"\n",
    "        \n",
    "        params = []\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            for column, value in filters.items():\n",
    "                table = {\n",
    "                    'name': 'e',\n",
    "                    'status': 'e',\n",
    "                    'model_type': 'mc',\n",
    "                    'batch_size': 'dc'\n",
    "                }.get(column.split('.')[0], 'e')\n",
    "                \n",
    "                conditions.append(f\"{table}.{column} = ?\")\n",
    "                params.append(value)\n",
    "                \n",
    "            if conditions:\n",
    "                query += \" WHERE \" + \" AND \".join(conditions)\n",
    "                \n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            df = pd.read_sql_query(query, conn, params=params)\n",
    "            \n",
    "        # Parse JSON columns\n",
    "        json_columns = {\n",
    "            'environment_info': 'experiments',\n",
    "            'run_numbers': 'dataset_configs',\n",
    "            'selections': 'dataset_configs',\n",
    "            'normalization_params': 'dataset_configs',\n",
    "            'data_quality_metrics': 'dataset_configs',\n",
    "            'architecture': 'model_configs',\n",
    "            'hyperparameters': 'model_configs',\n",
    "            'config': 'training_info',\n",
    "            'training_history': 'training_info',\n",
    "            'final_metrics': 'training_info',\n",
    "            'hardware_metrics': 'training_info'\n",
    "        }\n",
    "        \n",
    "        for col in json_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(lambda x: json.loads(x) if pd.notna(x) else {})\n",
    "        \n",
    "        # Extract specific metrics if requested\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                df[f\"metric_{metric}\"] = df['final_metrics'].apply(\n",
    "                    lambda x: x.get(metric, None) if isinstance(x, dict) else None\n",
    "                )\n",
    "                \n",
    "        # Sort if requested\n",
    "        if sort_by:\n",
    "            df = df.sort_values(sort_by, ascending=ascending)\n",
    "            \n",
    "        return df\n",
    "        \n",
    "    def delete_experiment(self, experiment_id: str):\n",
    "        \"\"\"Delete an experiment and all associated files\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            # Delete from all tables\n",
    "            for table in ['training_info', 'model_configs', 'dataset_configs', \n",
    "                         'checkpoints', 'experiments']:\n",
    "                conn.execute(f\"DELETE FROM {table} WHERE experiment_id = ?\", \n",
    "                           (experiment_id,))\n",
    "            \n",
    "        # Delete files\n",
    "        exp_dir = self.model_store / experiment_id\n",
    "        if exp_dir.exists():\n",
    "            shutil.rmtree(exp_dir)\n",
    "            \n",
    "    def get_experiment_details(self, experiment_id: str) -> dict:\n",
    "        \"\"\"Get complete experiment information including all configs and results\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            # Get all information from all tables\n",
    "            experiment = conn.execute(\n",
    "                \"SELECT * FROM experiments WHERE experiment_id = ?\",\n",
    "                (experiment_id,)\n",
    "            ).fetchone()\n",
    "            \n",
    "            if experiment is None:\n",
    "                raise ValueError(f\"No experiment found with id {experiment_id}\")\n",
    "                \n",
    "            dataset_config = conn.execute(\n",
    "                \"SELECT * FROM dataset_configs WHERE experiment_id = ?\",\n",
    "                (experiment_id,)\n",
    "            ).fetchone()\n",
    "            \n",
    "            model_config = conn.execute(\n",
    "                \"SELECT * FROM model_configs WHERE experiment_id = ?\",\n",
    "                (experiment_id,)\n",
    "            ).fetchone()\n",
    "            \n",
    "            training_info = conn.execute(\n",
    "                \"SELECT * FROM training_info WHERE experiment_id = ?\",\n",
    "                (experiment_id,)\n",
    "            ).fetchone()\n",
    "            \n",
    "            checkpoints = conn.execute(\n",
    "                \"SELECT * FROM checkpoints WHERE experiment_id = ?\",\n",
    "                (experiment_id,)\n",
    "            ).fetchall()\n",
    "            \n",
    "        # Combine all information\n",
    "        details = {\n",
    "            \"experiment_info\": dict(zip(\n",
    "                ['experiment_id', 'timestamp', 'name', 'description', 'status', 'environment_info'],\n",
    "                experiment\n",
    "            )),\n",
    "            \"dataset_config\": dict(zip(\n",
    "                ['experiment_id', 'run_numbers', 'selections', 'normalization_params',\n",
    "                 'train_fraction', 'validation_fraction', 'test_fraction',\n",
    "                 'batch_size', 'shuffle_buffer', 'data_quality_metrics'],\n",
    "                dataset_config\n",
    "            )),\n",
    "            \"model_config\": dict(zip(\n",
    "                ['experiment_id', 'model_type', 'architecture', 'hyperparameters'],\n",
    "                model_config\n",
    "            )),\n",
    "            \"training_info\": dict(zip(\n",
    "                ['experiment_id', 'config', 'start_time', 'end_time', \n",
    "                 'epochs_completed', 'training_history', 'final_metrics', 'hardware_metrics'],\n",
    "                training_info\n",
    "            )),\n",
    "            \"checkpoints\": [\n",
    "                dict(zip(\n",
    "                    ['checkpoint_id', 'experiment_id', 'name', 'timestamp', 'metadata'],\n",
    "                    checkpoint\n",
    "                ))\n",
    "                for checkpoint in checkpoints\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Parse JSON fields\n",
    "        details['experiment_info']['environment_info'] = json.loads(details['experiment_info']['environment_info'])\n",
    "        details['dataset_config']['run_numbers'] = json.loads(details['dataset_config']['run_numbers'])\n",
    "        details['dataset_config']['selections'] = json.loads(details['dataset_config']['selections'])\n",
    "        details['dataset_config']['normalization_params'] = json.loads(details['dataset_config']['normalization_params'])\n",
    "        details['dataset_config']['data_quality_metrics'] = json.loads(details['dataset_config']['data_quality_metrics'])\n",
    "        details['model_config']['architecture'] = json.loads(details['model_config']['architecture'])\n",
    "        details['model_config']['hyperparameters'] = json.loads(details['model_config']['hyperparameters'])\n",
    "        details['training_info']['config'] = json.loads(details['training_info']['config'])\n",
    "        details['training_info']['training_history'] = json.loads(details['training_info']['training_history'])\n",
    "        details['training_info']['final_metrics'] = json.loads(details['training_info']['final_metrics'])\n",
    "        if details['training_info']['hardware_metrics']:\n",
    "            details['training_info']['hardware_metrics'] = json.loads(details['training_info']['hardware_metrics'])\n",
    "        \n",
    "        for checkpoint in details['checkpoints']:\n",
    "            checkpoint['metadata'] = json.loads(checkpoint['metadata'])\n",
    "            \n",
    "        return details\n",
    "    \n",
    "    def get_performance_summary(self, experiment_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of model performance metrics\n",
    "        \n",
    "        Args:\n",
    "            experiment_id: Experiment identifier\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing performance metrics and training time\n",
    "        \"\"\"\n",
    "        experiment = self.get_experiment_details(experiment_id)\n",
    "        training_info = experiment['training_info']\n",
    "        \n",
    "        # Calculate training time\n",
    "        if training_info['start_time'] and training_info['end_time']:\n",
    "            start = datetime.fromisoformat(training_info['start_time'])\n",
    "            end = datetime.fromisoformat(training_info['end_time'])\n",
    "            training_duration = (end - start).total_seconds()\n",
    "        else:\n",
    "            training_duration = None\n",
    "            \n",
    "        # Get final metrics\n",
    "        final_metrics = training_info['final_metrics']\n",
    "        \n",
    "        # Get training history progression\n",
    "        history = training_info['training_history']\n",
    "        metric_progression = {\n",
    "            metric: [epoch_data.get(metric) for epoch_data in history.values()]\n",
    "            for metric in set().union(*[epoch_data.keys() for epoch_data in history.values()])\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'training_duration': training_duration,\n",
    "            'epochs_completed': training_info['epochs_completed'],\n",
    "            'final_metrics': final_metrics,\n",
    "            'metric_progression': metric_progression,\n",
    "            'hardware_metrics': training_info.get('hardware_metrics', {})\n",
    "        }\n",
    "\n",
    "    def compare_experiments(self, \n",
    "                          experiment_ids: List[str],\n",
    "                          metrics: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Compare multiple experiments\n",
    "        \n",
    "        Args:\n",
    "            experiment_ids: List of experiment IDs to compare\n",
    "            metrics: Optional list of specific metrics to compare\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with experiment comparison\n",
    "        \"\"\"\n",
    "        experiments = []\n",
    "        for exp_id in experiment_ids:\n",
    "            details = self.get_experiment_details(exp_id)\n",
    "            exp_summary = {\n",
    "                'experiment_id': exp_id,\n",
    "                'name': details['experiment_info']['name'],\n",
    "                'status': details['experiment_info']['status'],\n",
    "                'model_type': details['model_config']['model_type'],\n",
    "                'run_numbers': details['dataset_config']['run_numbers'],\n",
    "                'batch_size': details['dataset_config']['batch_size'],\n",
    "                'epochs_completed': details['training_info']['epochs_completed']\n",
    "            }\n",
    "            \n",
    "            # Add final metrics\n",
    "            if metrics:\n",
    "                for metric in metrics:\n",
    "                    exp_summary[f'final_{metric}'] = details['training_info']['final_metrics'].get(metric)\n",
    "                    \n",
    "            # Add training duration if available\n",
    "            if details['training_info']['start_time'] and details['training_info']['end_time']:\n",
    "                start = datetime.fromisoformat(details['training_info']['start_time'])\n",
    "                end = datetime.fromisoformat(details['training_info']['end_time'])\n",
    "                exp_summary['training_duration'] = (end - start).total_seconds()\n",
    "                \n",
    "            experiments.append(exp_summary)\n",
    "            \n",
    "        return pd.DataFrame(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATLASDataManager:\n",
    "    \"\"\"Manages ATLAS PHYSLITE data access\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str = \"atlas_data\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_url = \"https://opendata.cern.ch/record/80001/files\"\n",
    "        self._setup_directories()\n",
    "    \n",
    "    def _setup_directories(self):\n",
    "        \"\"\"Create necessary directory structure\"\"\"\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        (self.base_dir / \"catalogs\").mkdir(exist_ok=True)\n",
    "    \n",
    "    def _download_file(self, url: str, output_path: Path, desc: str = None) -> bool:\n",
    "        \"\"\"Download a single file if it doesn't exist\"\"\"\n",
    "        if output_path.exists():\n",
    "            return False\n",
    "        \n",
    "        print(f\"Downloading file: {url}\")    \n",
    "        response = requests.get(f\"https://opendata.cern.ch{url}\", stream=True)\n",
    "        if response.status_code == 200:\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(output_path, 'wb') as f, tqdm(\n",
    "                desc=desc,\n",
    "                total=total_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    size = f.write(data)\n",
    "                    pbar.update(size)\n",
    "            return True\n",
    "        else:\n",
    "            raise Exception(f\"Download failed with status code: {response.status_code}\")\n",
    "    \n",
    "    def download_run_catalog(self, run_number: str, index: int = 0) -> Path:\n",
    "        \"\"\"\n",
    "        Download a specific run catalog file.\n",
    "        \n",
    "        Args:\n",
    "            run_number: ATLAS run number\n",
    "            index: Catalog index\n",
    "            \n",
    "        Returns:\n",
    "            Path to the downloaded catalog file\n",
    "        \"\"\"\n",
    "        padded_run = run_number.zfill(8)  # Ensure run number has leading zeros\n",
    "        url = f\"/record/80001/files/data16_13TeV_Run_{padded_run}_file_index.json_{index}\"\n",
    "        output_path = self.base_dir / \"catalogs\" / f\"Run_{run_number}_catalog_{index}.root\"\n",
    "        \n",
    "        self._download_file(url, output_path, f\"Downloading catalog {index} for Run {run_number}\")\n",
    "        return output_path\n",
    "    \n",
    "    def get_run_catalog_path(self, run_number: str, index: int = 0) -> Path:\n",
    "        \"\"\"Get path to a run catalog file\"\"\"\n",
    "        return self.base_dir / \"catalogs\" / f\"Run_{run_number}_catalog_{index}.root\"\n",
    "    \n",
    "    def verify_catalog_file(self, file_path: Path) -> bool:\n",
    "        \"\"\"Verify that a catalog file contains valid track data\"\"\"\n",
    "        try:\n",
    "            with uproot.open(file_path) as file:\n",
    "                if \"CollectionTree;1\" not in file:\n",
    "                    print(f\"No CollectionTree found in {file_path}\")\n",
    "                    return False\n",
    "                \n",
    "                tree = file[\"CollectionTree;1\"]\n",
    "                required_branches = [\n",
    "                    \"InDetTrackParticlesAuxDyn.d0\",\n",
    "                    \"InDetTrackParticlesAuxDyn.z0\",\n",
    "                    \"InDetTrackParticlesAuxDyn.phi\",\n",
    "                    \"InDetTrackParticlesAuxDyn.theta\",\n",
    "                    \"InDetTrackParticlesAuxDyn.qOverP\"\n",
    "                ]\n",
    "                \n",
    "                for branch in required_branches:\n",
    "                    if branch not in tree:\n",
    "                        print(f\"Missing required branch: {branch}\")\n",
    "                        return False\n",
    "                \n",
    "                # Try reading some data\n",
    "                try:\n",
    "                    data = tree[\"InDetTrackParticlesAuxDyn.d0\"].array(library=\"np\")\n",
    "                    if len(data) == 0:\n",
    "                        print(f\"No events found in {file_path}\")\n",
    "                        return False\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading data: {str(e)}\")\n",
    "                    return False\n",
    "                \n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error verifying file {file_path}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get statistics about downloaded catalog files.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing catalog statistics\n",
    "        \"\"\"\n",
    "        catalog_dir = self.base_dir / \"catalogs\"\n",
    "        catalog_files = list(catalog_dir.glob(\"*.root\"))\n",
    "        \n",
    "        total_events = 0\n",
    "        total_tracks = 0\n",
    "        \n",
    "        # Calculate events and tracks from catalogs\n",
    "        for catalog in catalog_files:\n",
    "            try:\n",
    "                with uproot.open(catalog) as file:\n",
    "                    if \"CollectionTree;1\" in file:\n",
    "                        tree = file[\"CollectionTree;1\"]\n",
    "                        total_events += len(tree)\n",
    "                        \n",
    "                        # Get track count from d0 branch\n",
    "                        try:\n",
    "                            tracks = tree[\"InDetTrackParticlesAuxDyn.d0\"].array(library=\"np\")\n",
    "                            total_tracks += sum(len(t) for t in tracks)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read stats from {catalog}: {e}\")\n",
    "        \n",
    "        stats = {\n",
    "            \"catalogs\": len(catalog_files),\n",
    "            \"total_events\": total_events,\n",
    "            \"total_tracks\": total_tracks,\n",
    "            \"total_size\": sum(f.stat().st_size for f in catalog_files) / (1024 * 1024 * 1024)  # in GB\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def print_status(self):\n",
    "        \"\"\"Print current status of catalogs\"\"\"\n",
    "        stats = self.get_stats()\n",
    "        print(f\"=== ATLAS Data Status ===\")\n",
    "        print(f\"Number of catalogs: {stats['catalogs']}\")\n",
    "        print(f\"Total events: {stats['total_events']:,}\")\n",
    "        print(f\"Total tracks: {stats['total_tracks']:,}\")\n",
    "        print(f\"Total data size: {stats['total_size']:.2f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedDatasetManager:\n",
    "    \"\"\"Manages dataset creation from ATLAS PHYSLITE data\"\"\"\n",
    "    def __init__(self, \n",
    "                 run_numbers: List[str],\n",
    "                 base_dir: str = \"atlas_data\",\n",
    "                 batch_size: int = 1000,\n",
    "                 cache_size: Optional[int] = None,\n",
    "                 shuffle_buffer: int = 10000,\n",
    "                 validation_fraction: float = 0.2):\n",
    "        self.run_numbers = run_numbers\n",
    "        self.batch_size = batch_size\n",
    "        self.cache_size = cache_size\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.atlas_manager = ATLASDataManager(base_dir)\n",
    "        self.normalization_params = None\n",
    "        \n",
    "    def ensure_data_available(self):\n",
    "        \"\"\"Download and verify all necessary catalog files\"\"\"\n",
    "        catalog_paths = []\n",
    "        for run_number in self.run_numbers:\n",
    "            catalog_path = self.atlas_manager.download_run_catalog(run_number)\n",
    "            if not self.atlas_manager.verify_catalog_file(catalog_path):\n",
    "                raise ValueError(f\"Invalid catalog file for run {run_number}\")\n",
    "            catalog_paths.append(catalog_path)\n",
    "        return catalog_paths\n",
    "    \n",
    "    def _flatten_and_clean_track_arrays(self, arrays) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Flatten and clean track parameter arrays\n",
    "        \n",
    "        Args:\n",
    "            arrays: List or array of track parameters\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned numpy array\n",
    "        \"\"\"\n",
    "        # Convert to numpy array if not already\n",
    "        if not isinstance(arrays, np.ndarray):\n",
    "            arrays = np.array(arrays)\n",
    "        \n",
    "        # Handle empty arrays\n",
    "        if len(arrays) == 0:\n",
    "            return np.array([], dtype=np.float64)\n",
    "        \n",
    "        # Flatten if array is jagged\n",
    "        if isinstance(arrays[0], (list, np.ndarray)):\n",
    "            flattened = np.concatenate([arr for arr in arrays if len(arr) > 0])\n",
    "        else:\n",
    "            flattened = arrays\n",
    "            \n",
    "        # Convert to float64 and remove any invalid values\n",
    "        clean = np.array(flattened, dtype=np.float64)\n",
    "        mask = ~(np.isnan(clean) | np.isinf(clean))\n",
    "        return clean[mask]\n",
    "    \n",
    "    def _read_catalog_data(self, catalog_path: Path) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Read track data from a catalog file\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nReading catalog: {catalog_path}\")\n",
    "            with uproot.open(catalog_path) as file:\n",
    "                tree = file[\"CollectionTree;1\"]\n",
    "                track_data = {}\n",
    "                \n",
    "                # Read track parameters\n",
    "                track_branches = [\n",
    "                    \"InDetTrackParticlesAuxDyn.d0\",\n",
    "                    \"InDetTrackParticlesAuxDyn.z0\",\n",
    "                    \"InDetTrackParticlesAuxDyn.phi\",\n",
    "                    \"InDetTrackParticlesAuxDyn.theta\",\n",
    "                    \"InDetTrackParticlesAuxDyn.qOverP\",\n",
    "                    \"InDetTrackParticlesAuxDyn.chiSquared\",\n",
    "                    \"InDetTrackParticlesAuxDyn.numberDoF\"\n",
    "                ]\n",
    "                \n",
    "                for branch in track_branches:\n",
    "                    param_name = branch.split('.')[-1]\n",
    "                    print(f\"Reading {param_name}...\")\n",
    "                    try:\n",
    "                        arrays = tree[branch].array(library=\"np\")\n",
    "                        track_data[param_name] = self._flatten_and_clean_track_arrays(arrays)\n",
    "                        print(f\"  Shape: {track_data[param_name].shape}, \"\n",
    "                            f\"dtype: {track_data[param_name].dtype}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {param_name}: {str(e)}\")\n",
    "                        return None\n",
    "                    \n",
    "                return track_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading catalog {catalog_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def create_raw_dataset(self) -> np.ndarray:\n",
    "        \"\"\"Create raw feature array from ATLAS data\"\"\"\n",
    "        catalog_paths = self.ensure_data_available()\n",
    "        all_track_data = []\n",
    "        \n",
    "        print(\"\\nReading catalog files...\")\n",
    "        for catalog_path in catalog_paths:\n",
    "            print(f\"Processing {catalog_path}\")\n",
    "            track_data = self._read_catalog_data(catalog_path)\n",
    "            if track_data:\n",
    "                all_track_data.append(track_data)\n",
    "        \n",
    "        if not all_track_data:\n",
    "            raise ValueError(\"No valid track data found in catalogs\")\n",
    "            \n",
    "        print(\"\\nCombining track data...\")\n",
    "        combined_data = {\n",
    "            key: np.concatenate([data[key] for data in all_track_data])\n",
    "            for key in all_track_data[0].keys()\n",
    "        }\n",
    "        \n",
    "        print(\"\\nCalculating derived quantities...\")\n",
    "        features = self._calculate_derived_quantities(combined_data)\n",
    "        print(f\"Raw feature array shape: {features.shape}\")\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _calculate_derived_quantities(self, data: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Calculate derived quantities from raw track parameters\"\"\"\n",
    "        # Convert to float64 for precision\n",
    "        qOverP = data['qOverP'].astype(np.float64)\n",
    "        theta = data['theta'].astype(np.float64)\n",
    "        \n",
    "        # Calculate pT (transverse momentum)\n",
    "        print(\"Calculating pT...\")\n",
    "        pT = np.abs(1.0 / (qOverP * 1000)) * np.sin(theta)\n",
    "        \n",
    "        # Calculate eta (pseudorapidity)\n",
    "        print(\"Calculating eta...\")\n",
    "        eta = -np.log(np.tan(theta / 2))\n",
    "        \n",
    "        # Calculate chi2/ndof\n",
    "        print(\"Calculating chi2/ndof...\")\n",
    "        chi2_per_ndof = (data['chiSquared'].astype(np.float64) / \n",
    "                        data['numberDoF'].astype(np.float64))\n",
    "        \n",
    "        # Stack features\n",
    "        features = np.stack([\n",
    "            pT,\n",
    "            eta,\n",
    "            data['phi'],\n",
    "            data['d0'],\n",
    "            data['z0'],\n",
    "            chi2_per_ndof\n",
    "        ], axis=1)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def apply_selections(self, \n",
    "                        features: np.ndarray,\n",
    "                        selections: Dict[str, Union[float, Tuple[float, float]]]) -> np.ndarray:\n",
    "        \"\"\"Apply physics selections to feature array\"\"\"\n",
    "        print(\"\\nApplying selections...\")\n",
    "        mask = np.ones(len(features), dtype=bool)\n",
    "        \n",
    "        feature_indices = {\n",
    "            'pt': 0,\n",
    "            'eta': 1,\n",
    "            'phi': 2,\n",
    "            'd0': 3,\n",
    "            'z0': 4,\n",
    "            'chi2_per_ndof': 5\n",
    "        }\n",
    "        \n",
    "        for feature, selection in selections.items():\n",
    "            if feature not in feature_indices:\n",
    "                continue\n",
    "                \n",
    "            idx = feature_indices[feature]\n",
    "            if isinstance(selection, tuple):\n",
    "                min_val, max_val = selection\n",
    "                if min_val is not None:\n",
    "                    mask &= (features[:, idx] >= min_val)\n",
    "                if max_val is not None:\n",
    "                    mask &= (features[:, idx] <= max_val)\n",
    "            else:\n",
    "                mask &= (features[:, idx] >= selection)\n",
    "        \n",
    "        selected_features = features[mask]\n",
    "        print(f\"Selected {len(selected_features)} / {len(features)} events\")\n",
    "        return selected_features\n",
    "    \n",
    "    def compute_normalization(self, features: np.ndarray) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Compute normalization parameters from training data\"\"\"\n",
    "        print(\"\\nComputing normalization parameters...\")\n",
    "        means = np.mean(features, axis=0)\n",
    "        stds = np.std(features, axis=0)\n",
    "        self.normalization_params = {'means': means, 'stds': stds}\n",
    "        return self.normalization_params\n",
    "    \n",
    "    def apply_normalization(self, features: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply normalization using computed parameters\"\"\"\n",
    "        if self.normalization_params is None:\n",
    "            raise ValueError(\"Normalization parameters not computed. Run compute_normalization first.\")\n",
    "        \n",
    "        normalized = (features - self.normalization_params['means']) / self.normalization_params['stds']\n",
    "        return normalized\n",
    "    \n",
    "    def split_dataset(self, \n",
    "                        features: np.ndarray,\n",
    "                        train_fraction: float = 0.8,\n",
    "                        validation_fraction: float = 0.1\n",
    "                        ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Split data into training, validation, and test sets\n",
    "        \n",
    "        Args:\n",
    "            features: Input feature array\n",
    "            train_fraction: Fraction of data for training (default: 0.8)\n",
    "            validation_fraction: Fraction of data for validation (default: 0.1)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (train_features, val_features, test_features)\n",
    "        \"\"\"\n",
    "        print(\"\\nSplitting into train/validation/test sets...\")\n",
    "        total_samples = len(features)\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        train_size = int(total_samples * train_fraction)\n",
    "        val_size = int(total_samples * validation_fraction)\n",
    "        \n",
    "        # Shuffle indices\n",
    "        indices = np.random.permutation(total_samples)\n",
    "        \n",
    "        # Split indices\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:train_size + val_size]\n",
    "        test_indices = indices[train_size + val_size:]\n",
    "        \n",
    "        # Create splits\n",
    "        train_features = features[train_indices]\n",
    "        val_features = features[val_indices]\n",
    "        test_features = features[test_indices]\n",
    "        \n",
    "        print(f\"Training samples: {len(train_features)}\")\n",
    "        print(f\"Validation samples: {len(val_features)}\")\n",
    "        print(f\"Test samples: {len(test_features)}\")\n",
    "        \n",
    "        return train_features, val_features, test_features\n",
    "    \n",
    "    def create_tf_dataset(self, \n",
    "                         features: np.ndarray, \n",
    "                         is_training: bool = True\n",
    "                         ) -> tf.data.Dataset:\n",
    "        \"\"\"Create TensorFlow dataset from feature array\"\"\"\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(features)\n",
    "        \n",
    "        if is_training:\n",
    "            if self.cache_size:\n",
    "                dataset = dataset.cache()\n",
    "            dataset = dataset.shuffle(self.shuffle_buffer)\n",
    "        \n",
    "        # Cast to float32 and create input=output pairs for autoencoder\n",
    "        dataset = dataset.map(\n",
    "            lambda x: (tf.cast(x, tf.float32), tf.cast(x, tf.float32))\n",
    "        )\n",
    "        \n",
    "        dataset = dataset.batch(self.batch_size, drop_remainder=True)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def create_datasets(self, \n",
    "                            selections: Optional[Dict] = None\n",
    "                            ) -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "        \"\"\"Create complete training, validation and test datasets with all preprocessing steps\"\"\"\n",
    "        # 1. Create raw dataset\n",
    "        features = self.create_raw_dataset()\n",
    "        \n",
    "        # 2. Validate raw data quality\n",
    "        print(\"\\nValidating raw data quality...\")\n",
    "        validation_results = self.validate_data_quality(features)\n",
    "        if validation_results['status'] == 'fail':\n",
    "            raise ValueError(\"Data quality validation failed\")\n",
    "        \n",
    "        # 3. Apply selections if provided\n",
    "        if selections:\n",
    "            features = self.apply_selections(features, selections)\n",
    "        \n",
    "        # 4. Split into train/validation/test\n",
    "        train_features, val_features, test_features = self.split_dataset(features)\n",
    "        \n",
    "        # 5. Compute normalization from training data only\n",
    "        self.compute_normalization(train_features)\n",
    "        \n",
    "        # 6. Apply normalization to all sets\n",
    "        train_features_norm = self.apply_normalization(train_features)\n",
    "        val_features_norm = self.apply_normalization(val_features)\n",
    "        test_features_norm = self.apply_normalization(test_features)\n",
    "        \n",
    "        # 7. Create final datasets\n",
    "        train_dataset = self.create_tf_dataset(train_features_norm, is_training=True)\n",
    "        val_dataset = self.create_tf_dataset(val_features_norm, is_training=False)\n",
    "        test_dataset = self.create_tf_dataset(test_features_norm, is_training=False)\n",
    "        \n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    def validate_data_quality(self, data) -> Dict:\n",
    "        \"\"\"Run basic data quality checks on either tf.data.Dataset or numpy array\n",
    "        \n",
    "        Args:\n",
    "            data: Either a tf.data.Dataset or numpy array to validate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing validation results and statistics\n",
    "        \"\"\"\n",
    "        feature_info = {\n",
    "            'pt': {'range': (0, 5000)},\n",
    "            'eta': {'range': (-5, 5)},\n",
    "            'phi': {'range': (-np.pi, np.pi)}, \n",
    "            'd0': {'range': (-10, 10)},\n",
    "            'z0': {'range': (-200, 200)},\n",
    "            'chi2_per_ndof': {'range': (0, 100)}\n",
    "        }\n",
    "        \n",
    "        validation_results = {\n",
    "            'total_tracks': 0,\n",
    "            'out_of_range_tracks': {name: 0 for name in feature_info},\n",
    "            'null_values': {name: 0 for name in feature_info},\n",
    "            'status': 'pass'\n",
    "        }\n",
    "\n",
    "        print(f\"\\nStarting data validation...\")\n",
    "        print(f\"Input type: {type(data)}\")\n",
    "        \n",
    "        # Handle both tf.data.Dataset and numpy array inputs\n",
    "        if isinstance(data, tf.data.Dataset):\n",
    "            print(\"Validating TensorFlow dataset...\")\n",
    "            for batch in data:\n",
    "                if isinstance(batch, tuple):  # (input, target) pairs\n",
    "                    batch = batch[0]  # Take input part\n",
    "                batch_np = batch.numpy()\n",
    "                validation_results['total_tracks'] += len(batch_np)\n",
    "                self._validate_batch(batch_np, validation_results, feature_info)\n",
    "        else:  # Assuming numpy array\n",
    "            print(f\"Validating numpy array with shape: {data.shape}\")\n",
    "            validation_results['total_tracks'] = len(data)\n",
    "            self._validate_batch(data, validation_results, feature_info)\n",
    "            \n",
    "        # Set status based on validation results\n",
    "        for name in feature_info:\n",
    "            if validation_results['null_values'][name] > 0:\n",
    "                print(f\"Found null values in {name}\")\n",
    "                validation_results['status'] = 'fail'\n",
    "                break\n",
    "                \n",
    "            # Allow a small fraction (0.1%) of tracks to be out of range\n",
    "            out_of_range_fraction = (validation_results['out_of_range_tracks'][name] / \n",
    "                                validation_results['total_tracks'])\n",
    "            if out_of_range_fraction > 0.001:\n",
    "                print(f\"Too many out of range values in {name}: {out_of_range_fraction:.3%}\")\n",
    "                validation_results['status'] = 'warning'\n",
    "        \n",
    "        # Add summary statistics\n",
    "        validation_results['summary'] = {\n",
    "            'null_fraction': {\n",
    "                name: count / validation_results['total_tracks']\n",
    "                for name, count in validation_results['null_values'].items()\n",
    "            },\n",
    "            'out_of_range_fraction': {\n",
    "                name: count / validation_results['total_tracks']\n",
    "                for name, count in validation_results['out_of_range_tracks'].items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"Validation complete. Status: {validation_results['status']}\")\n",
    "        return validation_results\n",
    "\n",
    "    def _validate_batch(self, batch_data: np.ndarray, validation_results: Dict, feature_info: Dict):\n",
    "        \"\"\"Helper method to validate a batch of data\n",
    "        \n",
    "        Args:\n",
    "            batch_data: Numpy array of shape (batch_size, n_features)\n",
    "            validation_results: Dictionary to store validation results\n",
    "            feature_info: Dictionary containing valid ranges for each feature\n",
    "        \"\"\"\n",
    "        for i, (name, info) in enumerate(feature_info.items()):\n",
    "            feature_data = batch_data[:, i]\n",
    "            \n",
    "            # Check for null/nan values\n",
    "            null_mask = np.isnan(feature_data) | np.isinf(feature_data)\n",
    "            n_nulls = np.sum(null_mask)\n",
    "            validation_results['null_values'][name] += n_nulls\n",
    "            if n_nulls > 0:\n",
    "                print(f\"Found {n_nulls} null/inf values in {name}\")\n",
    "            \n",
    "            # Check value ranges\n",
    "            if 'range' in info:\n",
    "                min_val, max_val = info['range']\n",
    "                range_mask = (feature_data < min_val) | (feature_data > max_val)\n",
    "                n_out_of_range = np.sum(range_mask)\n",
    "                validation_results['out_of_range_tracks'][name] += n_out_of_range\n",
    "                if n_out_of_range > 0:\n",
    "                    print(f\"Found {n_out_of_range} out-of-range values in {name}\")\n",
    "        \n",
    "    def get_data_status(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get status of catalog files and processing configuration\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing data and processing status\n",
    "        \"\"\"\n",
    "        # Get basic stats from atlas manager\n",
    "        stats = self.atlas_manager.get_stats()\n",
    "        \n",
    "        # Add processing configuration\n",
    "        stats.update({\n",
    "            \"runs\": len(self.run_numbers),\n",
    "            \"cache_size\": self.cache_size,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"shuffle_buffer\": self.shuffle_buffer,\n",
    "            \"run_numbers\": self.run_numbers\n",
    "        })\n",
    "        \n",
    "        # Add average tracks per event\n",
    "        if stats['total_events'] > 0:\n",
    "            stats['avg_tracks_per_event'] = stats['total_tracks'] / stats['total_events']\n",
    "        \n",
    "        # Add data quality metrics if a dataset has been created\n",
    "        try:\n",
    "            dataset = self.create_inner_track_dataset()\n",
    "            validation_results = self.validate_data_quality(dataset)\n",
    "            stats['data_quality'] = {\n",
    "                'status': validation_results['status'],\n",
    "                'null_fractions': validation_results['summary']['null_fraction'],\n",
    "                'out_of_range_fractions': validation_results['summary']['out_of_range_fraction']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            stats['data_quality'] = {'status': 'unknown', 'error': str(e)}\n",
    "        \n",
    "        return stats\n",
    "        \n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        \"\"\"Get list of feature names in order\"\"\"\n",
    "        return [\n",
    "            'pT',\n",
    "            'eta',\n",
    "            'phi',\n",
    "            'd0',\n",
    "            'z0',\n",
    "            'chi2_per_ndof'\n",
    "        ]\n",
    "    \n",
    "    def get_feature_info(self) -> Dict:\n",
    "        \"\"\"Get information about features\"\"\"\n",
    "        return {\n",
    "            'pT': {'units': 'GeV', 'range': (0, 5000), 'description': 'Transverse momentum'},\n",
    "            'eta': {'units': None, 'range': (-5, 5), 'description': 'Pseudorapidity'},\n",
    "            'phi': {'units': 'rad', 'range': (-np.pi, np.pi), 'description': 'Azimuthal angle'},\n",
    "            'd0': {'units': 'mm', 'range': (-10, 10), 'description': 'Transverse impact parameter'},\n",
    "            'z0': {'units': 'mm', 'range': (-200, 200), 'description': 'Longitudinal impact parameter'},\n",
    "            'chi2_per_ndof': {'units': None, 'range': (0, 100), 'description': 'Track fit quality'}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    \"\"\"Base class for all models\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "    @abstractmethod\n",
    "    def build(self, input_shape: tuple) -> None:\n",
    "        \"\"\"Build the model architecture\"\"\"\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"Get model configuration\"\"\"\n",
    "        pass\n",
    "        \n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save model weights and config\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built yet\")\n",
    "        self.model.save(path)\n",
    "        \n",
    "    def load(self, path: str) -> None:\n",
    "        \"\"\"Load model weights\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built yet\")\n",
    "        self.model.load_weights(path)\n",
    "\n",
    "class AutoEncoder(BaseModel):\n",
    "    \"\"\"Basic autoencoder with configurable architecture\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        latent_dim: int,\n",
    "        encoder_layers: List[int],\n",
    "        decoder_layers: List[int],\n",
    "        quant_bits: Optional[int] = None,\n",
    "        activation: str = 'relu',\n",
    "        name: str = 'autoencoder'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize autoencoder model\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input data\n",
    "            latent_dim: Dimension of latent space\n",
    "            encoder_layers: List of layer sizes for encoder\n",
    "            decoder_layers: List of layer sizes for decoder\n",
    "            quant_bits: Number of bits for quantization (optional)\n",
    "            activation: Activation function to use\n",
    "            name: Model name\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.decoder_layers = decoder_layers\n",
    "        self.quant_bits = quant_bits\n",
    "        self.activation = activation\n",
    "        self.name = name\n",
    "\n",
    "class AutoEncoder(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        latent_dim: int,\n",
    "        encoder_layers: List[int],\n",
    "        decoder_layers: List[int],\n",
    "        quant_bits: Optional[int] = None,\n",
    "        activation: str = 'relu',\n",
    "        name: str = 'autoencoder'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.decoder_layers = decoder_layers\n",
    "        self.quant_bits = quant_bits\n",
    "        self.activation = activation\n",
    "        self.name = name\n",
    "        \n",
    "    def build(self, input_shape: tuple = None) -> None:\n",
    "        \"\"\"Build encoder and decoder networks\"\"\"\n",
    "        if input_shape is None:\n",
    "            input_shape = (self.input_dim,)\n",
    "        \n",
    "        # Input layer\n",
    "        inputs = keras.Input(shape=input_shape, name='input_layer')\n",
    "        \n",
    "        # Create encoder layers\n",
    "        encoder_layers = []\n",
    "        for i, units in enumerate(self.encoder_layers):\n",
    "            if self.quant_bits:\n",
    "                # Dense layer\n",
    "                dense = QDense(\n",
    "                    units,\n",
    "                    kernel_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                    bias_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                    name=f'encoder_dense_{i}'\n",
    "                )\n",
    "                encoder_layers.append(dense)\n",
    "                \n",
    "                # Activation layer\n",
    "                activation = QActivation(\n",
    "                    quantized_relu(self.quant_bits),\n",
    "                    name=f'encoder_activation_{i}'\n",
    "                )\n",
    "                encoder_layers.append(activation)\n",
    "                \n",
    "                # Batch normalization\n",
    "                batch_norm = keras.layers.BatchNormalization(\n",
    "                    name=f'encoder_bn_{i}'\n",
    "                )\n",
    "                encoder_layers.append(batch_norm)\n",
    "            else:\n",
    "                dense = keras.layers.Dense(\n",
    "                    units,\n",
    "                    name=f'encoder_dense_{i}'\n",
    "                )\n",
    "                encoder_layers.append(dense)\n",
    "                \n",
    "                activation = keras.layers.Activation(\n",
    "                    self.activation,\n",
    "                    name=f'encoder_activation_{i}'\n",
    "                )\n",
    "                encoder_layers.append(activation)\n",
    "                \n",
    "                batch_norm = keras.layers.BatchNormalization(\n",
    "                    name=f'encoder_bn_{i}'\n",
    "                )\n",
    "                encoder_layers.append(batch_norm)\n",
    "        \n",
    "        # Create latent layer\n",
    "        if self.quant_bits:\n",
    "            latent_layer = QDense(\n",
    "                self.latent_dim,\n",
    "                kernel_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                bias_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                name='latent_layer'\n",
    "            )\n",
    "        else:\n",
    "            latent_layer = keras.layers.Dense(\n",
    "                self.latent_dim,\n",
    "                name='latent_layer'\n",
    "            )\n",
    "        \n",
    "        # Create decoder layers\n",
    "        decoder_layers = []\n",
    "        for i, units in enumerate(self.decoder_layers):\n",
    "            if self.quant_bits:\n",
    "                # Dense layer\n",
    "                dense = QDense(\n",
    "                    units,\n",
    "                    kernel_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                    bias_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                    name=f'decoder_dense_{i}'\n",
    "                )\n",
    "                decoder_layers.append(dense)\n",
    "                \n",
    "                # Activation layer\n",
    "                activation = QActivation(\n",
    "                    quantized_relu(self.quant_bits),\n",
    "                    name=f'decoder_activation_{i}'\n",
    "                )\n",
    "                decoder_layers.append(activation)\n",
    "                \n",
    "                # Batch normalization\n",
    "                batch_norm = keras.layers.BatchNormalization(\n",
    "                    name=f'decoder_bn_{i}'\n",
    "                )\n",
    "                decoder_layers.append(batch_norm)\n",
    "            else:\n",
    "                dense = keras.layers.Dense(\n",
    "                    units,\n",
    "                    name=f'decoder_dense_{i}'\n",
    "                )\n",
    "                decoder_layers.append(dense)\n",
    "                \n",
    "                activation = keras.layers.Activation(\n",
    "                    self.activation,\n",
    "                    name=f'decoder_activation_{i}'\n",
    "                )\n",
    "                decoder_layers.append(activation)\n",
    "                \n",
    "                batch_norm = keras.layers.BatchNormalization(\n",
    "                    name=f'decoder_bn_{i}'\n",
    "                )\n",
    "                decoder_layers.append(batch_norm)\n",
    "        \n",
    "        # Create output layer\n",
    "        if self.quant_bits:\n",
    "            output_layer = QDense(\n",
    "                self.input_dim,\n",
    "                kernel_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                bias_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                name='output_layer'\n",
    "            )\n",
    "        else:\n",
    "            output_layer = keras.layers.Dense(\n",
    "                self.input_dim,\n",
    "                name='output_layer'\n",
    "            )\n",
    "        \n",
    "        # Build the model by applying layers sequentially\n",
    "        # Encoder\n",
    "        x = inputs\n",
    "        for layer in encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Latent space\n",
    "        latent = latent_layer(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = latent\n",
    "        for layer in decoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Output\n",
    "        outputs = output_layer(x)\n",
    "        \n",
    "        # Create model\n",
    "        self.model = keras.Model(inputs=inputs, outputs=outputs, name=self.name)\n",
    "        \n",
    "        print(\"\\nModel layer structure:\")\n",
    "        for layer in self.model.layers:\n",
    "            print(f\"Layer: {layer.name}, Type: {type(layer)}\")\n",
    "        \n",
    "    def get_config(self) -> dict:\n",
    "        return {\n",
    "            \"model_type\": \"autoencoder\",\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"encoder_layers\": self.encoder_layers,\n",
    "            \"decoder_layers\": self.decoder_layers,\n",
    "            \"quant_bits\": self.quant_bits,\n",
    "            \"activation\": self.activation,\n",
    "            \"name\": self.name\n",
    "        }\n",
    "\n",
    "class ModelFactory:\n",
    "    \"\"\"Factory for creating different model types\"\"\"\n",
    "    @staticmethod\n",
    "    def create_model(model_type: str, config: dict) -> BaseModel:\n",
    "        if model_type == \"autoencoder\":\n",
    "            config_copy = config.copy()\n",
    "            config_copy.pop('model_type', None)  # Remove model_type key\n",
    "            return AutoEncoder(\n",
    "                input_dim=config['input_dim'],\n",
    "                latent_dim=config['latent_dim'],\n",
    "                encoder_layers=config['encoder_layers'],\n",
    "                decoder_layers=config['decoder_layers'],\n",
    "                quant_bits=config.get('quant_bits', None),\n",
    "                activation=config.get('activation', 'relu'),\n",
    "                name=config.get('name', 'autoencoder')\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "            \n",
    "    @staticmethod\n",
    "    def from_config(config: dict) -> BaseModel:\n",
    "        \"\"\"Create model from config dictionary\"\"\"\n",
    "        model_type = config.pop(\"model_type\")\n",
    "        return ModelFactory.create_model(model_type, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Handles model training and evaluation\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: BaseModel,\n",
    "        training_config: dict,\n",
    "        optimizer: Optional[tf.keras.optimizers.Optimizer] = None,\n",
    "        loss: Optional[tf.keras.losses.Loss] = None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.config = training_config\n",
    "        \n",
    "        # Set up training parameters\n",
    "        self.batch_size = training_config.get(\"batch_size\", 32)\n",
    "        self.epochs = training_config.get(\"epochs\", 10)\n",
    "        self.validation_split = training_config.get(\"validation_split\", 0.2)\n",
    "        \n",
    "        # Set up optimizer and loss\n",
    "        self.optimizer = optimizer or tf.keras.optimizers.Adam(\n",
    "            learning_rate=training_config.get(\"learning_rate\", 0.001)\n",
    "        )\n",
    "        self.loss = loss or tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        # Training history\n",
    "        self.history = None\n",
    "        \n",
    "    def compile_model(self):\n",
    "        \"\"\"Compile the model with optimizer and loss\"\"\"\n",
    "        if self.model.model is None:\n",
    "            raise ValueError(\"Model not built yet\")\n",
    "            \n",
    "        self.model.model.compile(\n",
    "            optimizer=self.optimizer,\n",
    "            loss=self.loss,\n",
    "            metrics=['mse'],\n",
    "            run_eagerly=True \n",
    "        )\n",
    "        \n",
    "    def train(\n",
    "        self,\n",
    "        dataset: tf.data.Dataset,\n",
    "        validation_data: Optional[tf.data.Dataset] = None,\n",
    "        callbacks: List[tf.keras.callbacks.Callback] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if self.model.model is None:\n",
    "            raise ValueError(\"Model not built yet\")\n",
    "        \n",
    "        # Remove the tuple unpacking\n",
    "        train_dataset = dataset\n",
    "        val_dataset = validation_data\n",
    "        \n",
    "        # Compile model\n",
    "        self.compile_model()\n",
    "        \n",
    "        # Setup callbacks\n",
    "        if callbacks is None:\n",
    "            callbacks = []\n",
    "            \n",
    "        # Train the model\n",
    "        self.history = self.model.model.fit(\n",
    "            train_dataset,\n",
    "            epochs=self.epochs,\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        return self.get_training_summary()\n",
    "        \n",
    "    def evaluate(\n",
    "        self,\n",
    "        dataset: tf.data.Dataset\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the model\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        if self.model.model is None:\n",
    "            raise ValueError(\"Model not built yet\")\n",
    "            \n",
    "        results = self.model.model.evaluate(dataset, return_dict=True)\n",
    "        return results\n",
    "        \n",
    "    def get_training_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of training results\"\"\"\n",
    "        if self.history is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "            \n",
    "        return {\n",
    "            \"training_config\": self.config,\n",
    "            \"final_loss\": float(self.history.history['loss'][-1]),\n",
    "            \"final_val_loss\": float(self.history.history['val_loss'][-1]),\n",
    "            \"history\": {\n",
    "                metric: [float(val) for val in values]\n",
    "                for metric, values in self.history.history.items()\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_model_pipeline():\n",
    "#     \"\"\"Test the complete model pipeline including factory and trainer\"\"\"\n",
    "    \n",
    "#     print(\"\\n=== Starting Model Pipeline Test ===\")\n",
    "#     print(f\"TensorFlow eager execution: {tf.executing_eagerly()}\")\n",
    "#     print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Step 1: Setup data manager\n",
    "#         print(\"\\nSetting up data manager...\")\n",
    "#         data_manager = IntegratedDatasetManager(\n",
    "#             run_numbers=[\"00296939\", \"00296942\"],\n",
    "#             batch_size=1000,\n",
    "#             cache_size=10000,\n",
    "#             validation_fraction=0.2\n",
    "#         )\n",
    "        \n",
    "#         # Step 2: Create datasets with selections\n",
    "#         print(\"\\nCreating datasets with selections...\")\n",
    "#         selections = {\n",
    "#             'eta': (-2.5, 2.5),\n",
    "#             'chi2_per_ndof': (0.0, 3.0)\n",
    "#         }\n",
    "        \n",
    "#         train_dataset, val_dataset, test_dataset = data_manager.create_datasets(selections=selections)   \n",
    "\n",
    "#         # Debug dataset structure\n",
    "#         print(\"\\nValidating dataset structure...\")\n",
    "#         for batch_x, batch_y in train_dataset.take(1):\n",
    "#             print(f\"Input batch shape: {batch_x.shape}\")\n",
    "#             print(f\"Output batch shape: {batch_y.shape}\")\n",
    "#             print(f\"Batch dtype: {batch_x.dtype}\")\n",
    "#             print(f\"Sample of first 5 elements (input):\")\n",
    "#             print(batch_x[:5].numpy())\n",
    "            \n",
    "#             # Check for NaN or Inf values\n",
    "#             has_nan = tf.reduce_any(tf.math.is_nan(batch_x))\n",
    "#             has_inf = tf.reduce_any(tf.math.is_inf(batch_x))\n",
    "#             print(f\"Contains NaN: {has_nan}\")\n",
    "#             print(f\"Contains Inf: {has_inf}\")\n",
    "\n",
    "#         # Count total elements\n",
    "#         print(\"\\nCounting dataset elements...\")\n",
    "#         train_elements = 0\n",
    "#         val_elements = 0\n",
    "#         for batch_x, _ in train_dataset:\n",
    "#             train_elements += batch_x.shape[0]\n",
    "#         for batch_x, _ in val_dataset:\n",
    "#             val_elements += batch_x.shape[0]\n",
    "\n",
    "#         print(f\"Training elements: {train_elements}\")\n",
    "#         print(f\"Validation elements: {val_elements}\")\n",
    "#         # Step 2: Test Model Factory\n",
    "#         print(\"\\nTesting Model Factory...\")\n",
    "        \n",
    "#         # Define model configuration\n",
    "#         model_config = {\n",
    "#             \"model_type\": \"autoencoder\",\n",
    "#             \"input_dim\": 6,  # [pT, eta, phi, d0, z0, chi2_per_ndof]\n",
    "#             \"latent_dim\": 3,\n",
    "#             \"encoder_layers\": [64, 32],\n",
    "#             \"decoder_layers\": [32, 64],\n",
    "#             \"quant_bits\": 8,\n",
    "#             \"activation\": \"relu\",\n",
    "#             \"name\": \"track_autoencoder\"\n",
    "#         }\n",
    "        \n",
    "#         print(\"\\nModel Configuration:\")\n",
    "#         for key, value in model_config.items():\n",
    "#             print(f\"  {key}: {value}\")\n",
    "        \n",
    "#         # Create model\n",
    "#         print(\"\\nCreating model...\")\n",
    "#         model = ModelFactory.create_model(\n",
    "#             model_type=\"autoencoder\",\n",
    "#             config=model_config\n",
    "#         )\n",
    "        \n",
    "#         # Build model\n",
    "#         print(\"Building model...\")\n",
    "#         model.build(input_shape=(6,))\n",
    "        \n",
    "#         # Print model summary\n",
    "#         print(\"\\nModel Summary:\")\n",
    "#         model.model.summary()\n",
    "        \n",
    "#         # Step 3: Test Model Trainer\n",
    "#         print(\"\\nTesting Model Trainer...\")\n",
    "        \n",
    "#         # Define training configuration\n",
    "#         training_config = {\n",
    "#             \"batch_size\": 1000,\n",
    "#             \"epochs\": 5,  # Reduced for testing\n",
    "#             \"learning_rate\": 0.001,\n",
    "#             \"validation_split\": 0.2,\n",
    "#             \"early_stopping\": {\n",
    "#                 \"patience\": 3,\n",
    "#                 \"min_delta\": 1e-4\n",
    "#             }\n",
    "#         }\n",
    "        \n",
    "#         print(\"\\nTraining Configuration:\")\n",
    "#         for key, value in training_config.items():\n",
    "#             print(f\"  {key}: {value}\")\n",
    "        \n",
    "#         # Create trainer\n",
    "#         trainer = ModelTrainer(\n",
    "#             model=model,\n",
    "#             training_config=training_config\n",
    "#         )\n",
    "        \n",
    "#         # Setup callbacks\n",
    "#         callbacks = [\n",
    "#             tf.keras.callbacks.EarlyStopping(\n",
    "#                 patience=training_config[\"early_stopping\"][\"patience\"],\n",
    "#                 min_delta=training_config[\"early_stopping\"][\"min_delta\"],\n",
    "#                 restore_best_weights=True\n",
    "#             )\n",
    "#         ]\n",
    "        \n",
    "#         # Train model\n",
    "#         print(\"\\nStarting training...\")\n",
    "#         training_results = trainer.train(\n",
    "#             dataset=train_dataset,\n",
    "#             validation_data=val_dataset,\n",
    "#             callbacks=callbacks\n",
    "#         )\n",
    "        \n",
    "#         # Print training results\n",
    "#         print(\"\\nTraining Results:\")\n",
    "#         print(f\"Final loss: {training_results['final_loss']:.6f}\")\n",
    "#         if 'final_val_loss' in training_results:\n",
    "#             print(f\"Final validation loss: {training_results['final_val_loss']:.6f}\")\n",
    "        \n",
    "#         # Step 4: Test Model Evaluation\n",
    "#         print(\"\\nTesting model evaluation...\")\n",
    "#         eval_results = trainer.evaluate(test_dataset)  # Use test dataset for evaluation\n",
    "\n",
    "#         print(\"\\nEvaluation Results:\")\n",
    "#         for metric, value in eval_results.items():\n",
    "#             print(f\"  {metric}: {value:.6f}\")\n",
    "        \n",
    "#         # Step 5: Test Model Saving/Loading\n",
    "#         print(\"\\nTesting model save/load...\")\n",
    "        \n",
    "#         # Create temporary directory for model\n",
    "#         save_dir = Path(\"test_models\")\n",
    "#         save_dir.mkdir(exist_ok=True)\n",
    "#         model_path = save_dir / \"test_autoencoder.h5\"\n",
    "        \n",
    "#         # Save model\n",
    "#         print(f\"Saving model to {model_path}\")\n",
    "#         model.save(str(model_path))\n",
    "        \n",
    "#         print(\"\\n=== Model Pipeline Test Completed Successfully ===\")\n",
    "#         return True\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n!!! Error during model pipeline test !!!\")\n",
    "#         print(f\"Error type: {type(e).__name__}\")\n",
    "#         print(f\"Error message: {str(e)}\")\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Model Pipeline Test ===\n",
      "TensorFlow eager execution: True\n",
      "TensorFlow version: 2.13.1\n",
      "\n",
      "Setting up data manager...\n",
      "\n",
      "Creating datasets with selections...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll tests passed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mtest_model_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreating datasets with selections...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m selections \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.5\u001b[39m, \u001b[38;5;241m2.5\u001b[39m),\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchi2_per_ndof\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m3.0\u001b[39m)\n\u001b[1;32m     23\u001b[0m }\n\u001b[0;32m---> 25\u001b[0m train_dataset, val_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdata_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselections\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Debug dataset structure\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValidating dataset structure...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 270\u001b[0m, in \u001b[0;36mIntegratedDatasetManager.create_datasets\u001b[0;34m(self, selections)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create complete training, validation and test datasets with all preprocessing steps\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# 1. Create raw dataset\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_raw_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# 2. Validate raw data quality\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValidating raw data quality...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 96\u001b[0m, in \u001b[0;36mIntegratedDatasetManager.create_raw_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_raw_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create raw feature array from ATLAS data\"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     catalog_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_data_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     all_track_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mReading catalog files...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36mIntegratedDatasetManager.ensure_data_available\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run_number \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_numbers:\n\u001b[1;32m     22\u001b[0m     catalog_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matlas_manager\u001b[38;5;241m.\u001b[39mdownload_run_catalog(run_number)\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matlas_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify_catalog_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatalog_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid catalog file for run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m     catalog_paths\u001b[38;5;241m.\u001b[39mappend(catalog_path)\n",
      "Cell \u001b[0;32mIn[3], line 68\u001b[0m, in \u001b[0;36mATLASDataManager.verify_catalog_file\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo CollectionTree found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCollectionTree;1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     69\u001b[0m required_branches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInDetTrackParticlesAuxDyn.d0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInDetTrackParticlesAuxDyn.z0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInDetTrackParticlesAuxDyn.qOverP\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m ]\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m branch \u001b[38;5;129;01min\u001b[39;00m required_branches:\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/reading.py:2116\u001b[0m, in \u001b[0;36mReadOnlyDirectory.__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step\n\u001b[1;32m   2115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/reading.py:2513\u001b[0m, in \u001b[0;36mReadOnlyKey.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2510\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mclass_named(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fClassName)\n\u001b[1;32m   2512\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2513\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselffile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m uproot\u001b[38;5;241m.\u001b[39mdeserialization\u001b[38;5;241m.\u001b[39mDeserializationError:\n\u001b[1;32m   2516\u001b[0m     breadcrumbs \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbreadcrumbs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/model.py:1362\u001b[0m, in \u001b[0;36mDispatchByVersion.read\u001b[0;34m(cls, chunk, cursor, context, file, selffile, parent, concrete)\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1355\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mUnknown version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassname_decode(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m that cannot be skipped \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"because its number of bytes is unknown.\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m             )\n\u001b[1;32m   1360\u001b[0m         \u001b[38;5;66;03m# versioned_cls.read starts with numbytes_version again because move=False (above)\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m         temp_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(\n\u001b[0;32m-> 1362\u001b[0m             \u001b[43mversioned_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m                \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselffile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1365\u001b[0m             chunk,\n\u001b[1;32m   1366\u001b[0m             cursor,\n\u001b[1;32m   1367\u001b[0m             context,\n\u001b[1;32m   1368\u001b[0m             file,\n\u001b[1;32m   1369\u001b[0m         )\n\u001b[1;32m   1371\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m forth_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1372\u001b[0m             forth_obj\u001b[38;5;241m.\u001b[39mpop_active_node()\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/model.py:854\u001b[0m, in \u001b[0;36mModel.read\u001b[0;34m(cls, chunk, cursor, context, file, selffile, parent, concrete)\u001b[0m\n\u001b[1;32m    852\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39madd_node(forth_stash)\n\u001b[1;32m    853\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39mpush_active_node(forth_stash)\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_members\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forth_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    856\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39mpop_active_node()\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/models/TTree.py:773\u001b[0m, in \u001b[0;36mModel_TTree_v20.read_members\u001b[0;34m(self, chunk, cursor, context, file)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfClusterSize\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    768\u001b[0m     chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmember(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfNClusterRange\u001b[39m\u001b[38;5;124m\"\u001b[39m), tmp, context\n\u001b[1;32m    769\u001b[0m )\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfIOFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mclass_named(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROOT::TIOFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    771\u001b[0m     chunk, cursor, context, file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcrete\n\u001b[1;32m    772\u001b[0m )\n\u001b[0;32m--> 773\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfBranches\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_named\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTObjArray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcrete\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfLeaves\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mclass_named(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTObjArray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    777\u001b[0m     chunk, cursor, context, file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcrete\n\u001b[1;32m    778\u001b[0m )\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfAliases\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m uproot\u001b[38;5;241m.\u001b[39mdeserialization\u001b[38;5;241m.\u001b[39mread_object_any(\n\u001b[1;32m    780\u001b[0m     chunk, cursor, context, file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcrete\n\u001b[1;32m    781\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/model.py:854\u001b[0m, in \u001b[0;36mModel.read\u001b[0;34m(cls, chunk, cursor, context, file, selffile, parent, concrete)\u001b[0m\n\u001b[1;32m    852\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39madd_node(forth_stash)\n\u001b[1;32m    853\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39mpush_active_node(forth_stash)\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_members\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forth_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    856\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39mpop_active_node()\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/models/TObjArray.py:57\u001b[0m, in \u001b[0;36mModel_TObjArray.read_members\u001b[0;34m(self, chunk, cursor, context, file)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfSize\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m---> 57\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[43muproot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_object_any\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mappend(item)\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/deserialization.py:312\u001b[0m, in \u001b[0;36mread_object_any\u001b[0;34m(chunk, cursor, context, file, selffile, parent, as_class)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DeserializationError(\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"invalid class-tag reference: {}\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m             file\u001b[38;5;241m.\u001b[39mfile_path,\n\u001b[1;32m    309\u001b[0m         )\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mrefs[ref]  \u001b[38;5;66;03m# reference class\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselffile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     obj \u001b[38;5;241m=\u001b[39m as_class\u001b[38;5;241m.\u001b[39mread(chunk, cursor, context, file, selffile, parent)\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/model.py:1362\u001b[0m, in \u001b[0;36mDispatchByVersion.read\u001b[0;34m(cls, chunk, cursor, context, file, selffile, parent, concrete)\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1355\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mUnknown version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassname_decode(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m that cannot be skipped \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"because its number of bytes is unknown.\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m             )\n\u001b[1;32m   1360\u001b[0m         \u001b[38;5;66;03m# versioned_cls.read starts with numbytes_version again because move=False (above)\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m         temp_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(\n\u001b[0;32m-> 1362\u001b[0m             \u001b[43mversioned_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m                \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselffile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1365\u001b[0m             chunk,\n\u001b[1;32m   1366\u001b[0m             cursor,\n\u001b[1;32m   1367\u001b[0m             context,\n\u001b[1;32m   1368\u001b[0m             file,\n\u001b[1;32m   1369\u001b[0m         )\n\u001b[1;32m   1371\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m forth_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1372\u001b[0m             forth_obj\u001b[38;5;241m.\u001b[39mpop_active_node()\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/model.py:854\u001b[0m, in \u001b[0;36mModel.read\u001b[0;34m(cls, chunk, cursor, context, file, selffile, parent, concrete)\u001b[0m\n\u001b[1;32m    852\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39madd_node(forth_stash)\n\u001b[1;32m    853\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39mpush_active_node(forth_stash)\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_members\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forth_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    856\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39mpop_active_node()\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/models/TBranch.py:754\u001b[0m, in \u001b[0;36mModel_TBranchElement_v10.read_members\u001b[0;34m(self, chunk, cursor, context, file)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_memberwise:\n\u001b[1;32m    749\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    750\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mmemberwise serialization of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;124min file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    752\u001b[0m             )\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bases\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 754\u001b[0m             \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_named\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTBranch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m                \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconcrete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m         )\n\u001b[1;32m    764\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor_baskets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bases[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_cursor_baskets\n\u001b[1;32m    765\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfClassName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mclass_named(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTString\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    766\u001b[0m             chunk, cursor, context, file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcrete\n\u001b[1;32m    767\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/model.py:818\u001b[0m, in \u001b[0;36mModel.read\u001b[0;34m(cls, chunk, cursor, context, file, selffile, parent, concrete)\u001b[0m\n\u001b[1;32m    816\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39madd_node(forth_stash)\n\u001b[1;32m    817\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39mpush_active_node(forth_stash)\n\u001b[0;32m--> 818\u001b[0m temp_var \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselffile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcrete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forth_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39mpop_active_node()\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/model.py:854\u001b[0m, in \u001b[0;36mModel.read\u001b[0;34m(cls, chunk, cursor, context, file, selffile, parent, concrete)\u001b[0m\n\u001b[1;32m    852\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39madd_node(forth_stash)\n\u001b[1;32m    853\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39mpush_active_node(forth_stash)\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_members\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forth_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    856\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39mpop_active_node()\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/models/TBranch.py:488\u001b[0m, in \u001b[0;36mModel_TBranch_v13.read_members\u001b[0;34m(self, chunk, cursor, context, file)\u001b[0m\n\u001b[1;32m    476\u001b[0m (\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfOffset\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfMaxBaskets\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfZipBytes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    484\u001b[0m ) \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfields(chunk, _tbranch13_format2, context)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfBranches\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mclass_named(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTObjArray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    486\u001b[0m     chunk, cursor, context, file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcrete\n\u001b[1;32m    487\u001b[0m )\n\u001b[0;32m--> 488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfLeaves\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_named\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTObjArray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcrete\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor_baskets \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimal_ttree_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/model.py:854\u001b[0m, in \u001b[0;36mModel.read\u001b[0;34m(cls, chunk, cursor, context, file, selffile, parent, concrete)\u001b[0m\n\u001b[1;32m    852\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39madd_node(forth_stash)\n\u001b[1;32m    853\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39mpush_active_node(forth_stash)\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_members\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forth_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    856\u001b[0m     forth_obj\u001b[38;5;241m.\u001b[39mpop_active_node()\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/models/TObjArray.py:57\u001b[0m, in \u001b[0;36mModel_TObjArray.read_members\u001b[0;34m(self, chunk, cursor, context, file)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_members[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfSize\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m---> 57\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[43muproot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_object_any\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mappend(item)\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/deserialization.py:312\u001b[0m, in \u001b[0;36mread_object_any\u001b[0;34m(chunk, cursor, context, file, selffile, parent, as_class)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DeserializationError(\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"invalid class-tag reference: {}\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m             file\u001b[38;5;241m.\u001b[39mfile_path,\n\u001b[1;32m    309\u001b[0m         )\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mrefs[ref]  \u001b[38;5;66;03m# reference class\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselffile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     obj \u001b[38;5;241m=\u001b[39m as_class\u001b[38;5;241m.\u001b[39mread(chunk, cursor, context, file, selffile, parent)\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/model.py:1328\u001b[0m, in \u001b[0;36mDispatchByVersion.read\u001b[0;34m(cls, chunk, cursor, context, file, selffile, parent, concrete)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;66;03m# Ignores context[\"reading\"], because otherwise, there would be nothing to do.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m start_index \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39m_index\n\u001b[1;32m   1324\u001b[0m (\n\u001b[1;32m   1325\u001b[0m     num_bytes,\n\u001b[1;32m   1326\u001b[0m     version,\n\u001b[1;32m   1327\u001b[0m     is_memberwise,\n\u001b[0;32m-> 1328\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43muproot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumbytes_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmove\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m versioned_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mclass_of_version(version)\n\u001b[1;32m   1332\u001b[0m forth_obj \u001b[38;5;241m=\u001b[39m uproot\u001b[38;5;241m.\u001b[39m_awkwardforth\u001b[38;5;241m.\u001b[39mget_forth_obj(context)\n",
      "File \u001b[0;32m~/miniconda3/envs/hep_foundation/lib/python3.9/site-packages/uproot/deserialization.py:142\u001b[0m, in \u001b[0;36mnumbytes_version\u001b[0;34m(chunk, cursor, context, move)\u001b[0m\n\u001b[1;32m    139\u001b[0m     num_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     version \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfield(chunk, _numbytes_version_2, context, move\u001b[38;5;241m=\u001b[39mmove)\n\u001b[0;32m--> 142\u001b[0m is_memberwise \u001b[38;5;241m=\u001b[39m \u001b[43mversion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muproot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkStreamedMemberWise\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_memberwise:\n\u001b[1;32m    144\u001b[0m     version \u001b[38;5;241m=\u001b[39m version \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39muproot\u001b[38;5;241m.\u001b[39mconst\u001b[38;5;241m.\u001b[39mkStreamedMemberWise\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     success = test_model_pipeline()\n",
    "#     if success:\n",
    "#         print(\"\\nAll tests passed successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(\"\\nTest failed with error:\")\n",
    "#     print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_pipeline():\n",
    "    \"\"\"Test the complete model pipeline including factory, trainer, and registry\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting Model Pipeline Test\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"TensorFlow: {tf.__version__} (Eager: {tf.executing_eagerly()})\")\n",
    "    \n",
    "    try:\n",
    "        # Helper function for JSON serialization\n",
    "        def ensure_serializable(obj):\n",
    "            \"\"\"Recursively convert numpy types to Python native types\"\"\"\n",
    "            if isinstance(obj, dict):\n",
    "                return {key: ensure_serializable(value) for key, value in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [ensure_serializable(item) for item in obj]\n",
    "            elif isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return obj\n",
    "\n",
    "        # 1. Initialize Registry\n",
    "        print(\"Initializing model registry...\")\n",
    "        registry = ModelRegistry(\"experiments\")\n",
    "        \n",
    "        # 2. Setup Data Pipeline\n",
    "        print(\"Setting up data pipeline...\")\n",
    "        data_manager = IntegratedDatasetManager(\n",
    "            run_numbers=[\"00296939\", \"00296942\"],\n",
    "            batch_size=1000,\n",
    "            cache_size=10000,\n",
    "            validation_fraction=0.15,\n",
    "            shuffle_buffer=10000\n",
    "        )\n",
    "        \n",
    "        # Create datasets with selections\n",
    "        selections = {\n",
    "            'eta': (-2.5, 2.5),\n",
    "            'chi2_per_ndof': (0.0, 3.0)\n",
    "        }\n",
    "        \n",
    "        train_dataset, val_dataset, test_dataset = data_manager.create_datasets(\n",
    "            selections=selections\n",
    "        )\n",
    "        \n",
    "        # 3. Prepare Configurations\n",
    "        print(\"Preparing configurations...\")\n",
    "        \n",
    "        # Dataset config (ensure all values are serializable)\n",
    "        dataset_config = ensure_serializable({\n",
    "            \"run_numbers\": data_manager.run_numbers,\n",
    "            \"selections\": selections,\n",
    "            \"normalization_params\": data_manager.normalization_params,\n",
    "            \"train_fraction\": 0.7,\n",
    "            \"validation_fraction\": 0.15,\n",
    "            \"test_fraction\": 0.15,\n",
    "            \"batch_size\": data_manager.batch_size,\n",
    "            \"shuffle_buffer\": data_manager.shuffle_buffer,\n",
    "            \"data_quality_metrics\": data_manager.validate_data_quality(train_dataset)\n",
    "        })\n",
    "\n",
    "        # Model config - flat version for ModelFactory\n",
    "        model_config_flat = {\n",
    "            \"model_type\": \"autoencoder\",\n",
    "            \"input_dim\": 6,\n",
    "            \"latent_dim\": 3,\n",
    "            \"encoder_layers\": [64, 32],\n",
    "            \"decoder_layers\": [32, 64],\n",
    "            \"quant_bits\": 8,\n",
    "            \"activation\": \"relu\",\n",
    "            \"name\": \"track_autoencoder\"\n",
    "        }\n",
    "        \n",
    "        # Model config - nested version for registry\n",
    "        model_config_registry = {\n",
    "            \"model_type\": \"autoencoder\",\n",
    "            \"architecture\": {\n",
    "                \"input_dim\": model_config_flat[\"input_dim\"],\n",
    "                \"latent_dim\": model_config_flat[\"latent_dim\"],\n",
    "                \"encoder_layers\": model_config_flat[\"encoder_layers\"],\n",
    "                \"decoder_layers\": model_config_flat[\"decoder_layers\"]\n",
    "            },\n",
    "            \"hyperparameters\": {\n",
    "                \"activation\": model_config_flat[\"activation\"],\n",
    "                \"quant_bits\": model_config_flat[\"quant_bits\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Training config\n",
    "        training_config = {\n",
    "            \"batch_size\": 1000,\n",
    "            \"epochs\": 5,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"early_stopping\": {\n",
    "                \"patience\": 3,\n",
    "                \"min_delta\": 1e-4\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 4. Register Experiment\n",
    "        print(\"Registering experiment...\")\n",
    "        experiment_id = registry.register_experiment(\n",
    "            name=\"autoencoder_test\",\n",
    "            dataset_config=dataset_config,\n",
    "            model_config=model_config_registry,\n",
    "            training_config=training_config,\n",
    "            description=\"Testing autoencoder on track data with enhanced monitoring\"\n",
    "        )\n",
    "        print(f\"Created experiment: {experiment_id}\")\n",
    "        \n",
    "        # 5. Create and Build Model\n",
    "        print(\"Creating model...\")\n",
    "        try:\n",
    "            model = ModelFactory.create_model(\n",
    "                model_type=\"autoencoder\",\n",
    "                config=model_config_flat\n",
    "            )\n",
    "            model.build(input_shape=(6,))\n",
    "        except Exception as e:\n",
    "            print(f\"Model creation failed: {str(e)}\")\n",
    "            print(f\"Model config used: {json.dumps(model_config_flat, indent=2)}\")\n",
    "            raise\n",
    "        \n",
    "        # 6. Setup Training\n",
    "        print(\"Setting up training...\")\n",
    "        trainer = ModelTrainer(\n",
    "            model=model,\n",
    "            training_config=training_config\n",
    "        )\n",
    "        \n",
    "        # Setup callbacks\n",
    "        class RegistryCallback(tf.keras.callbacks.Callback):\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                logs = ensure_serializable(logs or {})\n",
    "                registry.update_training_progress(\n",
    "                    experiment_id=experiment_id,\n",
    "                    epoch=epoch,\n",
    "                    metrics=logs,\n",
    "                    hardware_metrics=get_hardware_metrics()\n",
    "                )\n",
    "                \n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                patience=training_config[\"early_stopping\"][\"patience\"],\n",
    "                min_delta=training_config[\"early_stopping\"][\"min_delta\"],\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            RegistryCallback()\n",
    "        ]\n",
    "        \n",
    "        # 7. Train Model\n",
    "        print(\"Starting training...\")\n",
    "        training_start_time = datetime.now()  # Record start time\n",
    "        training_results = trainer.train(\n",
    "            dataset=train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        training_end_time = datetime.now()  # Record end time\n",
    "        training_duration = (training_end_time - training_start_time).total_seconds()\n",
    "        \n",
    "        # 8. Evaluate Model\n",
    "        print(\"Evaluating model...\")\n",
    "        test_results = trainer.evaluate(test_dataset)\n",
    "        \n",
    "        # Record final results\n",
    "        registry.complete_training(\n",
    "            experiment_id=experiment_id,\n",
    "            final_metrics=ensure_serializable({\n",
    "                **training_results,\n",
    "                **test_results,\n",
    "                'test_loss': test_results['loss'],\n",
    "                'training_duration': training_duration\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        # 9. Save Model\n",
    "        print(\"Saving model checkpoint...\")\n",
    "        registry.save_checkpoint(\n",
    "            experiment_id=experiment_id,\n",
    "            models={\"autoencoder\": model.model},\n",
    "            checkpoint_name=\"final\",\n",
    "            metadata=ensure_serializable({\n",
    "                \"test_loss\": test_results['loss'],\n",
    "                \"final_train_loss\": training_results['final_loss']\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        # 10. Display Results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Experiment Results\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        details = registry.get_experiment_details(experiment_id)\n",
    "        performance = registry.get_performance_summary(experiment_id)\n",
    "\n",
    "        print(f\"\\nExperiment ID: {experiment_id}\")\n",
    "        print(f\"Status: {details['experiment_info']['status']}\")\n",
    "\n",
    "        # Handle potential None values for duration\n",
    "        duration = performance.get('training_duration')\n",
    "        if duration is not None:\n",
    "            print(f\"Training Duration: {duration:.2f}s\")\n",
    "        else:\n",
    "            print(\"Training Duration: Not available\")\n",
    "\n",
    "        print(f\"Epochs Completed: {performance['epochs_completed']}\")\n",
    "\n",
    "        print(\"\\nMetrics:\")\n",
    "        def print_metrics(metrics, indent=2):\n",
    "            \"\"\"Helper function to print metrics with proper formatting\"\"\"\n",
    "            for key, value in metrics.items():\n",
    "                indent_str = \" \" * indent\n",
    "                if isinstance(value, dict):\n",
    "                    print(f\"{indent_str}{key}:\")\n",
    "                    print_metrics(value, indent + 2)\n",
    "                elif isinstance(value, (float, int)):\n",
    "                    print(f\"{indent_str}{key}: {value:.6f}\")\n",
    "                else:\n",
    "                    print(f\"{indent_str}{key}: {value}\")\n",
    "\n",
    "        # Print metrics using the helper function\n",
    "        print_metrics(performance['final_metrics'])\n",
    "        \n",
    "        # 11. Visualize Results\n",
    "        if True:  # Change to control visualization\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            history = performance['metric_progression']\n",
    "            plt.plot(history['loss'], label='Training Loss')\n",
    "            plt.plot(history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training History')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"Pipeline test completed successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline test failed: {type(e).__name__}: {str(e)}\")\n",
    "        print(f\"Error context:\")\n",
    "        raise\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    \"\"\"\n",
    "    Get GPU memory usage as a percentage.\n",
    "    Returns a dictionary with memory usage for each GPU.\n",
    "    \n",
    "    Returns:\n",
    "        Dict: GPU index to memory usage percentage\n",
    "        or None if no GPU is available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First try using TensorFlow's device API\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if not gpus:\n",
    "            return None\n",
    "            \n",
    "        memory_usage = {}\n",
    "        for gpu_id, gpu in enumerate(gpus):\n",
    "            # Get memory info for this GPU\n",
    "            memory_info = tf.config.experimental.get_memory_info(f'GPU:{gpu_id}')\n",
    "            \n",
    "            # Calculate percentage (note: this might not be available on all systems)\n",
    "            if hasattr(memory_info, 'peak') and hasattr(memory_info, 'total'):\n",
    "                memory_usage[f'gpu_{gpu_id}'] = (memory_info.peak / memory_info.total) * 100\n",
    "                \n",
    "        return memory_usage if memory_usage else None\n",
    "        \n",
    "    except:\n",
    "        try:\n",
    "            # Fallback to nvidia-smi command\n",
    "            import subprocess\n",
    "            import re\n",
    "            \n",
    "            output = subprocess.check_output(\n",
    "                ['nvidia-smi', '--query-gpu=index,memory.used,memory.total', '--format=csv,nounits,noheader'],\n",
    "                encoding='utf-8'\n",
    "            )\n",
    "            \n",
    "            memory_usage = {}\n",
    "            for line in output.strip().split('\\n'):\n",
    "                gpu_id, memory_used, memory_total = map(int, line.split(','))\n",
    "                memory_percentage = (memory_used / memory_total) * 100\n",
    "                memory_usage[f'gpu_{gpu_id}'] = memory_percentage\n",
    "                \n",
    "            return memory_usage\n",
    "            \n",
    "        except:\n",
    "            # If both methods fail, return None\n",
    "            return None\n",
    "\n",
    "def get_hardware_metrics():\n",
    "    \"\"\"Get current hardware usage metrics including GPU if available\"\"\"\n",
    "    metrics = {\n",
    "        'cpu_percent': psutil.cpu_percent(),\n",
    "        'memory_percent': psutil.virtual_memory().percent,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Add GPU metrics if available\n",
    "    gpu_metrics = get_gpu_memory_usage()\n",
    "    if gpu_metrics:\n",
    "        metrics.update(gpu_metrics)\n",
    "    \n",
    "    # Add additional system info\n",
    "    metrics.update({\n",
    "        'cpu_count': psutil.cpu_count(),\n",
    "        'memory_total': psutil.virtual_memory().total / (1024 ** 3),  # Convert to GB\n",
    "        'memory_available': psutil.virtual_memory().available / (1024 ** 3)  # Convert to GB\n",
    "    })\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "def print_hardware_info():\n",
    "    \"\"\"Print current hardware metrics for debugging\"\"\"\n",
    "    metrics = get_hardware_metrics()\n",
    "    \n",
    "    print(\"\\nHardware Information:\")\n",
    "    print(f\"CPU Usage: {metrics['cpu_percent']}%\")\n",
    "    print(f\"CPU Count: {metrics['cpu_count']}\")\n",
    "    print(f\"Memory Usage: {metrics['memory_percent']}%\")\n",
    "    print(f\"Total Memory: {metrics['memory_total']:.1f} GB\")\n",
    "    print(f\"Available Memory: {metrics['memory_available']:.1f} GB\")\n",
    "    \n",
    "    # Print GPU info if available\n",
    "    gpu_metrics = {k: v for k, v in metrics.items() if k.startswith('gpu_')}\n",
    "    if gpu_metrics:\n",
    "        print(\"\\nGPU Memory Usage:\")\n",
    "        for gpu, usage in gpu_metrics.items():\n",
    "            print(f\"{gpu}: {usage:.1f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo GPU detected or metrics unavailable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Starting Model Pipeline Test\n",
      "==================================================\n",
      "TensorFlow: 2.13.1 (Eager: True)\n",
      "Initializing model registry...\n",
      "Setting up data pipeline...\n",
      "\n",
      "Reading catalog files...\n",
      "Processing atlas_data/catalogs/Run_00296939_catalog_0.root\n",
      "\n",
      "Reading catalog: atlas_data/catalogs/Run_00296939_catalog_0.root\n",
      "Reading d0...\n",
      "  Shape: (35477,), dtype: float64\n",
      "Reading z0...\n",
      "  Shape: (35477,), dtype: float64\n",
      "Reading phi...\n",
      "  Shape: (35477,), dtype: float64\n",
      "Reading theta...\n",
      "  Shape: (35477,), dtype: float64\n",
      "Reading qOverP...\n",
      "  Shape: (35477,), dtype: float64\n",
      "Reading chiSquared...\n",
      "  Shape: (35477,), dtype: float64\n",
      "Reading numberDoF...\n",
      "  Shape: (35477,), dtype: float64\n",
      "Processing atlas_data/catalogs/Run_00296942_catalog_0.root\n",
      "\n",
      "Reading catalog: atlas_data/catalogs/Run_00296942_catalog_0.root\n",
      "Reading d0...\n",
      "  Shape: (55671,), dtype: float64\n",
      "Reading z0...\n",
      "  Shape: (55671,), dtype: float64\n",
      "Reading phi...\n",
      "  Shape: (55671,), dtype: float64\n",
      "Reading theta...\n",
      "  Shape: (55671,), dtype: float64\n",
      "Reading qOverP...\n",
      "  Shape: (55671,), dtype: float64\n",
      "Reading chiSquared...\n",
      "  Shape: (55671,), dtype: float64\n",
      "Reading numberDoF...\n",
      "  Shape: (55671,), dtype: float64\n",
      "\n",
      "Combining track data...\n",
      "\n",
      "Calculating derived quantities...\n",
      "Calculating pT...\n",
      "Calculating eta...\n",
      "Calculating chi2/ndof...\n",
      "Raw feature array shape: (91148, 6)\n",
      "\n",
      "Validating raw data quality...\n",
      "\n",
      "Starting data validation...\n",
      "Input type: <class 'numpy.ndarray'>\n",
      "Validating numpy array with shape: (91148, 6)\n",
      "Found 48 out-of-range values in d0\n",
      "Validation complete. Status: pass\n",
      "\n",
      "Applying selections...\n",
      "Selected 90432 / 91148 events\n",
      "\n",
      "Splitting into train/validation/test sets...\n",
      "Training samples: 72345\n",
      "Validation samples: 9043\n",
      "Test samples: 9044\n",
      "\n",
      "Computing normalization parameters...\n",
      "Preparing configurations...\n",
      "\n",
      "Starting data validation...\n",
      "Input type: <class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'>\n",
      "Validating TensorFlow dataset...\n",
      "Found 677 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 603 out-of-range values in chi2_per_ndof\n",
      "Found 700 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 580 out-of-range values in chi2_per_ndof\n",
      "Found 698 out-of-range values in pt\n",
      "Found 3 out-of-range values in d0\n",
      "Found 614 out-of-range values in chi2_per_ndof\n",
      "Found 690 out-of-range values in pt\n",
      "Found 637 out-of-range values in chi2_per_ndof\n",
      "Found 688 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 623 out-of-range values in chi2_per_ndof\n",
      "Found 686 out-of-range values in pt\n",
      "Found 578 out-of-range values in chi2_per_ndof\n",
      "Found 703 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 599 out-of-range values in chi2_per_ndof\n",
      "Found 676 out-of-range values in pt\n",
      "Found 589 out-of-range values in chi2_per_ndof\n",
      "Found 691 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 586 out-of-range values in chi2_per_ndof\n",
      "Found 697 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 606 out-of-range values in chi2_per_ndof\n",
      "Found 688 out-of-range values in pt\n",
      "Found 3 out-of-range values in d0\n",
      "Found 583 out-of-range values in chi2_per_ndof\n",
      "Found 652 out-of-range values in pt\n",
      "Found 589 out-of-range values in chi2_per_ndof\n",
      "Found 699 out-of-range values in pt\n",
      "Found 3 out-of-range values in d0\n",
      "Found 597 out-of-range values in chi2_per_ndof\n",
      "Found 685 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 586 out-of-range values in chi2_per_ndof\n",
      "Found 668 out-of-range values in pt\n",
      "Found 634 out-of-range values in chi2_per_ndof\n",
      "Found 694 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 609 out-of-range values in chi2_per_ndof\n",
      "Found 696 out-of-range values in pt\n",
      "Found 597 out-of-range values in chi2_per_ndof\n",
      "Found 702 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 588 out-of-range values in chi2_per_ndof\n",
      "Found 718 out-of-range values in pt\n",
      "Found 627 out-of-range values in chi2_per_ndof\n",
      "Found 699 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 588 out-of-range values in chi2_per_ndof\n",
      "Found 693 out-of-range values in pt\n",
      "Found 3 out-of-range values in d0\n",
      "Found 609 out-of-range values in chi2_per_ndof\n",
      "Found 690 out-of-range values in pt\n",
      "Found 571 out-of-range values in chi2_per_ndof\n",
      "Found 694 out-of-range values in pt\n",
      "Found 608 out-of-range values in chi2_per_ndof\n",
      "Found 689 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 616 out-of-range values in chi2_per_ndof\n",
      "Found 687 out-of-range values in pt\n",
      "Found 3 out-of-range values in d0\n",
      "Found 598 out-of-range values in chi2_per_ndof\n",
      "Found 661 out-of-range values in pt\n",
      "Found 605 out-of-range values in chi2_per_ndof\n",
      "Found 693 out-of-range values in pt\n",
      "Found 596 out-of-range values in chi2_per_ndof\n",
      "Found 683 out-of-range values in pt\n",
      "Found 606 out-of-range values in chi2_per_ndof\n",
      "Found 686 out-of-range values in pt\n",
      "Found 581 out-of-range values in chi2_per_ndof\n",
      "Found 694 out-of-range values in pt\n",
      "Found 586 out-of-range values in chi2_per_ndof\n",
      "Found 698 out-of-range values in pt\n",
      "Found 613 out-of-range values in chi2_per_ndof\n",
      "Found 712 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 609 out-of-range values in chi2_per_ndof\n",
      "Found 704 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 585 out-of-range values in chi2_per_ndof\n",
      "Found 692 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 621 out-of-range values in chi2_per_ndof\n",
      "Found 697 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 605 out-of-range values in chi2_per_ndof\n",
      "Found 684 out-of-range values in pt\n",
      "Found 3 out-of-range values in d0\n",
      "Found 602 out-of-range values in chi2_per_ndof\n",
      "Found 693 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 610 out-of-range values in chi2_per_ndof\n",
      "Found 681 out-of-range values in pt\n",
      "Found 587 out-of-range values in chi2_per_ndof\n",
      "Found 691 out-of-range values in pt\n",
      "Found 624 out-of-range values in chi2_per_ndof\n",
      "Found 695 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 602 out-of-range values in chi2_per_ndof\n",
      "Found 671 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 590 out-of-range values in chi2_per_ndof\n",
      "Found 685 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 586 out-of-range values in chi2_per_ndof\n",
      "Found 705 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 627 out-of-range values in chi2_per_ndof\n",
      "Found 686 out-of-range values in pt\n",
      "Found 3 out-of-range values in d0\n",
      "Found 612 out-of-range values in chi2_per_ndof\n",
      "Found 685 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 602 out-of-range values in chi2_per_ndof\n",
      "Found 708 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 593 out-of-range values in chi2_per_ndof\n",
      "Found 676 out-of-range values in pt\n",
      "Found 3 out-of-range values in d0\n",
      "Found 595 out-of-range values in chi2_per_ndof\n",
      "Found 691 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 582 out-of-range values in chi2_per_ndof\n",
      "Found 674 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 572 out-of-range values in chi2_per_ndof\n",
      "Found 683 out-of-range values in pt\n",
      "Found 626 out-of-range values in chi2_per_ndof\n",
      "Found 708 out-of-range values in pt\n",
      "Found 599 out-of-range values in chi2_per_ndof\n",
      "Found 671 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 603 out-of-range values in chi2_per_ndof\n",
      "Found 693 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 607 out-of-range values in chi2_per_ndof\n",
      "Found 694 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 547 out-of-range values in chi2_per_ndof\n",
      "Found 675 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 596 out-of-range values in chi2_per_ndof\n",
      "Found 694 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 584 out-of-range values in chi2_per_ndof\n",
      "Found 703 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 577 out-of-range values in chi2_per_ndof\n",
      "Found 708 out-of-range values in pt\n",
      "Found 603 out-of-range values in chi2_per_ndof\n",
      "Found 692 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 614 out-of-range values in chi2_per_ndof\n",
      "Found 715 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 604 out-of-range values in chi2_per_ndof\n",
      "Found 686 out-of-range values in pt\n",
      "Found 607 out-of-range values in chi2_per_ndof\n",
      "Found 694 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 581 out-of-range values in chi2_per_ndof\n",
      "Found 710 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 640 out-of-range values in chi2_per_ndof\n",
      "Found 677 out-of-range values in pt\n",
      "Found 613 out-of-range values in chi2_per_ndof\n",
      "Found 683 out-of-range values in pt\n",
      "Found 599 out-of-range values in chi2_per_ndof\n",
      "Found 675 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 586 out-of-range values in chi2_per_ndof\n",
      "Found 694 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 580 out-of-range values in chi2_per_ndof\n",
      "Found 684 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 622 out-of-range values in chi2_per_ndof\n",
      "Found 685 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 619 out-of-range values in chi2_per_ndof\n",
      "Found 687 out-of-range values in pt\n",
      "Found 2 out-of-range values in d0\n",
      "Found 607 out-of-range values in chi2_per_ndof\n",
      "Found 703 out-of-range values in pt\n",
      "Found 1 out-of-range values in d0\n",
      "Found 609 out-of-range values in chi2_per_ndof\n",
      "Found 685 out-of-range values in pt\n",
      "Found 603 out-of-range values in chi2_per_ndof\n",
      "Too many out of range values in pt: 69.033%\n",
      "Too many out of range values in d0: 0.110%\n",
      "Too many out of range values in chi2_per_ndof: 60.044%\n",
      "Validation complete. Status: warning\n",
      "Registering experiment...\n",
      "Created experiment: b5226c22-891d-4d8f-8648-cd45f5ecb525\n",
      "Creating model...\n",
      "\n",
      "Model layer structure:\n",
      "Layer: input_layer, Type: <class 'keras.src.engine.input_layer.InputLayer'>\n",
      "Layer: encoder_dense_0, Type: <class 'qkeras.qlayers.QDense'>\n",
      "Layer: encoder_activation_0, Type: <class 'qkeras.qlayers.QActivation'>\n",
      "Layer: encoder_bn_0, Type: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: encoder_dense_1, Type: <class 'qkeras.qlayers.QDense'>\n",
      "Layer: encoder_activation_1, Type: <class 'qkeras.qlayers.QActivation'>\n",
      "Layer: encoder_bn_1, Type: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: latent_layer, Type: <class 'qkeras.qlayers.QDense'>\n",
      "Layer: decoder_dense_0, Type: <class 'qkeras.qlayers.QDense'>\n",
      "Layer: decoder_activation_0, Type: <class 'qkeras.qlayers.QActivation'>\n",
      "Layer: decoder_bn_0, Type: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: decoder_dense_1, Type: <class 'qkeras.qlayers.QDense'>\n",
      "Layer: decoder_activation_1, Type: <class 'qkeras.qlayers.QActivation'>\n",
      "Layer: decoder_bn_1, Type: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: output_layer, Type: <class 'qkeras.qlayers.QDense'>\n",
      "Setting up training...\n",
      "Starting training...\n",
      "Epoch 1/5\n",
      "72/72 [==============================] - 9s 126ms/step - loss: 1.1479 - mse: 1.1479 - val_loss: 0.7313 - val_mse: 0.7313\n",
      "Epoch 2/5\n",
      "72/72 [==============================] - 9s 121ms/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.5694 - val_mse: 0.5694\n",
      "Epoch 3/5\n",
      "72/72 [==============================] - 9s 123ms/step - loss: 0.6032 - mse: 0.6032 - val_loss: 0.4583 - val_mse: 0.4583\n",
      "Epoch 4/5\n",
      "72/72 [==============================] - 9s 123ms/step - loss: 0.5554 - mse: 0.5554 - val_loss: 0.3991 - val_mse: 0.3991\n",
      "Epoch 5/5\n",
      "72/72 [==============================] - 9s 124ms/step - loss: 0.5239 - mse: 0.5239 - val_loss: 0.3635 - val_mse: 0.3635\n",
      "Evaluating model...\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.4534 - mse: 0.4534\n",
      "Saving model checkpoint...\n",
      "INFO:tensorflow:Assets written to: experiments/model_store/b5226c22-891d-4d8f-8648-cd45f5ecb525/checkpoints/final/autoencoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/model_store/b5226c22-891d-4d8f-8648-cd45f5ecb525/checkpoints/final/autoencoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Experiment Results\n",
      "==================================================\n",
      "\n",
      "Experiment ID: b5226c22-891d-4d8f-8648-cd45f5ecb525\n",
      "Status: checkpoint_saved\n",
      "Training Duration: Not available\n",
      "Epochs Completed: 4\n",
      "\n",
      "Metrics:\n",
      "  training_config:\n",
      "    batch_size: 1000.000000\n",
      "    epochs: 5.000000\n",
      "    learning_rate: 0.001000\n",
      "    early_stopping:\n",
      "      patience: 3.000000\n",
      "      min_delta: 0.000100\n",
      "  final_loss: 0.523857\n",
      "  final_val_loss: 0.363539\n",
      "  history:\n",
      "    loss: [1.147887110710144, 0.6974677443504333, 0.6032010316848755, 0.5554032921791077, 0.5238567590713501]\n",
      "    mse: [1.147887110710144, 0.6974677443504333, 0.6032010316848755, 0.5554032921791077, 0.5238567590713501]\n",
      "    val_loss: [0.7313452959060669, 0.5693614482879639, 0.4583092927932739, 0.3991236388683319, 0.3635394275188446]\n",
      "    val_mse: [0.7313452959060669, 0.5693614482879639, 0.4583092927932739, 0.3991236388683319, 0.3635394275188446]\n",
      "  loss: 0.453360\n",
      "  mse: 0.453360\n",
      "  test_loss: 0.453360\n",
      "  training_duration: 44.777804\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline test completed successfully\n",
      "\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    success = test_model_pipeline()\n",
    "    if success:\n",
    "        print(\"\\nAll tests passed successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"\\nTest failed with error:\")\n",
    "    print(str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hep_foundation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
