{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 15:25:33.318917: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-03 15:25:33.399456: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-03 15:25:33.400517: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-03 15:25:34.792613: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union, Any, Optional, Tuple\n",
    "import uuid\n",
    "import uproot\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from abc import ABC, abstractmethod\n",
    "from tensorflow import keras\n",
    "from qkeras import QDense, QActivation, quantized_bits, quantized_relu\n",
    "import platform\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATLAS_RUN_NUMBERS = [\"00296939\", \"00296942\", \"00297447\", \"00297170\", \"00297041\", \"00297730\", \"00298591\", \"00298595\", \"00298609\", \"00298633\", \"00298687\", \"00298690\", \"00298771\", \"00298773\", \"00298862\", \"00298967\", \"00299055\", \"00299144\", \"00299147\", \"00299184\", \"00299241\", \"00299243\", \"00299278\", \"00299288\", \"00299315\", \"00299340\", \"00299343\", \"00299390\", \"00299584\", \"00300279\", \"00300287\", \"00300345\", \"00300415\", \"00300418\", \"00300487\", \"00300540\", \"00300571\", \"00300600\", \"00300655\", \"00300687\", \"00300784\", \"00300800\", \"00300863\", \"00300908\", \"00301912\", \"00301915\", \"00301918\", \"00301932\", \"00301973\", \"00302053\", \"00302137\", \"00302265\", \"00302269\", \"00302300\", \"00302347\", \"00302380\", \"00302391\", \"00302393\", \"00302737\", \"00302829\", \"00302831\", \"00302872\", \"00302919\", \"00302925\", \"00302956\", \"00303007\", \"00303059\", \"00303079\", \"00303201\", \"00303208\", \"00303264\", \"00303266\", \"00303291\", \"00303304\", \"00303338\", \"00303421\", \"00303499\", \"00303560\", \"00303638\", \"00303726\", \"00303811\", \"00303817\", \"00303819\", \"00303832\", \"00303846\", \"00303892\", \"00303943\", \"00304006\", \"00304008\", \"00304128\", \"00304178\", \"00304198\", \"00304211\", \"00304243\", \"00304308\", \"00304337\", \"00304409\", \"00304431\", \"00304494\", \"00305291\", \"00305293\", \"00305359\", \"00305380\", \"00305543\", \"00305571\", \"00305618\", \"00305671\", \"00305674\", \"00305723\", \"00305727\", \"00305735\", \"00305777\", \"00305811\", \"00305920\", \"00306247\", \"00306269\", \"00306278\", \"00306310\", \"00306384\", \"00306419\", \"00306442\", \"00306448\", \"00306451\", \"00306556\", \"00306655\", \"00306657\", \"00306714\", \"00307124\", \"00307126\", \"00307195\", \"00307259\", \"00307306\", \"00307354\", \"00307358\", \"00307394\", \"00307454\", \"00307514\", \"00307539\", \"00307569\", \"00307601\", \"00307619\", \"00307656\", \"00307710\", \"00307716\", \"00307732\", \"00307861\", \"00307935\", \"00308047\", \"00308084\", \"00309311\", \"00309314\", \"00309346\", \"00309375\", \"00309390\", \"00309440\", \"00309516\", \"00309640\", \"00309674\", \"00309759\", \"00310015\", \"00310210\", \"00310247\", \"00310249\", \"00310341\", \"00310370\", \"00310405\", \"00310468\", \"00310473\", \"00310574\", \"00310634\", \"00310691\", \"00310738\", \"00310781\", \"00310809\", \"00310863\", \"00310872\", \"00310969\", \"00311071\", \"00311170\", \"00311244\", \"00311287\", \"00311321\", \"00311365\", \"00311402\", \"00311473\", \"00311481\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_usage():\n",
    "        \"\"\"Get current system memory and CPU usage with proper cleanup\"\"\"\n",
    "        # Force garbage collection before checking memory\n",
    "        gc.collect()\n",
    "        \n",
    "        # Get memory info\n",
    "        memory = psutil.virtual_memory()\n",
    "        cpu_percent = psutil.cpu_percent(interval=0.1)  # Shorter interval\n",
    "        \n",
    "        # Calculate memory in GB\n",
    "        total_gb = memory.total / (1024**3)\n",
    "        available_gb = memory.available / (1024**3)\n",
    "        used_gb = memory.used / (1024**3)\n",
    "        \n",
    "        return {\n",
    "            'memory': {\n",
    "                'total_gb': total_gb,\n",
    "                'available_gb': available_gb,\n",
    "                'used_gb': used_gb,\n",
    "                'percent': memory.percent\n",
    "            },\n",
    "            'cpu': {\n",
    "                'percent': cpu_percent\n",
    "            }\n",
    "        }\n",
    "\n",
    "def print_system_usage(prefix=\"\"):\n",
    "    \"\"\"Print current system usage with optional prefix\"\"\"\n",
    "    usage = get_system_usage()\n",
    "    print(f\"\\n{prefix}System Usage:\")\n",
    "    print(f\"Memory: {usage['memory']['used_gb']:.1f}GB / {usage['memory']['total_gb']:.1f}GB ({usage['memory']['percent']}%)\")\n",
    "    print(f\"Available Memory: {usage['memory']['available_gb']:.1f}GB\")\n",
    "    print(f\"CPU Usage: {usage['cpu']['percent']}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRegistry:\n",
    "    \"\"\"\n",
    "    Enhanced central registry for managing ML experiments, models, and metrics\n",
    "    Tracks detailed dataset configurations and training metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, base_path: str):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.db_path = self.base_path / \"registry.db\"\n",
    "        self.model_store = self.base_path / \"model_store\"\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        self.model_store.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize database\n",
    "        self._initialize_db()\n",
    "        \n",
    "    def _initialize_db(self):\n",
    "        \"\"\"Create database tables if they don't exist\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            # Main experiments table\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS experiments (\n",
    "                    experiment_id TEXT PRIMARY KEY,\n",
    "                    timestamp DATETIME,\n",
    "                    name TEXT,\n",
    "                    description TEXT,\n",
    "                    status TEXT,\n",
    "                    environment_info JSON\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            # Dataset configuration table\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS dataset_configs (\n",
    "                    experiment_id TEXT PRIMARY KEY,\n",
    "                    run_numbers JSON,\n",
    "                    track_selections JSON,\n",
    "                    event_selections JSON,\n",
    "                    max_tracks_per_event INTEGER,\n",
    "                    min_tracks_per_event INTEGER,\n",
    "                    normalization_params JSON,\n",
    "                    train_fraction FLOAT,\n",
    "                    validation_fraction FLOAT,\n",
    "                    test_fraction FLOAT,\n",
    "                    batch_size INTEGER,\n",
    "                    shuffle_buffer INTEGER,\n",
    "                    data_quality_metrics JSON,\n",
    "                    FOREIGN KEY(experiment_id) REFERENCES experiments(experiment_id)\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            # Model configuration table\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS model_configs (\n",
    "                    experiment_id TEXT PRIMARY KEY,\n",
    "                    model_type TEXT,\n",
    "                    architecture JSON,\n",
    "                    hyperparameters JSON,\n",
    "                    FOREIGN KEY(experiment_id) REFERENCES experiments(experiment_id)\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            # Training configuration and results\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS training_info (\n",
    "                    experiment_id TEXT PRIMARY KEY,\n",
    "                    config JSON,\n",
    "                    start_time DATETIME,\n",
    "                    end_time DATETIME,\n",
    "                    epochs_completed INTEGER,\n",
    "                    training_history JSON,\n",
    "                    final_metrics JSON,\n",
    "                    hardware_metrics JSON,\n",
    "                    FOREIGN KEY(experiment_id) REFERENCES experiments(experiment_id)\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            # Checkpoints table\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS checkpoints (\n",
    "                    checkpoint_id TEXT PRIMARY KEY,\n",
    "                    experiment_id TEXT,\n",
    "                    name TEXT,\n",
    "                    timestamp DATETIME,\n",
    "                    metadata JSON,\n",
    "                    FOREIGN KEY(experiment_id) REFERENCES experiments(experiment_id)\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "    def register_experiment(self,\n",
    "                          name: str,\n",
    "                          dataset_config: dict,\n",
    "                          model_config: dict,\n",
    "                          training_config: dict,\n",
    "                          description: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        Register new experiment with enhanced configuration tracking\n",
    "        \n",
    "        Args:\n",
    "            name: Human readable experiment name\n",
    "            dataset_config: Dataset parameters including:\n",
    "                - run_numbers: List of ATLAS run numbers\n",
    "                - track_selections: Dictionary of track-level selection criteria\n",
    "                - event_selections: Dictionary of event-level selection criteria\n",
    "                - max_tracks_per_event: Maximum number of tracks to keep per event\n",
    "                - min_tracks_per_event: Minimum number of tracks required per event\n",
    "                - normalization_params: Dictionary of normalization parameters\n",
    "                - train_fraction: Fraction of data for training\n",
    "                - validation_fraction: Fraction for validation\n",
    "                - test_fraction: Fraction for testing\n",
    "                - batch_size: Batch size used\n",
    "                - shuffle_buffer: Shuffle buffer size\n",
    "                - data_quality_metrics: Results of data validation\n",
    "            model_config: Model configuration including:\n",
    "                - model_type: Type of model (e.g., \"autoencoder\")\n",
    "                - architecture: Network architecture details\n",
    "                - hyperparameters: Model hyperparameters\n",
    "            training_config: Training parameters\n",
    "            description: Optional experiment description\n",
    "        \"\"\"\n",
    "        experiment_id = str(uuid.uuid4())\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        # Get environment info\n",
    "        environment_info = {\n",
    "            \"python_version\": platform.python_version(),\n",
    "            \"tensorflow_version\": tf.__version__,\n",
    "            \"platform\": platform.platform(),\n",
    "            \"cpu_count\": os.cpu_count()\n",
    "        }\n",
    "        try:\n",
    "            environment_info[\"gpu_devices\"] = tf.config.list_physical_devices('GPU')\n",
    "        except:\n",
    "            environment_info[\"gpu_devices\"] = []\n",
    "            \n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            # Insert main experiment info\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO experiments \n",
    "                (experiment_id, timestamp, name, description, status, environment_info)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    experiment_id,\n",
    "                    timestamp,\n",
    "                    name,\n",
    "                    description,\n",
    "                    \"registered\",\n",
    "                    json.dumps(environment_info)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Insert dataset configuration\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO dataset_configs\n",
    "                (experiment_id, run_numbers, track_selections, event_selections,\n",
    "                max_tracks_per_event, min_tracks_per_event, normalization_params,\n",
    "                train_fraction, validation_fraction, test_fraction,\n",
    "                batch_size, shuffle_buffer, data_quality_metrics)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    experiment_id,\n",
    "                    json.dumps(dataset_config['run_numbers']),\n",
    "                    json.dumps(dataset_config['track_selections']),\n",
    "                    json.dumps(dataset_config['event_selections']),\n",
    "                    dataset_config['max_tracks_per_event'],\n",
    "                    dataset_config['min_tracks_per_event'],\n",
    "                    json.dumps(dataset_config.get('normalization_params', {})),\n",
    "                    dataset_config['train_fraction'],\n",
    "                    dataset_config['validation_fraction'],\n",
    "                    dataset_config['test_fraction'],\n",
    "                    dataset_config['batch_size'],\n",
    "                    dataset_config['shuffle_buffer'],\n",
    "                    json.dumps(dataset_config.get('data_quality_metrics', {}))\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Insert model configuration\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO model_configs\n",
    "                (experiment_id, model_type, architecture, hyperparameters)\n",
    "                VALUES (?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    experiment_id,\n",
    "                    model_config['model_type'],\n",
    "                    json.dumps(model_config['architecture']),\n",
    "                    json.dumps(model_config.get('hyperparameters', {}))\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Insert initial training info\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO training_info\n",
    "                (experiment_id, config, start_time, epochs_completed, training_history, final_metrics)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    experiment_id,\n",
    "                    json.dumps(training_config),\n",
    "                    None,\n",
    "                    0,\n",
    "                    \"{}\",\n",
    "                    \"{}\"\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Create experiment directory structure\n",
    "        exp_dir = self.model_store / experiment_id\n",
    "        exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save configurations as YAML for easy reading\n",
    "        configs_dir = exp_dir / \"configs\"\n",
    "        configs_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        with open(configs_dir / \"dataset_config.yaml\", 'w') as f:\n",
    "            yaml.dump(dataset_config, f)\n",
    "        with open(configs_dir / \"model_config.yaml\", 'w') as f:\n",
    "            yaml.dump(model_config, f)\n",
    "        with open(configs_dir / \"training_config.yaml\", 'w') as f:\n",
    "            yaml.dump(training_config, f)\n",
    "            \n",
    "        return experiment_id\n",
    "        \n",
    "    def update_training_progress(self,\n",
    "                               experiment_id: str,\n",
    "                               epoch: int,\n",
    "                               metrics: Dict[str, float],\n",
    "                               hardware_metrics: Optional[Dict] = None):\n",
    "        \"\"\"Update training progress and metrics\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            current = conn.execute(\n",
    "                \"SELECT training_history FROM training_info WHERE experiment_id = ?\",\n",
    "                (experiment_id,)\n",
    "            ).fetchone()\n",
    "            \n",
    "            if current is None:\n",
    "                raise ValueError(f\"No experiment found with id {experiment_id}\")\n",
    "                \n",
    "            history = json.loads(current[0])\n",
    "            \n",
    "            # Update history\n",
    "            if str(epoch) not in history:\n",
    "                history[str(epoch)] = {}\n",
    "            history[str(epoch)].update(metrics)\n",
    "            \n",
    "            # Update training info\n",
    "            updates = {\n",
    "                \"epochs_completed\": epoch,\n",
    "                \"training_history\": json.dumps(history)\n",
    "            }\n",
    "            \n",
    "            if hardware_metrics:\n",
    "                updates[\"hardware_metrics\"] = json.dumps(hardware_metrics)\n",
    "                \n",
    "            update_sql = \"UPDATE training_info SET \" + \\\n",
    "                        \", \".join(f\"{k} = ?\" for k in updates.keys()) + \\\n",
    "                        \" WHERE experiment_id = ?\"\n",
    "            \n",
    "            conn.execute(update_sql, list(updates.values()) + [experiment_id])\n",
    "            \n",
    "    def complete_training(self,\n",
    "                         experiment_id: str,\n",
    "                         final_metrics: Dict[str, float]):\n",
    "        \"\"\"Mark training as complete and save final metrics\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                UPDATE training_info \n",
    "                SET end_time = ?, final_metrics = ?\n",
    "                WHERE experiment_id = ?\n",
    "                \"\"\",\n",
    "                (datetime.now(), json.dumps(final_metrics), experiment_id)\n",
    "            )\n",
    "            \n",
    "            conn.execute(\n",
    "                \"UPDATE experiments SET status = ? WHERE experiment_id = ?\",\n",
    "                (\"completed\", experiment_id)\n",
    "            )\n",
    "\n",
    "    def save_checkpoint(self, \n",
    "                       experiment_id: str,\n",
    "                       models: Dict[str, Any],\n",
    "                       checkpoint_name: str = \"latest\",\n",
    "                       metadata: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Save model checkpoints for an experiment\n",
    "        \n",
    "        Args:\n",
    "            experiment_id: Experiment identifier\n",
    "            models: Dictionary of named models to save\n",
    "            checkpoint_name: Name for this checkpoint\n",
    "            metadata: Optional metadata about the checkpoint\n",
    "        \"\"\"\n",
    "        exp_dir = self.model_store / experiment_id\n",
    "        checkpoint_dir = exp_dir / \"checkpoints\" / checkpoint_name\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        checkpoint_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Save each model\n",
    "        for name, model in models.items():\n",
    "            model_path = checkpoint_dir / name\n",
    "            model.save(model_path)\n",
    "            \n",
    "        # Save checkpoint metadata\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        metadata.update({\n",
    "            \"saved_models\": list(models.keys()),\n",
    "            \"checkpoint_path\": str(checkpoint_dir)\n",
    "        })\n",
    "            \n",
    "        # Record checkpoint in database\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO checkpoints\n",
    "                (checkpoint_id, experiment_id, name, timestamp, metadata)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    checkpoint_id,\n",
    "                    experiment_id,\n",
    "                    checkpoint_name,\n",
    "                    datetime.now(),\n",
    "                    json.dumps(metadata)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Update experiment status\n",
    "            conn.execute(\n",
    "                \"UPDATE experiments SET status = ? WHERE experiment_id = ?\",\n",
    "                (\"checkpoint_saved\", experiment_id)\n",
    "            )\n",
    "            \n",
    "    def load_checkpoint(self, \n",
    "                       experiment_id: str,\n",
    "                       checkpoint_name: str = \"latest\") -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Get paths to saved model checkpoints\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of model names to their saved paths\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            result = conn.execute(\n",
    "                \"\"\"\n",
    "                SELECT metadata FROM checkpoints \n",
    "                WHERE experiment_id = ? AND name = ?\n",
    "                ORDER BY timestamp DESC LIMIT 1\n",
    "                \"\"\",\n",
    "                (experiment_id, checkpoint_name)\n",
    "            ).fetchone()\n",
    "            \n",
    "        if result is None:\n",
    "            raise ValueError(\n",
    "                f\"No checkpoint '{checkpoint_name}' found for experiment {experiment_id}\"\n",
    "            )\n",
    "            \n",
    "        metadata = json.loads(result[0])\n",
    "        checkpoint_path = Path(metadata[\"checkpoint_path\"])\n",
    "        \n",
    "        if not checkpoint_path.exists():\n",
    "            raise ValueError(f\"Checkpoint directory not found: {checkpoint_path}\")\n",
    "            \n",
    "        return {\n",
    "            model_name: str(checkpoint_path / model_name)\n",
    "            for model_name in metadata[\"saved_models\"]\n",
    "            if (checkpoint_path / model_name).exists()\n",
    "        }\n",
    "\n",
    "    def query_experiments(self, \n",
    "                         filters: Optional[Dict] = None,\n",
    "                         metrics: Optional[List[str]] = None,\n",
    "                         sort_by: Optional[str] = None,\n",
    "                         ascending: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query experiments with enhanced filtering and sorting\n",
    "        \n",
    "        Args:\n",
    "            filters: Dictionary of column:value pairs to filter on\n",
    "            metrics: List of specific metrics to include\n",
    "            sort_by: Column to sort by\n",
    "            ascending: Sort order\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame of matching experiments\n",
    "        \"\"\"\n",
    "        # Build base query joining all tables\n",
    "        query = \"\"\"\n",
    "        SELECT e.*, dc.*, mc.*, ti.* \n",
    "        FROM experiments e\n",
    "        LEFT JOIN dataset_configs dc ON e.experiment_id = dc.experiment_id\n",
    "        LEFT JOIN model_configs mc ON e.experiment_id = mc.experiment_id\n",
    "        LEFT JOIN training_info ti ON e.experiment_id = ti.experiment_id\n",
    "        \"\"\"\n",
    "        \n",
    "        params = []\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            for column, value in filters.items():\n",
    "                table = {\n",
    "                    'name': 'e',\n",
    "                    'status': 'e',\n",
    "                    'model_type': 'mc',\n",
    "                    'batch_size': 'dc'\n",
    "                }.get(column.split('.')[0], 'e')\n",
    "                \n",
    "                conditions.append(f\"{table}.{column} = ?\")\n",
    "                params.append(value)\n",
    "                \n",
    "            if conditions:\n",
    "                query += \" WHERE \" + \" AND \".join(conditions)\n",
    "                \n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            df = pd.read_sql_query(query, conn, params=params)\n",
    "            \n",
    "        # Parse JSON columns\n",
    "        json_columns = {\n",
    "            'environment_info': 'experiments',\n",
    "            'run_numbers': 'dataset_configs',\n",
    "            'selections': 'dataset_configs',\n",
    "            'normalization_params': 'dataset_configs',\n",
    "            'data_quality_metrics': 'dataset_configs',\n",
    "            'architecture': 'model_configs',\n",
    "            'hyperparameters': 'model_configs',\n",
    "            'config': 'training_info',\n",
    "            'training_history': 'training_info',\n",
    "            'final_metrics': 'training_info',\n",
    "            'hardware_metrics': 'training_info'\n",
    "        }\n",
    "        \n",
    "        for col in json_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(lambda x: json.loads(x) if pd.notna(x) else {})\n",
    "        \n",
    "        # Extract specific metrics if requested\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                df[f\"metric_{metric}\"] = df['final_metrics'].apply(\n",
    "                    lambda x: x.get(metric, None) if isinstance(x, dict) else None\n",
    "                )\n",
    "                \n",
    "        # Sort if requested\n",
    "        if sort_by:\n",
    "            df = df.sort_values(sort_by, ascending=ascending)\n",
    "            \n",
    "        return df\n",
    "        \n",
    "    def delete_experiment(self, experiment_id: str):\n",
    "        \"\"\"Delete an experiment and all associated files\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            # Delete from all tables\n",
    "            for table in ['training_info', 'model_configs', 'dataset_configs', \n",
    "                         'checkpoints', 'experiments']:\n",
    "                conn.execute(f\"DELETE FROM {table} WHERE experiment_id = ?\", \n",
    "                           (experiment_id,))\n",
    "            \n",
    "        # Delete files\n",
    "        exp_dir = self.model_store / experiment_id\n",
    "        if exp_dir.exists():\n",
    "            shutil.rmtree(exp_dir)\n",
    "            \n",
    "    def get_experiment_details(self, experiment_id: str) -> dict:\n",
    "        \"\"\"Get complete experiment information including all configs and results\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            # Get all information from all tables\n",
    "            experiment = conn.execute(\n",
    "                \"SELECT * FROM experiments WHERE experiment_id = ?\",\n",
    "                (experiment_id,)\n",
    "            ).fetchone()\n",
    "            \n",
    "            if experiment is None:\n",
    "                raise ValueError(f\"No experiment found with id {experiment_id}\")\n",
    "                \n",
    "            dataset_config = conn.execute(\n",
    "                \"SELECT * FROM dataset_configs WHERE experiment_id = ?\",\n",
    "                (experiment_id,)\n",
    "            ).fetchone()\n",
    "            \n",
    "            model_config = conn.execute(\n",
    "                \"SELECT * FROM model_configs WHERE experiment_id = ?\",\n",
    "                (experiment_id,)\n",
    "            ).fetchone()\n",
    "            \n",
    "            training_info = conn.execute(\n",
    "                \"SELECT * FROM training_info WHERE experiment_id = ?\",\n",
    "                (experiment_id,)\n",
    "            ).fetchone()\n",
    "            \n",
    "            checkpoints = conn.execute(\n",
    "                \"SELECT * FROM checkpoints WHERE experiment_id = ?\",\n",
    "                (experiment_id,)\n",
    "            ).fetchall()\n",
    "            \n",
    "        # Combine all information\n",
    "        details = {\n",
    "            \"experiment_info\": dict(zip(\n",
    "                ['experiment_id', 'timestamp', 'name', 'description', 'status', 'environment_info'],\n",
    "                experiment\n",
    "            )),\n",
    "            \"dataset_config\": dict(zip(\n",
    "                ['experiment_id', 'run_numbers', 'selections', 'normalization_params',\n",
    "                 'train_fraction', 'validation_fraction', 'test_fraction',\n",
    "                 'batch_size', 'shuffle_buffer', 'data_quality_metrics'],\n",
    "                dataset_config\n",
    "            )),\n",
    "            \"model_config\": dict(zip(\n",
    "                ['experiment_id', 'model_type', 'architecture', 'hyperparameters'],\n",
    "                model_config\n",
    "            )),\n",
    "            \"training_info\": dict(zip(\n",
    "                ['experiment_id', 'config', 'start_time', 'end_time', \n",
    "                 'epochs_completed', 'training_history', 'final_metrics', 'hardware_metrics'],\n",
    "                training_info\n",
    "            )),\n",
    "            \"checkpoints\": [\n",
    "                dict(zip(\n",
    "                    ['checkpoint_id', 'experiment_id', 'name', 'timestamp', 'metadata'],\n",
    "                    checkpoint\n",
    "                ))\n",
    "                for checkpoint in checkpoints\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Parse JSON fields\n",
    "        details['experiment_info']['environment_info'] = json.loads(details['experiment_info']['environment_info'])\n",
    "        details['dataset_config']['run_numbers'] = json.loads(details['dataset_config']['run_numbers'])\n",
    "        details['dataset_config']['selections'] = json.loads(details['dataset_config']['selections'])\n",
    "        details['dataset_config']['normalization_params'] = json.loads(details['dataset_config']['normalization_params'])\n",
    "        details['dataset_config']['data_quality_metrics'] = json.loads(details['dataset_config']['data_quality_metrics'])\n",
    "        details['model_config']['architecture'] = json.loads(details['model_config']['architecture'])\n",
    "        details['model_config']['hyperparameters'] = json.loads(details['model_config']['hyperparameters'])\n",
    "        details['training_info']['config'] = json.loads(details['training_info']['config'])\n",
    "        details['training_info']['training_history'] = json.loads(details['training_info']['training_history'])\n",
    "        details['training_info']['final_metrics'] = json.loads(details['training_info']['final_metrics'])\n",
    "        if details['training_info']['hardware_metrics']:\n",
    "            details['training_info']['hardware_metrics'] = json.loads(details['training_info']['hardware_metrics'])\n",
    "        \n",
    "        for checkpoint in details['checkpoints']:\n",
    "            checkpoint['metadata'] = json.loads(checkpoint['metadata'])\n",
    "            \n",
    "        return details\n",
    "    \n",
    "    def get_performance_summary(self, experiment_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of model performance metrics\n",
    "        \n",
    "        Args:\n",
    "            experiment_id: Experiment identifier\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing performance metrics and training time\n",
    "        \"\"\"\n",
    "        experiment = self.get_experiment_details(experiment_id)\n",
    "        training_info = experiment['training_info']\n",
    "        \n",
    "        # Calculate training time\n",
    "        if training_info['start_time'] and training_info['end_time']:\n",
    "            start = datetime.fromisoformat(training_info['start_time'])\n",
    "            end = datetime.fromisoformat(training_info['end_time'])\n",
    "            training_duration = (end - start).total_seconds()\n",
    "        else:\n",
    "            training_duration = None\n",
    "            \n",
    "        # Get final metrics\n",
    "        final_metrics = training_info['final_metrics']\n",
    "        \n",
    "        # Get training history progression\n",
    "        history = training_info['training_history']\n",
    "        metric_progression = {\n",
    "            metric: [epoch_data.get(metric) for epoch_data in history.values()]\n",
    "            for metric in set().union(*[epoch_data.keys() for epoch_data in history.values()])\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'training_duration': training_duration,\n",
    "            'epochs_completed': training_info['epochs_completed'],\n",
    "            'final_metrics': final_metrics,\n",
    "            'metric_progression': metric_progression,\n",
    "            'hardware_metrics': training_info.get('hardware_metrics', {})\n",
    "        }\n",
    "\n",
    "    def compare_experiments(self, \n",
    "                          experiment_ids: List[str],\n",
    "                          metrics: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Compare multiple experiments\n",
    "        \n",
    "        Args:\n",
    "            experiment_ids: List of experiment IDs to compare\n",
    "            metrics: Optional list of specific metrics to compare\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with experiment comparison\n",
    "        \"\"\"\n",
    "        experiments = []\n",
    "        for exp_id in experiment_ids:\n",
    "            details = self.get_experiment_details(exp_id)\n",
    "            exp_summary = {\n",
    "                'experiment_id': exp_id,\n",
    "                'name': details['experiment_info']['name'],\n",
    "                'status': details['experiment_info']['status'],\n",
    "                'model_type': details['model_config']['model_type'],\n",
    "                'run_numbers': details['dataset_config']['run_numbers'],\n",
    "                'batch_size': details['dataset_config']['batch_size'],\n",
    "                'epochs_completed': details['training_info']['epochs_completed']\n",
    "            }\n",
    "            \n",
    "            # Add final metrics\n",
    "            if metrics:\n",
    "                for metric in metrics:\n",
    "                    exp_summary[f'final_{metric}'] = details['training_info']['final_metrics'].get(metric)\n",
    "                    \n",
    "            # Add training duration if available\n",
    "            if details['training_info']['start_time'] and details['training_info']['end_time']:\n",
    "                start = datetime.fromisoformat(details['training_info']['start_time'])\n",
    "                end = datetime.fromisoformat(details['training_info']['end_time'])\n",
    "                exp_summary['training_duration'] = (end - start).total_seconds()\n",
    "                \n",
    "            experiments.append(exp_summary)\n",
    "            \n",
    "        return pd.DataFrame(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATLASDataManager:\n",
    "    \"\"\"Manages ATLAS PHYSLITE data access\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str = \"atlas_data\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_url = \"https://opendata.cern.ch/record/80001/files\"\n",
    "        self._setup_directories()\n",
    "        self.catalog_counts = {}  # Cache for number of catalogs per run\n",
    "    \n",
    "    def get_catalog_count(self, run_number: str) -> int:\n",
    "        \"\"\"\n",
    "        Discover how many catalog files exist for a run by probing the server\n",
    "        \n",
    "        Args:\n",
    "            run_number: ATLAS run number\n",
    "            \n",
    "        Returns:\n",
    "            Number of available catalog files\n",
    "        \"\"\"\n",
    "        if run_number in self.catalog_counts:\n",
    "            return self.catalog_counts[run_number]\n",
    "            \n",
    "        padded_run = run_number.zfill(8)\n",
    "        index = 0\n",
    "        \n",
    "        while True:\n",
    "            url = f\"/record/80001/files/data16_13TeV_Run_{padded_run}_file_index.json_{index}\"\n",
    "            response = requests.head(f\"https://opendata.cern.ch{url}\")\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                break\n",
    "                \n",
    "            index += 1\n",
    "        \n",
    "        self.catalog_counts[run_number] = index\n",
    "        return index\n",
    "    \n",
    "    def download_run_catalog(self, run_number: str, index: int = 0) -> Optional[Path]:\n",
    "        \"\"\"\n",
    "        Download a specific run catalog file.\n",
    "        \n",
    "        Args:\n",
    "            run_number: ATLAS run number\n",
    "            index: Catalog index\n",
    "            \n",
    "        Returns:\n",
    "            Path to the downloaded catalog file or None if file doesn't exist\n",
    "        \"\"\"\n",
    "        padded_run = run_number.zfill(8)\n",
    "        url = f\"/record/80001/files/data16_13TeV_Run_{padded_run}_file_index.json_{index}\"\n",
    "        output_path = self.base_dir / \"catalogs\" / f\"Run_{run_number}_catalog_{index}.root\"\n",
    "        \n",
    "        try:\n",
    "            if self._download_file(url, output_path, f\"Downloading catalog {index} for Run {run_number}\"):\n",
    "                return output_path\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download catalog {index} for run {run_number}: {str(e)}\")\n",
    "            if output_path.exists():\n",
    "                output_path.unlink()  # Clean up partial download\n",
    "            return None\n",
    "    \n",
    "    # def get_run_info(self, run_number: str) -> Dict:\n",
    "    #     \"\"\"\n",
    "    #     Get information about available catalogs for a run\n",
    "        \n",
    "    #     Returns:\n",
    "    #         Dictionary containing run information including number of catalogs\n",
    "    #     \"\"\"\n",
    "    #     n_catalogs = self.get_catalog_count(run_number)\n",
    "        \n",
    "    #     # Test download of first catalog to get sample info\n",
    "    #     sample_path = self.download_run_catalog(run_number, 0)\n",
    "    #     sample_info = {}\n",
    "        \n",
    "    #     if sample_path and sample_path.exists():\n",
    "    #         try:\n",
    "    #             with uproot.open(sample_path) as file:\n",
    "    #                 tree = file[\"CollectionTree;1\"]\n",
    "    #                 sample_info = {\n",
    "    #                     'events_per_catalog': len(tree),\n",
    "    #                     'estimated_total_events': len(tree) * n_catalogs\n",
    "    #                 }\n",
    "    #         finally:\n",
    "    #             sample_path.unlink()  # Clean up sample file\n",
    "        \n",
    "    #     return {\n",
    "    #         'run_number': run_number,\n",
    "    #         'n_catalogs': n_catalogs,\n",
    "    #         **sample_info\n",
    "    #     }\n",
    "    \n",
    "    # def iter_catalogs(self, run_number: str, start_index: int = 0, end_index: Optional[int] = None):\n",
    "    #     \"\"\"\n",
    "    #     Iterator that yields catalogs for a run one at a time, cleaning up after each\n",
    "        \n",
    "    #     Args:\n",
    "    #         run_number: Run number to process\n",
    "    #         start_index: Starting catalog index\n",
    "    #         end_index: Optional ending catalog index (exclusive)\n",
    "            \n",
    "    #     Yields:\n",
    "    #         Tuple of (index, catalog_path)\n",
    "    #     \"\"\"\n",
    "    #     if end_index is None:\n",
    "    #         end_index = self.get_catalog_count(run_number)\n",
    "        \n",
    "    #     for index in range(start_index, end_index):\n",
    "    #         catalog_path = self.download_run_catalog(run_number, index)\n",
    "    #         if catalog_path and catalog_path.exists():\n",
    "    #             try:\n",
    "    #                 yield index, catalog_path\n",
    "    #             finally:\n",
    "    #                 catalog_path.unlink()  # Clean up after processing\n",
    "    \n",
    "    def _setup_directories(self):\n",
    "        \"\"\"Create necessary directory structure\"\"\"\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        (self.base_dir / \"catalogs\").mkdir(exist_ok=True)\n",
    "\n",
    "    \n",
    "    def _download_file(self, url: str, output_path: Path, desc: str = None) -> bool:\n",
    "        \"\"\"Download a single file if it doesn't exist\"\"\"\n",
    "        if output_path.exists():\n",
    "            return False\n",
    "        \n",
    "        print(f\"Downloading file: {url}\")    \n",
    "        response = requests.get(f\"https://opendata.cern.ch{url}\", stream=True)\n",
    "        if response.status_code == 200:\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(output_path, 'wb') as f, tqdm(\n",
    "                desc=desc,\n",
    "                total=total_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    size = f.write(data)\n",
    "                    pbar.update(size)\n",
    "            return True\n",
    "        else:\n",
    "            raise Exception(f\"Download failed with status code: {response.status_code}\")\n",
    "    \n",
    "    def get_run_catalog_path(self, run_number: str, index: int = 0) -> Path:\n",
    "        \"\"\"Get path to a run catalog file\"\"\"\n",
    "        return self.base_dir / \"catalogs\" / f\"Run_{run_number}_catalog_{index}.root\"\n",
    "    \n",
    "    # def verify_catalog_file(self, file_path: Path) -> bool:\n",
    "    #     \"\"\"Verify that a catalog file contains valid track data\"\"\"\n",
    "    #     try:\n",
    "    #         with uproot.open(file_path) as file:\n",
    "    #             if \"CollectionTree;1\" not in file:\n",
    "    #                 print(f\"No CollectionTree found in {file_path}\")\n",
    "    #                 return False\n",
    "                \n",
    "    #             tree = file[\"CollectionTree;1\"]\n",
    "    #             required_branches = [\n",
    "    #                 \"InDetTrackParticlesAuxDyn.d0\",\n",
    "    #                 \"InDetTrackParticlesAuxDyn.z0\",\n",
    "    #                 \"InDetTrackParticlesAuxDyn.phi\",\n",
    "    #                 \"InDetTrackParticlesAuxDyn.theta\",\n",
    "    #                 \"InDetTrackParticlesAuxDyn.qOverP\"\n",
    "    #             ]\n",
    "                \n",
    "    #             for branch in required_branches:\n",
    "    #                 if branch not in tree:\n",
    "    #                     print(f\"Missing required branch: {branch}\")\n",
    "    #                     return False\n",
    "                \n",
    "    #             # Try reading some data\n",
    "    #             try:\n",
    "    #                 data = tree[\"InDetTrackParticlesAuxDyn.d0\"].array(library=\"np\")\n",
    "    #                 if len(data) == 0:\n",
    "    #                     print(f\"No events found in {file_path}\")\n",
    "    #                     return False\n",
    "    #             except Exception as e:\n",
    "    #                 print(f\"Error reading data: {str(e)}\")\n",
    "    #                 return False\n",
    "                \n",
    "    #             return True\n",
    "                \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error verifying file {file_path}: {str(e)}\")\n",
    "    #         return False\n",
    "    \n",
    "    # def get_stats(self) -> Dict:\n",
    "    #     \"\"\"\n",
    "    #     Get statistics about downloaded catalog files.\n",
    "        \n",
    "    #     Returns:\n",
    "    #         Dictionary containing catalog statistics\n",
    "    #     \"\"\"\n",
    "    #     catalog_dir = self.base_dir / \"catalogs\"\n",
    "    #     catalog_files = list(catalog_dir.glob(\"*.root\"))\n",
    "        \n",
    "    #     total_events = 0\n",
    "    #     total_tracks = 0\n",
    "        \n",
    "    #     # Calculate events and tracks from catalogs\n",
    "    #     for catalog in catalog_files:\n",
    "    #         try:\n",
    "    #             with uproot.open(catalog) as file:\n",
    "    #                 if \"CollectionTree;1\" in file:\n",
    "    #                     tree = file[\"CollectionTree;1\"]\n",
    "    #                     total_events += len(tree)\n",
    "                        \n",
    "    #                     # Get track count from d0 branch\n",
    "    #                     try:\n",
    "    #                         tracks = tree[\"InDetTrackParticlesAuxDyn.d0\"].array(library=\"np\")\n",
    "    #                         total_tracks += sum(len(t) for t in tracks)\n",
    "    #                     except Exception:\n",
    "    #                         pass\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"Warning: Could not read stats from {catalog}: {e}\")\n",
    "        \n",
    "    #     stats = {\n",
    "    #         \"catalogs\": len(catalog_files),\n",
    "    #         \"total_events\": total_events,\n",
    "    #         \"total_tracks\": total_tracks,\n",
    "    #         \"total_size\": sum(f.stat().st_size for f in catalog_files) / (1024 * 1024 * 1024)  # in GB\n",
    "    #     }\n",
    "    #     return stats\n",
    "    \n",
    "    # def print_status(self):\n",
    "    #     \"\"\"Print current status of catalogs\"\"\"\n",
    "    #     stats = self.get_stats()\n",
    "    #     print(f\"=== ATLAS Data Status ===\")\n",
    "    #     print(f\"Number of catalogs: {stats['catalogs']}\")\n",
    "    #     print(f\"Total events: {stats['total_events']:,}\")\n",
    "    #     print(f\"Total tracks: {stats['total_tracks']:,}\")\n",
    "    #     print(f\"Total data size: {stats['total_size']:.2f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectionConfig:\n",
    "    \"\"\"Configuration for track and event selections\"\"\"\n",
    "    def __init__(self,\n",
    "                 # Required parameters\n",
    "                 max_tracks_per_event: int,\n",
    "                 min_tracks_per_event: int,\n",
    "                 # Optional selection criteria\n",
    "                 track_selections: Optional[Dict[str, Union[float, Tuple[float, float]]]] = None,\n",
    "                 event_selections: Optional[Dict[str, Union[float, Tuple[float, float]]]] = None):\n",
    "        # Required parameters\n",
    "        self.max_tracks_per_event = max_tracks_per_event\n",
    "        self.min_tracks_per_event = min_tracks_per_event\n",
    "        \n",
    "        # Optional selections\n",
    "        self.track_selections = track_selections or {}\n",
    "        self.event_selections = event_selections or {}\n",
    "        \n",
    "    def apply_track_selections(self, track_features: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Apply selections to individual tracks\"\"\"\n",
    "        # Start with all tracks\n",
    "        mask = np.ones(len(next(iter(track_features.values()))), dtype=bool)\n",
    "        \n",
    "        # Map feature names to indices\n",
    "        feature_map = {\n",
    "            'pt': track_features['pt'],\n",
    "            'eta': track_features['eta'],\n",
    "            'phi': track_features['phi'],\n",
    "            'd0': track_features['d0'],\n",
    "            'z0': track_features['z0'],\n",
    "            'chi2_per_ndof': track_features['chi2_per_ndof']\n",
    "        }\n",
    "        \n",
    "        # Apply each selection\n",
    "        for feature, criteria in self.track_selections.items():\n",
    "            if feature not in feature_map:\n",
    "                continue\n",
    "                \n",
    "            feature_values = feature_map[feature]\n",
    "            if isinstance(criteria, tuple):\n",
    "                min_val, max_val = criteria\n",
    "                if min_val is not None:\n",
    "                    mask &= (feature_values >= min_val)\n",
    "                if max_val is not None:\n",
    "                    mask &= (feature_values <= max_val)\n",
    "            else:\n",
    "                mask &= (feature_values >= criteria)\n",
    "                \n",
    "        return mask\n",
    "    \n",
    "    def apply_event_selections(self, event_features: Dict[str, np.ndarray]) -> bool:\n",
    "        \"\"\"Apply selections to entire events\"\"\"\n",
    "        # First check minimum tracks requirement\n",
    "        n_tracks = len(event_features.get('pt', []))\n",
    "        if n_tracks < self.min_tracks_per_event:\n",
    "            return False\n",
    "            \n",
    "        # Apply event-level selections\n",
    "        for feature, criteria in self.event_selections.items():\n",
    "            if feature not in event_features:\n",
    "                continue\n",
    "                \n",
    "            value = event_features[feature]\n",
    "            if isinstance(criteria, tuple):\n",
    "                min_val, max_val = criteria\n",
    "                if min_val is not None and value < min_val:\n",
    "                    return False\n",
    "                if max_val is not None and value > max_val:\n",
    "                    return False\n",
    "            else:\n",
    "                if value < criteria:\n",
    "                    return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedDatasetManager:\n",
    "    \"\"\"Manages dataset creation from ATLAS PHYSLITE data\"\"\"\n",
    "    def __init__(self, \n",
    "                 run_numbers: List[str],\n",
    "                 catalog_limit: Optional[int] = None,\n",
    "                 base_dir: str = \"atlas_data\",\n",
    "                 batch_size: int = 1000,\n",
    "                 cache_size: Optional[int] = None,\n",
    "                 shuffle_buffer: int = 10000,\n",
    "                 validation_fraction: float = 0.2):\n",
    "        self.run_numbers = run_numbers\n",
    "        self.catalog_limit = catalog_limit\n",
    "        self.batch_size = batch_size\n",
    "        self.cache_size = cache_size\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.atlas_manager = ATLASDataManager(base_dir)\n",
    "        self.normalization_params = None\n",
    "        \n",
    "    # def _read_catalog_data(self, catalog_path: Path) -> Dict[str, np.ndarray]:\n",
    "    #     \"\"\"Read track data from a catalog file, maintaining event structure\"\"\"\n",
    "    #     try:\n",
    "    #         print(f\"\\nReading catalog: {catalog_path}\")\n",
    "    #         with uproot.open(catalog_path) as file:\n",
    "    #             tree = file[\"CollectionTree;1\"]\n",
    "    #             events = []\n",
    "                \n",
    "    #             # Process in chunks to manage memory\n",
    "    #             for arrays in tree.iterate([\n",
    "    #                 \"InDetTrackParticlesAuxDyn.d0\",\n",
    "    #                 \"InDetTrackParticlesAuxDyn.z0\",\n",
    "    #                 \"InDetTrackParticlesAuxDyn.phi\",\n",
    "    #                 \"InDetTrackParticlesAuxDyn.theta\",\n",
    "    #                 \"InDetTrackParticlesAuxDyn.qOverP\",\n",
    "    #                 \"InDetTrackParticlesAuxDyn.chiSquared\",\n",
    "    #                 \"InDetTrackParticlesAuxDyn.numberDoF\"\n",
    "    #             ], library=\"np\"):\n",
    "    #                 for evt_idx in range(len(arrays[\"InDetTrackParticlesAuxDyn.d0\"])):\n",
    "    #                     event_tracks = {\n",
    "    #                         'd0': arrays[\"InDetTrackParticlesAuxDyn.d0\"][evt_idx],\n",
    "    #                         'z0': arrays[\"InDetTrackParticlesAuxDyn.z0\"][evt_idx],\n",
    "    #                         'phi': arrays[\"InDetTrackParticlesAuxDyn.phi\"][evt_idx],\n",
    "    #                         'theta': arrays[\"InDetTrackParticlesAuxDyn.theta\"][evt_idx],\n",
    "    #                         'qOverP': arrays[\"InDetTrackParticlesAuxDyn.qOverP\"][evt_idx],\n",
    "    #                         'chiSquared': arrays[\"InDetTrackParticlesAuxDyn.chiSquared\"][evt_idx],\n",
    "    #                         'numberDoF': arrays[\"InDetTrackParticlesAuxDyn.numberDoF\"][evt_idx]\n",
    "    #                     }\n",
    "                        \n",
    "    #                     # Calculate derived quantities for this event's tracks\n",
    "    #                     pt, eta, chi2_per_ndof = self._calculate_track_quantities(event_tracks)\n",
    "                        \n",
    "    #                     # Select top N tracks by pT\n",
    "    #                     track_features = np.column_stack([pt, eta, event_tracks['phi'], \n",
    "    #                                                     event_tracks['d0'], event_tracks['z0'], \n",
    "    #                                                     chi2_per_ndof])\n",
    "                        \n",
    "    #                     events.append(track_features)\n",
    "                \n",
    "    #             return events\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error reading catalog {catalog_path}: {str(e)}\")\n",
    "    #         return None\n",
    "    \n",
    "    def event_generator(self, selection_config: SelectionConfig):\n",
    "        \"\"\"\n",
    "        Generator that yields processed events one at a time\n",
    "        \n",
    "        Args:\n",
    "            selection_config: SelectionConfig object containing selection criteria\n",
    "            \n",
    "        Yields:\n",
    "            Numpy array of shape (max_tracks_per_event, n_features)\n",
    "        \"\"\"\n",
    "        for run_number in self.run_numbers:\n",
    "            print(f\"\\nProcessing run {run_number}\")\n",
    "            catalog_idx = 0\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    # Get or download catalog\n",
    "                    catalog_path = self.atlas_manager.get_run_catalog_path(run_number, catalog_idx)\n",
    "                    if not catalog_path.exists():\n",
    "                        catalog_path = self.atlas_manager.download_run_catalog(run_number, catalog_idx)\n",
    "                        if catalog_path is None:\n",
    "                            break  # No more catalogs for this run\n",
    "                    \n",
    "                    print(f\"Processing catalog {catalog_idx}\")\n",
    "                    \n",
    "                    # Open file and process events\n",
    "                    with uproot.open(catalog_path) as file:\n",
    "                        tree = file[\"CollectionTree;1\"]\n",
    "                        \n",
    "                        # Define all branches we need\n",
    "                        branches = [\n",
    "                            \"InDetTrackParticlesAuxDyn.d0\",\n",
    "                            \"InDetTrackParticlesAuxDyn.z0\",\n",
    "                            \"InDetTrackParticlesAuxDyn.phi\",\n",
    "                            \"InDetTrackParticlesAuxDyn.theta\",\n",
    "                            \"InDetTrackParticlesAuxDyn.qOverP\",\n",
    "                            \"InDetTrackParticlesAuxDyn.chiSquared\",\n",
    "                            \"InDetTrackParticlesAuxDyn.numberDoF\"\n",
    "                        ]\n",
    "                        \n",
    "                        # Process in chunks to manage memory\n",
    "                        for arrays in tree.iterate(branches, library=\"np\", step_size=1000):\n",
    "                            n_events = len(arrays[\"InDetTrackParticlesAuxDyn.d0\"])\n",
    "                            \n",
    "                            for evt_idx in range(n_events):\n",
    "                                # Extract raw event data - directly use the numpy array for this event\n",
    "                                raw_event = {\n",
    "                                    'd0': arrays[\"InDetTrackParticlesAuxDyn.d0\"][evt_idx],  # Already a numpy array\n",
    "                                    'z0': arrays[\"InDetTrackParticlesAuxDyn.z0\"][evt_idx],\n",
    "                                    'phi': arrays[\"InDetTrackParticlesAuxDyn.phi\"][evt_idx],\n",
    "                                    'theta': arrays[\"InDetTrackParticlesAuxDyn.theta\"][evt_idx],\n",
    "                                    'qOverP': arrays[\"InDetTrackParticlesAuxDyn.qOverP\"][evt_idx],\n",
    "                                    'chiSquared': arrays[\"InDetTrackParticlesAuxDyn.chiSquared\"][evt_idx],\n",
    "                                    'numberDoF': arrays[\"InDetTrackParticlesAuxDyn.numberDoF\"][evt_idx]\n",
    "                                }\n",
    "                                \n",
    "                                # Skip empty events (no tracks)\n",
    "                                if len(raw_event['d0']) == 0:\n",
    "                                    continue\n",
    "                                    \n",
    "                                processed_event = self._process_event(raw_event, selection_config)\n",
    "                                if processed_event is not None:\n",
    "                                    yield processed_event\n",
    "                    \n",
    "                    catalog_idx += 1\n",
    "\n",
    "                    if self.catalog_limit and catalog_idx > self.catalog_limit:\n",
    "                        break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing catalog {catalog_idx} of run {run_number}: {str(e)}\")\n",
    "                    break\n",
    "                    \n",
    "                finally:\n",
    "                    # Clean up catalog file\n",
    "                    print(\"Not cleaning up catalog files for faster testing\")\n",
    "                    # if catalog_path and catalog_path.exists():\n",
    "                    #     catalog_path.unlink()\n",
    "\n",
    "    def _process_event(self, \n",
    "                    event_tracks: Dict[str, np.ndarray], \n",
    "                    selection_config: SelectionConfig) -> Optional[np.ndarray]:\n",
    "        \"\"\"Process a single event's tracks with selections\"\"\"\n",
    "        # Calculate derived quantities\n",
    "        track_features = {\n",
    "            'pt': np.abs(1.0 / (event_tracks['qOverP'] * 1000)) * np.sin(event_tracks['theta']),\n",
    "            'eta': -np.log(np.tan(event_tracks['theta'] / 2)),\n",
    "            'phi': event_tracks['phi'],\n",
    "            'd0': event_tracks['d0'],\n",
    "            'z0': event_tracks['z0'],\n",
    "            'chi2_per_ndof': event_tracks['chiSquared'] / event_tracks['numberDoF']\n",
    "        }\n",
    "        \n",
    "        # Add event-level features\n",
    "        event_features = {\n",
    "            'n_total_tracks': len(track_features['pt']),\n",
    "            'mean_pt': np.mean(track_features['pt']),\n",
    "            'max_pt': np.max(track_features['pt'])\n",
    "        }\n",
    "        \n",
    "        # Apply event-level selections first (cheaper)\n",
    "        if not selection_config.apply_event_selections(event_features):\n",
    "            return None\n",
    "        \n",
    "        # Apply track-level selections\n",
    "        good_tracks_mask = selection_config.apply_track_selections(track_features)\n",
    "        good_tracks = np.where(good_tracks_mask)[0]\n",
    "        \n",
    "        # Check if we still have enough tracks\n",
    "        if len(good_tracks) < selection_config.min_tracks_per_event:\n",
    "            return None\n",
    "        \n",
    "        # Sort by pT and take top N tracks\n",
    "        track_pts = track_features['pt'][good_tracks]\n",
    "        sorted_indices = np.argsort(track_pts)[::-1]\n",
    "        top_tracks = good_tracks[sorted_indices[:selection_config.max_tracks_per_event]]\n",
    "        \n",
    "        # Create feature array\n",
    "        features = np.column_stack([\n",
    "            track_features['pt'][top_tracks],\n",
    "            track_features['eta'][top_tracks],\n",
    "            track_features['phi'][top_tracks],\n",
    "            track_features['d0'][top_tracks],\n",
    "            track_features['z0'][top_tracks],\n",
    "            track_features['chi2_per_ndof'][top_tracks]\n",
    "        ])\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if len(features) < selection_config.max_tracks_per_event:\n",
    "            padding = np.zeros((selection_config.max_tracks_per_event - len(features), 6))\n",
    "            features = np.vstack([features, padding])\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def create_streaming_dataset(self, \n",
    "                               selection_config: SelectionConfig,\n",
    "                               compute_normalizing_stats: bool = True) -> tf.data.Dataset:\n",
    "        \"\"\"\n",
    "        Create a streaming dataset from the event generator\n",
    "        \n",
    "        Args:\n",
    "            selection_config: Optional dictionary of selection criteria\n",
    "            compute_normalizing_stats: Whether to compute normalization parameters\n",
    "                                     from a sample of the data\n",
    "        \"\"\"\n",
    "        print(\"\\nCreating streaming dataset\")\n",
    "        # Create dataset from generator\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: self.event_generator(selection_config),\n",
    "            output_signature=tf.TensorSpec(\n",
    "                shape=(selection_config.max_tracks_per_event, 6),\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\"Normalizing datatset\")\n",
    "        \n",
    "        # Compute normalization parameters if needed\n",
    "        if compute_normalizing_stats and self.normalization_params is None:\n",
    "            print(\"\\nComputing normalization parameters from sample...\")\n",
    "            sample_size = 10000\n",
    "            sample_events = []\n",
    "            for event in self.event_generator(selection_config):\n",
    "                sample_events.append(event)\n",
    "                if len(sample_events) >= sample_size:\n",
    "                    break\n",
    "            \n",
    "            if sample_events:\n",
    "                sample_data = np.vstack(sample_events)\n",
    "                self.normalization_params = {\n",
    "                    'means': np.mean(sample_data, axis=0),\n",
    "                    'stds': np.std(sample_data, axis=0)\n",
    "                }\n",
    "        \n",
    "        # Add normalization if parameters are available\n",
    "        if self.normalization_params is not None:\n",
    "            dataset = dataset.map(\n",
    "                lambda x: (x - self.normalization_params['means']) / self.normalization_params['stds']\n",
    "            )\n",
    "        \n",
    "        # Add shuffling and batching\n",
    "        if self.cache_size:\n",
    "            dataset = dataset.cache()\n",
    "        dataset = dataset.shuffle(self.shuffle_buffer)\n",
    "        dataset = dataset.batch(self.batch_size, drop_remainder=True)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    # def create_train_val_test_datasets(self, \n",
    "    #                                  selection_config: SelectionConfig,\n",
    "    #                                  val_fraction: float = 0.1,\n",
    "    #                                  test_fraction: float = 0.1) -> Tuple[tf.data.Dataset, \n",
    "    #                                                                     tf.data.Dataset, \n",
    "    #                                                                     tf.data.Dataset]:\n",
    "    #     \"\"\"Create training, validation, and test datasets\"\"\"\n",
    "    #     # Create base dataset\n",
    "    #     full_dataset = self.create_streaming_dataset(\n",
    "    #         selection_config=selection_config,\n",
    "    #         compute_normalizing_stats=True\n",
    "    #     )\n",
    "\n",
    "    #     print(\"Splitting dataset\")\n",
    "        \n",
    "    #     # Calculate split sizes\n",
    "    #     val_size = int(val_fraction * 100)\n",
    "    #     test_size = int(test_fraction * 100)\n",
    "        \n",
    "    #     # Split the dataset\n",
    "    #     test_dataset = full_dataset.take(test_size)\n",
    "    #     remaining = full_dataset.skip(test_size)\n",
    "    #     val_dataset = remaining.take(val_size)\n",
    "    #     train_dataset = remaining.skip(val_size)\n",
    "        \n",
    "    #     return train_dataset, val_dataset, test_dataset\n",
    "        \n",
    "    # def create_datasets(self, \n",
    "    #                    max_tracks_per_event: int = 30,\n",
    "    #                    min_tracks_per_event: int = 5,\n",
    "    #                    track_selections: Optional[Dict] = None,\n",
    "    #                    event_selections: Optional[Dict] = None):\n",
    "    #     \"\"\"Main entry point\"\"\"\n",
    "    #     # Create SelectionConfig object\n",
    "    #     selection_config = SelectionConfig(\n",
    "    #         max_tracks_per_event=max_tracks_per_event,\n",
    "    #         min_tracks_per_event=min_tracks_per_event,\n",
    "    #         track_selections=track_selections,\n",
    "    #         event_selections=event_selections\n",
    "    #     )\n",
    "        \n",
    "    #     # Create streaming dataset\n",
    "    #     return self.create_train_val_test_datasets(selection_config)\n",
    "    \n",
    "    # def _calculate_track_features(self, raw_event: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    #     \"\"\"\n",
    "    #     Calculate all track-level features from raw event data, safely handling edge cases\n",
    "        \n",
    "    #     Args:\n",
    "    #         raw_event: Dictionary containing raw track parameters\n",
    "            \n",
    "    #     Returns:\n",
    "    #         Dictionary of calculated track features\n",
    "    #     \"\"\"\n",
    "    #     try:\n",
    "    #         # Check for empty or invalid input\n",
    "    #         if any(len(arr) == 0 for arr in raw_event.values()):\n",
    "    #             return {\n",
    "    #                 'pt': np.array([]),\n",
    "    #                 'eta': np.array([]),\n",
    "    #                 'phi': np.array([]),\n",
    "    #                 'd0': np.array([]),\n",
    "    #                 'z0': np.array([]),\n",
    "    #                 'chi2_per_ndof': np.array([])\n",
    "    #             }\n",
    "                \n",
    "    #         # Handle potential division by zero in qOverP\n",
    "    #         qOverP = raw_event['qOverP']\n",
    "    #         mask = qOverP != 0\n",
    "    #         pt = np.zeros_like(qOverP)\n",
    "    #         pt[mask] = np.abs(1.0 / (qOverP[mask] * 1000)) * np.sin(raw_event['theta'][mask])\n",
    "            \n",
    "    #         # Calculate eta safely\n",
    "    #         theta = raw_event['theta']\n",
    "    #         eta = np.zeros_like(theta)\n",
    "    #         valid_theta = (theta > 0) & (theta < np.pi)  # Avoid theta = 0 or pi\n",
    "    #         eta[valid_theta] = -np.log(np.tan(theta[valid_theta] / 2))\n",
    "            \n",
    "    #         # Calculate chi2/ndof safely\n",
    "    #         ndof = raw_event['numberDoF']\n",
    "    #         chi2 = raw_event['chiSquared']\n",
    "    #         chi2_per_ndof = np.zeros_like(ndof)\n",
    "    #         valid_ndof = ndof > 0\n",
    "    #         chi2_per_ndof[valid_ndof] = chi2[valid_ndof] / ndof[valid_ndof]\n",
    "            \n",
    "    #         return {\n",
    "    #             'pt': pt,\n",
    "    #             'eta': eta,\n",
    "    #             'phi': raw_event['phi'],\n",
    "    #             'd0': raw_event['d0'],\n",
    "    #             'z0': raw_event['z0'],\n",
    "    #             'chi2_per_ndof': chi2_per_ndof\n",
    "    #         }\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Warning: Error calculating track features: {str(e)}\")\n",
    "    #         print(f\"Raw event shapes: {[(k, v.shape) for k, v in raw_event.items()]}\")\n",
    "    #         # Return empty arrays\n",
    "    #         return {\n",
    "    #             'pt': np.array([]),\n",
    "    #             'eta': np.array([]),\n",
    "    #             'phi': np.array([]),\n",
    "    #             'd0': np.array([]),\n",
    "    #             'z0': np.array([]),\n",
    "    #             'chi2_per_ndof': np.array([])\n",
    "    #         }\n",
    "\n",
    "    # def _calculate_event_features(self, track_features: Dict[str, np.ndarray]) -> Dict[str, Union[float, int]]:\n",
    "    #     \"\"\"\n",
    "    #     Calculate event-level features from track features, safely handling empty events\n",
    "        \n",
    "    #     Args:\n",
    "    #         track_features: Dictionary of track-level features\n",
    "            \n",
    "    #     Returns:\n",
    "    #         Dictionary of event-level features\n",
    "    #     \"\"\"\n",
    "    #     pt = track_features['pt']\n",
    "    #     eta = track_features['eta']\n",
    "        \n",
    "    #     # Handle empty events safely\n",
    "    #     if len(pt) == 0:\n",
    "    #         return {\n",
    "    #             'n_total_tracks': 0,\n",
    "    #             'mean_pt': 0.0,\n",
    "    #             'max_pt': 0.0,\n",
    "    #             'min_pt': 0.0,\n",
    "    #             'eta_spread': 0.0,\n",
    "    #             'total_pt': 0.0,\n",
    "    #             'n_high_pt_tracks': 0,\n",
    "    #             'n_central_tracks': 0\n",
    "    #         }\n",
    "        \n",
    "    #     # Calculate statistics for non-empty events\n",
    "    #     try:\n",
    "    #         return {\n",
    "    #             'n_total_tracks': len(pt),\n",
    "    #             'mean_pt': float(np.mean(pt)),\n",
    "    #             'max_pt': float(np.max(pt)),\n",
    "    #             'min_pt': float(np.min(pt)),\n",
    "    #             'eta_spread': float(np.std(eta)) if len(eta) > 1 else 0.0,\n",
    "    #             'total_pt': float(np.sum(pt)),\n",
    "    #             'n_high_pt_tracks': int(np.sum(pt > 10.0)),\n",
    "    #             'n_central_tracks': int(np.sum(np.abs(eta) < 1.5))\n",
    "    #         }\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Warning: Error calculating event features: {str(e)}\")\n",
    "    #         print(f\"Track features shapes: {[(k, v.shape) for k, v in track_features.items()]}\")\n",
    "    #         # Return safe default values\n",
    "    #         return {\n",
    "    #             'n_total_tracks': len(pt),\n",
    "    #             'mean_pt': 0.0,\n",
    "    #             'max_pt': 0.0,\n",
    "    #             'min_pt': 0.0,\n",
    "    #             'eta_spread': 0.0,\n",
    "    #             'total_pt': 0.0,\n",
    "    #             'n_high_pt_tracks': 0,\n",
    "    #             'n_central_tracks': 0\n",
    "    #         }\n",
    "    \n",
    "    # def compute_normalization(self, features: np.ndarray) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:\n",
    "    #     \"\"\"Compute normalization parameters from training data\"\"\"\n",
    "    #     print(\"\\nComputing normalization parameters...\")\n",
    "    #     means = np.mean(features, axis=0)\n",
    "    #     stds = np.std(features, axis=0)\n",
    "    #     self.normalization_params = {'means': means, 'stds': stds}\n",
    "    #     return self.normalization_params\n",
    "    \n",
    "    # def apply_normalization(self, features: np.ndarray) -> np.ndarray:\n",
    "    #     \"\"\"Apply normalization using computed parameters\"\"\"\n",
    "    #     if self.normalization_params is None:\n",
    "    #         raise ValueError(\"Normalization parameters not computed. Run compute_normalization first.\")\n",
    "        \n",
    "    #     normalized = (features - self.normalization_params['means']) / self.normalization_params['stds']\n",
    "    #     return normalized\n",
    "    \n",
    "    # def split_dataset(self, \n",
    "    #                     features: np.ndarray,\n",
    "    #                     train_fraction: float = 0.8,\n",
    "    #                     validation_fraction: float = 0.1\n",
    "    #                     ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    #     \"\"\"Split data into training, validation, and test sets\n",
    "        \n",
    "    #     Args:\n",
    "    #         features: Input feature array\n",
    "    #         train_fraction: Fraction of data for training (default: 0.8)\n",
    "    #         validation_fraction: Fraction of data for validation (default: 0.1)\n",
    "            \n",
    "    #     Returns:\n",
    "    #         Tuple of (train_features, val_features, test_features)\n",
    "    #     \"\"\"\n",
    "    #     print(\"\\nSplitting into train/validation/test sets...\")\n",
    "    #     total_samples = len(features)\n",
    "        \n",
    "    #     # Calculate split sizes\n",
    "    #     train_size = int(total_samples * train_fraction)\n",
    "    #     val_size = int(total_samples * validation_fraction)\n",
    "        \n",
    "    #     # Shuffle indices\n",
    "    #     indices = np.random.permutation(total_samples)\n",
    "        \n",
    "    #     # Split indices\n",
    "    #     train_indices = indices[:train_size]\n",
    "    #     val_indices = indices[train_size:train_size + val_size]\n",
    "    #     test_indices = indices[train_size + val_size:]\n",
    "        \n",
    "    #     # Create splits\n",
    "    #     train_features = features[train_indices]\n",
    "    #     val_features = features[val_indices]\n",
    "    #     test_features = features[test_indices]\n",
    "        \n",
    "    #     print(f\"Training samples: {len(train_features)}\")\n",
    "    #     print(f\"Validation samples: {len(val_features)}\")\n",
    "    #     print(f\"Test samples: {len(test_features)}\")\n",
    "        \n",
    "    #     return train_features, val_features, test_features\n",
    "\n",
    "    \n",
    "    def create_datasets(self, \n",
    "                    max_tracks_per_event: int = 30,\n",
    "                    min_tracks_per_event: int = 5,\n",
    "                    track_selections: Optional[Dict] = None,\n",
    "                    event_selections: Optional[Dict] = None,\n",
    "                    validation_fraction: float = 0.1,\n",
    "                    test_fraction: float = 0.1) -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "        \"\"\"\n",
    "        Create train/validation/test datasets with streaming and selections\n",
    "        \n",
    "        Args:\n",
    "            max_tracks_per_event: Maximum number of tracks to keep per event\n",
    "            min_tracks_per_event: Minimum number of tracks required per event\n",
    "            track_selections: Dictionary of track-level selection criteria\n",
    "            event_selections: Dictionary of event-level selection criteria\n",
    "            validation_fraction: Fraction of data for validation\n",
    "            test_fraction: Fraction of data for test set\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (train_dataset, val_dataset, test_dataset)\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"\\nInitial system state:\")\n",
    "        print_system_usage(\"Pre-processing: \")\n",
    "\n",
    "        # Create selection configuration\n",
    "        selection_config = SelectionConfig(\n",
    "            max_tracks_per_event=max_tracks_per_event,\n",
    "            min_tracks_per_event=min_tracks_per_event,\n",
    "            track_selections=track_selections,\n",
    "            event_selections=event_selections\n",
    "        )\n",
    "        \n",
    "        print(\"Creating base dataset...\")\n",
    "        base_dataset = self.create_streaming_dataset(\n",
    "            selection_config=selection_config,\n",
    "            compute_normalizing_stats=True\n",
    "        )\n",
    "\n",
    "        print(\"\\nSystem usage after dataset creation:\")\n",
    "        print_system_usage(\"Post-processing: \")\n",
    "\n",
    "        # Debug print - check if base dataset has data\n",
    "        print(\"Checking base dataset...\")\n",
    "        try:\n",
    "            for i, batch in enumerate(base_dataset):\n",
    "                print(f\"Base dataset batch {i} shape: {batch.shape}\")\n",
    "                if i == 0:  # Just check first batch\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking base dataset: {e}\")\n",
    "        \n",
    "        # Calculate split sizes (in number of batches)\n",
    "        val_size = int(validation_fraction * 100)  # Take 100 batches as reference\n",
    "        test_size = int(test_fraction * 100)\n",
    "        \n",
    "        # Split dataset\n",
    "        print(\"\\nSplitting datasets...\")\n",
    "        test_dataset = base_dataset.take(test_size)\n",
    "        print(\"Created test dataset\")\n",
    "        \n",
    "        remaining = base_dataset.skip(test_size)\n",
    "        print(\"Created remaining dataset\")\n",
    "        \n",
    "        val_dataset = remaining.take(val_size)\n",
    "        print(\"Created validation dataset\")\n",
    "        \n",
    "        train_dataset = remaining.skip(val_size)\n",
    "        print(\"Created training dataset\")\n",
    "        \n",
    "        # Cache each dataset (with unique identifiers)\n",
    "        print(\"\\nCaching datasets...\")\n",
    "        cache_dir = \"/tmp/tf_cache\"\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        test_dataset = test_dataset.cache(f\"{cache_dir}/test_cache\")\n",
    "        val_dataset = val_dataset.cache(f\"{cache_dir}/val_cache\")\n",
    "        train_dataset = train_dataset.cache(f\"{cache_dir}/train_cache\")\n",
    "\n",
    "        # Force cache loading\n",
    "        print(\"\\nForcing cache load...\")\n",
    "        for dataset, name in [(train_dataset, \"train\"), \n",
    "                            (val_dataset, \"validation\"), \n",
    "                            (test_dataset, \"test\")]:\n",
    "            print(f\"Loading {name} cache...\")\n",
    "            try:\n",
    "                for i, batch in enumerate(dataset):\n",
    "                    if i == 0:\n",
    "                        print(f\"First {name} batch shape: {batch.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {name} cache: {e}\")\n",
    "        \n",
    "        # Add shuffling and prefetching for training\n",
    "        train_dataset = train_dataset.shuffle(\n",
    "            buffer_size=self.shuffle_buffer,\n",
    "            reshuffle_each_iteration=True\n",
    "        )\n",
    "        \n",
    "        # Add prefetching to all datasets\n",
    "        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        print(\"\\nFinal system state after caching:\")\n",
    "        print_system_usage(\"Post-caching: \")\n",
    "        \n",
    "        # Print dataset sizes\n",
    "        print(\"\\nDataset splits:\")\n",
    "        print(f\"Training batches: {100 - val_size - test_size}\")\n",
    "        print(f\"Validation batches: {val_size}\")\n",
    "        print(f\"Test batches: {test_size}\")\n",
    "        \n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    # def validate_data_quality(self, data) -> Dict:\n",
    "        # \"\"\"Run basic data quality checks on either tf.data.Dataset or numpy array\n",
    "        \n",
    "        # Args:\n",
    "        #     data: Either a tf.data.Dataset or numpy array to validate\n",
    "            \n",
    "        # Returns:\n",
    "        #     Dictionary containing validation results and statistics\n",
    "        # \"\"\"\n",
    "        # feature_info = {\n",
    "        #     'pt': {'range': (0, 5000)},\n",
    "        #     'eta': {'range': (-5, 5)},\n",
    "        #     'phi': {'range': (-np.pi, np.pi)}, \n",
    "        #     'd0': {'range': (-10, 10)},\n",
    "        #     'z0': {'range': (-200, 200)},\n",
    "        #     'chi2_per_ndof': {'range': (0, 100)}\n",
    "        # }\n",
    "        \n",
    "        # validation_results = {\n",
    "        #     'total_tracks': 0,\n",
    "        #     'out_of_range_tracks': {name: 0 for name in feature_info},\n",
    "        #     'null_values': {name: 0 for name in feature_info},\n",
    "        #     'status': 'pass'\n",
    "        # }\n",
    "\n",
    "        # print(f\"\\nStarting data validation...\")\n",
    "        # print(f\"Input type: {type(data)}\")\n",
    "        \n",
    "        # # Handle both tf.data.Dataset and numpy array inputs\n",
    "        # if isinstance(data, tf.data.Dataset):\n",
    "        #     print(\"Validating TensorFlow dataset...\")\n",
    "        #     for batch in data:\n",
    "        #         if isinstance(batch, tuple):  # (input, target) pairs\n",
    "        #             batch = batch[0]  # Take input part\n",
    "        #         batch_np = batch.numpy()\n",
    "        #         validation_results['total_tracks'] += len(batch_np)\n",
    "        #         self._validate_batch(batch_np, validation_results, feature_info)\n",
    "        # else:  # Assuming numpy array\n",
    "        #     print(f\"Validating numpy array with shape: {data.shape}\")\n",
    "        #     validation_results['total_tracks'] = len(data)\n",
    "        #     self._validate_batch(data, validation_results, feature_info)\n",
    "            \n",
    "        # # Set status based on validation results\n",
    "        # for name in feature_info:\n",
    "        #     if validation_results['null_values'][name] > 0:\n",
    "        #         print(f\"Found null values in {name}\")\n",
    "        #         validation_results['status'] = 'fail'\n",
    "        #         break\n",
    "                \n",
    "        #     # Allow a small fraction (0.1%) of tracks to be out of range\n",
    "        #     out_of_range_fraction = (validation_results['out_of_range_tracks'][name] / \n",
    "        #                         validation_results['total_tracks'])\n",
    "        #     if out_of_range_fraction > 0.001:\n",
    "        #         print(f\"Too many out of range values in {name}: {out_of_range_fraction:.3%}\")\n",
    "        #         validation_results['status'] = 'warning'\n",
    "        \n",
    "        # # Add summary statistics\n",
    "        # validation_results['summary'] = {\n",
    "        #     'null_fraction': {\n",
    "        #         name: count / validation_results['total_tracks']\n",
    "        #         for name, count in validation_results['null_values'].items()\n",
    "        #     },\n",
    "        #     'out_of_range_fraction': {\n",
    "        #         name: count / validation_results['total_tracks']\n",
    "        #         for name, count in validation_results['out_of_range_tracks'].items()\n",
    "        #     }\n",
    "        # }\n",
    "        \n",
    "        # print(f\"Validation complete. Status: {validation_results['status']}\")\n",
    "        # return validation_results\n",
    "\n",
    "    # def _validate_batch(self, batch_data: np.ndarray, validation_results: Dict, feature_info: Dict):\n",
    "        # \"\"\"Helper method to validate a batch of data\n",
    "        \n",
    "        # Args:\n",
    "        #     batch_data: Numpy array of shape (batch_size, n_features)\n",
    "        #     validation_results: Dictionary to store validation results\n",
    "        #     feature_info: Dictionary containing valid ranges for each feature\n",
    "        # \"\"\"\n",
    "        # for i, (name, info) in enumerate(feature_info.items()):\n",
    "        #     feature_data = batch_data[:, i]\n",
    "            \n",
    "        #     # Check for null/nan values\n",
    "        #     null_mask = np.isnan(feature_data) | np.isinf(feature_data)\n",
    "        #     n_nulls = np.sum(null_mask)\n",
    "        #     validation_results['null_values'][name] += n_nulls\n",
    "        #     if n_nulls > 0:\n",
    "        #         print(f\"Found {n_nulls} null/inf values in {name}\")\n",
    "            \n",
    "        #     # Check value ranges\n",
    "        #     if 'range' in info:\n",
    "        #         min_val, max_val = info['range']\n",
    "        #         range_mask = (feature_data < min_val) | (feature_data > max_val)\n",
    "        #         n_out_of_range = np.sum(range_mask)\n",
    "        #         validation_results['out_of_range_tracks'][name] += n_out_of_range\n",
    "        #         if n_out_of_range > 0:\n",
    "        #             print(f\"Found {n_out_of_range} out-of-range values in {name}\")\n",
    "        \n",
    "    # def get_data_status(self) -> Dict:\n",
    "        # \"\"\"\n",
    "        # Get status of catalog files and processing configuration\n",
    "        \n",
    "        # Returns:\n",
    "        #     Dictionary containing data and processing status\n",
    "        # \"\"\"\n",
    "        # # Get basic stats from atlas manager\n",
    "        # stats = self.atlas_manager.get_stats()\n",
    "        \n",
    "        # # Add processing configuration\n",
    "        # stats.update({\n",
    "        #     \"runs\": len(self.run_numbers),\n",
    "        #     \"cache_size\": self.cache_size,\n",
    "        #     \"batch_size\": self.batch_size,\n",
    "        #     \"shuffle_buffer\": self.shuffle_buffer,\n",
    "        #     \"run_numbers\": self.run_numbers\n",
    "        # })\n",
    "        \n",
    "        # # Add average tracks per event\n",
    "        # if stats['total_events'] > 0:\n",
    "        #     stats['avg_tracks_per_event'] = stats['total_tracks'] / stats['total_events']\n",
    "        \n",
    "        # # Add data quality metrics if a dataset has been created\n",
    "        # try:\n",
    "        #     dataset = self.create_inner_track_dataset()\n",
    "        #     validation_results = self.validate_data_quality(dataset)\n",
    "        #     stats['data_quality'] = {\n",
    "        #         'status': validation_results['status'],\n",
    "        #         'null_fractions': validation_results['summary']['null_fraction'],\n",
    "        #         'out_of_range_fractions': validation_results['summary']['out_of_range_fraction']\n",
    "        #     }\n",
    "        # except Exception as e:\n",
    "        #     stats['data_quality'] = {'status': 'unknown', 'error': str(e)}\n",
    "        \n",
    "        # return stats\n",
    "        \n",
    "    # def get_feature_names(self) -> List[str]:\n",
    "    #     \"\"\"Get list of feature names in order\"\"\"\n",
    "    #     return [\n",
    "    #         'pT',\n",
    "    #         'eta',\n",
    "    #         'phi',\n",
    "    #         'd0',\n",
    "    #         'z0',\n",
    "    #         'chi2_per_ndof'\n",
    "    #     ]\n",
    "    \n",
    "    # def get_feature_info(self) -> Dict:\n",
    "    #     \"\"\"Get information about features\"\"\"\n",
    "    #     return {\n",
    "    #         'pT': {'units': 'GeV', 'range': (0, 5000), 'description': 'Transverse momentum'},\n",
    "    #         'eta': {'units': None, 'range': (-5, 5), 'description': 'Pseudorapidity'},\n",
    "    #         'phi': {'units': 'rad', 'range': (-np.pi, np.pi), 'description': 'Azimuthal angle'},\n",
    "    #         'd0': {'units': 'mm', 'range': (-10, 10), 'description': 'Transverse impact parameter'},\n",
    "    #         'z0': {'units': 'mm', 'range': (-200, 200), 'description': 'Longitudinal impact parameter'},\n",
    "    #         'chi2_per_ndof': {'units': None, 'range': (0, 100), 'description': 'Track fit quality'}\n",
    "    #     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    \"\"\"Base class for all models\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "    @abstractmethod\n",
    "    def build(self, input_shape: tuple) -> None:\n",
    "        \"\"\"Build the model architecture\"\"\"\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"Get model configuration\"\"\"\n",
    "        pass\n",
    "        \n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save model weights and config\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built yet\")\n",
    "        self.model.save(path)\n",
    "        \n",
    "    def load(self, path: str) -> None:\n",
    "        \"\"\"Load model weights\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built yet\")\n",
    "        self.model.load_weights(path)\n",
    "\n",
    "# class AutoEncoder(BaseModel):\n",
    "#     \"\"\"Basic autoencoder with configurable architecture\"\"\"\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         input_dim: int,\n",
    "#         latent_dim: int,\n",
    "#         encoder_layers: List[int],\n",
    "#         decoder_layers: List[int],\n",
    "#         quant_bits: Optional[int] = None,\n",
    "#         activation: str = 'relu',\n",
    "#         name: str = 'autoencoder'\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Initialize autoencoder model\n",
    "        \n",
    "#         Args:\n",
    "#             input_dim: Dimension of input data\n",
    "#             latent_dim: Dimension of latent space\n",
    "#             encoder_layers: List of layer sizes for encoder\n",
    "#             decoder_layers: List of layer sizes for decoder\n",
    "#             quant_bits: Number of bits for quantization (optional)\n",
    "#             activation: Activation function to use\n",
    "#             name: Model name\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.encoder_layers = encoder_layers\n",
    "#         self.decoder_layers = decoder_layers\n",
    "#         self.quant_bits = quant_bits\n",
    "#         self.activation = activation\n",
    "#         self.name = name\n",
    "\n",
    "class AutoEncoder(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        latent_dim: int,\n",
    "        encoder_layers: List[int],\n",
    "        decoder_layers: List[int],\n",
    "        quant_bits: Optional[int] = None,\n",
    "        activation: str = 'relu',\n",
    "        name: str = 'autoencoder'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.decoder_layers = decoder_layers\n",
    "        self.quant_bits = quant_bits\n",
    "        self.activation = activation\n",
    "        self.name = name\n",
    "        \n",
    "    def build(self, input_shape: tuple = None) -> None:\n",
    "        \"\"\"Build encoder and decoder networks\"\"\"\n",
    "        if input_shape is None:\n",
    "            input_shape = (self.input_dim,)\n",
    "        \n",
    "        # Input layer\n",
    "        inputs = keras.Input(shape=input_shape, name='input_layer')\n",
    "        \n",
    "        # Create encoder layers\n",
    "        encoder_layers = []\n",
    "        for i, units in enumerate(self.encoder_layers):\n",
    "            if self.quant_bits:\n",
    "                # Dense layer\n",
    "                dense = QDense(\n",
    "                    units,\n",
    "                    kernel_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                    bias_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                    name=f'encoder_dense_{i}'\n",
    "                )\n",
    "                encoder_layers.append(dense)\n",
    "                \n",
    "                # Activation layer\n",
    "                activation = QActivation(\n",
    "                    quantized_relu(self.quant_bits),\n",
    "                    name=f'encoder_activation_{i}'\n",
    "                )\n",
    "                encoder_layers.append(activation)\n",
    "                \n",
    "                # Batch normalization\n",
    "                batch_norm = keras.layers.BatchNormalization(\n",
    "                    name=f'encoder_bn_{i}'\n",
    "                )\n",
    "                encoder_layers.append(batch_norm)\n",
    "            else:\n",
    "                dense = keras.layers.Dense(\n",
    "                    units,\n",
    "                    name=f'encoder_dense_{i}'\n",
    "                )\n",
    "                encoder_layers.append(dense)\n",
    "                \n",
    "                activation = keras.layers.Activation(\n",
    "                    self.activation,\n",
    "                    name=f'encoder_activation_{i}'\n",
    "                )\n",
    "                encoder_layers.append(activation)\n",
    "                \n",
    "                batch_norm = keras.layers.BatchNormalization(\n",
    "                    name=f'encoder_bn_{i}'\n",
    "                )\n",
    "                encoder_layers.append(batch_norm)\n",
    "        \n",
    "        # Create latent layer\n",
    "        if self.quant_bits:\n",
    "            latent_layer = QDense(\n",
    "                self.latent_dim,\n",
    "                kernel_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                bias_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                name='latent_layer'\n",
    "            )\n",
    "        else:\n",
    "            latent_layer = keras.layers.Dense(\n",
    "                self.latent_dim,\n",
    "                name='latent_layer'\n",
    "            )\n",
    "        \n",
    "        # Create decoder layers\n",
    "        decoder_layers = []\n",
    "        for i, units in enumerate(self.decoder_layers):\n",
    "            if self.quant_bits:\n",
    "                # Dense layer\n",
    "                dense = QDense(\n",
    "                    units,\n",
    "                    kernel_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                    bias_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                    name=f'decoder_dense_{i}'\n",
    "                )\n",
    "                decoder_layers.append(dense)\n",
    "                \n",
    "                # Activation layer\n",
    "                activation = QActivation(\n",
    "                    quantized_relu(self.quant_bits),\n",
    "                    name=f'decoder_activation_{i}'\n",
    "                )\n",
    "                decoder_layers.append(activation)\n",
    "                \n",
    "                # Batch normalization\n",
    "                batch_norm = keras.layers.BatchNormalization(\n",
    "                    name=f'decoder_bn_{i}'\n",
    "                )\n",
    "                decoder_layers.append(batch_norm)\n",
    "            else:\n",
    "                dense = keras.layers.Dense(\n",
    "                    units,\n",
    "                    name=f'decoder_dense_{i}'\n",
    "                )\n",
    "                decoder_layers.append(dense)\n",
    "                \n",
    "                activation = keras.layers.Activation(\n",
    "                    self.activation,\n",
    "                    name=f'decoder_activation_{i}'\n",
    "                )\n",
    "                decoder_layers.append(activation)\n",
    "                \n",
    "                batch_norm = keras.layers.BatchNormalization(\n",
    "                    name=f'decoder_bn_{i}'\n",
    "                )\n",
    "                decoder_layers.append(batch_norm)\n",
    "        \n",
    "        # Create output layer\n",
    "        if self.quant_bits:\n",
    "            output_layer = QDense(\n",
    "                self.input_dim,\n",
    "                kernel_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                bias_quantizer=quantized_bits(self.quant_bits, 1, alpha=1.0),\n",
    "                name='output_layer'\n",
    "            )\n",
    "        else:\n",
    "            output_layer = keras.layers.Dense(\n",
    "                self.input_dim,\n",
    "                name='output_layer'\n",
    "            )\n",
    "        \n",
    "        # Build the model by applying layers sequentially\n",
    "        # Encoder\n",
    "        x = inputs\n",
    "        for layer in encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Latent space\n",
    "        latent = latent_layer(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = latent\n",
    "        for layer in decoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Output\n",
    "        outputs = output_layer(x)\n",
    "        \n",
    "        # Create model\n",
    "        self.model = keras.Model(inputs=inputs, outputs=outputs, name=self.name)\n",
    "        \n",
    "        print(\"\\nModel layer structure:\")\n",
    "        for layer in self.model.layers:\n",
    "            print(f\"Layer: {layer.name}, Type: {type(layer)}\")\n",
    "        \n",
    "    def get_config(self) -> dict:\n",
    "        return {\n",
    "            \"model_type\": \"autoencoder\",\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"encoder_layers\": self.encoder_layers,\n",
    "            \"decoder_layers\": self.decoder_layers,\n",
    "            \"quant_bits\": self.quant_bits,\n",
    "            \"activation\": self.activation,\n",
    "            \"name\": self.name\n",
    "        }\n",
    "\n",
    "class ModelFactory:\n",
    "    \"\"\"Factory for creating different model types\"\"\"\n",
    "    @staticmethod\n",
    "    def create_model(model_type: str, config: dict) -> BaseModel:\n",
    "        if model_type == \"autoencoder\":\n",
    "            config_copy = config.copy()\n",
    "            config_copy.pop('model_type', None)  # Remove model_type key\n",
    "            return AutoEncoder(\n",
    "                input_dim=config['input_dim'],\n",
    "                latent_dim=config['latent_dim'],\n",
    "                encoder_layers=config['encoder_layers'],\n",
    "                decoder_layers=config['decoder_layers'],\n",
    "                quant_bits=config.get('quant_bits', None),\n",
    "                activation=config.get('activation', 'relu'),\n",
    "                name=config.get('name', 'autoencoder')\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "            \n",
    "    @staticmethod\n",
    "    def from_config(config: dict) -> BaseModel:\n",
    "        \"\"\"Create model from config dictionary\"\"\"\n",
    "        model_type = config.pop(\"model_type\")\n",
    "        return ModelFactory.create_model(model_type, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Handles model training and evaluation\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: BaseModel,\n",
    "        training_config: dict,\n",
    "        optimizer: Optional[tf.keras.optimizers.Optimizer] = None,\n",
    "        loss: Optional[tf.keras.losses.Loss] = None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.config = training_config\n",
    "        \n",
    "        # Set up training parameters\n",
    "        self.batch_size = training_config.get(\"batch_size\", 32)\n",
    "        self.epochs = training_config.get(\"epochs\", 10)\n",
    "        self.validation_split = training_config.get(\"validation_split\", 0.2)\n",
    "        \n",
    "        # Set up optimizer and loss\n",
    "        self.optimizer = optimizer or tf.keras.optimizers.Adam(\n",
    "            learning_rate=training_config.get(\"learning_rate\", 0.001)\n",
    "        )\n",
    "        self.loss = loss or tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        # Training history\n",
    "        self.history = None\n",
    "        \n",
    "    def compile_model(self):\n",
    "        \"\"\"Compile the model with optimizer and loss\"\"\"\n",
    "        if self.model.model is None:\n",
    "            raise ValueError(\"Model not built yet\")\n",
    "            \n",
    "        self.model.model.compile(\n",
    "            optimizer=self.optimizer,\n",
    "            loss=self.loss,\n",
    "            metrics=['mse'],\n",
    "            run_eagerly=True \n",
    "        )\n",
    "        \n",
    "    def train(\n",
    "        self,\n",
    "        dataset: tf.data.Dataset,\n",
    "        validation_data: Optional[tf.data.Dataset] = None,\n",
    "        callbacks: List[tf.keras.callbacks.Callback] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if self.model.model is None:\n",
    "            raise ValueError(\"Model not built yet\")\n",
    "        \n",
    "        print(\"\\nChecking datasets before training:\")\n",
    "        # Check if datasets have any data\n",
    "        try:\n",
    "            print(\"Checking training dataset...\")\n",
    "            for i, batch in enumerate(dataset):\n",
    "                print(f\"Training batch {i} shape: {batch.shape}\")\n",
    "                if i == 0:  # Just check first batch\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking training dataset: {e}\")\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            try:\n",
    "                print(\"\\nChecking validation dataset...\")\n",
    "                for i, batch in enumerate(validation_data):\n",
    "                    print(f\"Validation batch {i} shape: {batch.shape}\")\n",
    "                    if i == 0:  # Just check first batch\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Error checking validation dataset: {e}\")\n",
    "        \n",
    "        # Compile model\n",
    "        self.compile_model()\n",
    "        \n",
    "        # Setup callbacks\n",
    "        if callbacks is None:\n",
    "            callbacks = []\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"\\nStarting model.fit...\")\n",
    "        self.history = self.model.model.fit(\n",
    "            dataset,\n",
    "            epochs=self.epochs,\n",
    "            validation_data=validation_data,\n",
    "            callbacks=callbacks,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        return self.get_training_summary()\n",
    "        \n",
    "    def evaluate(\n",
    "        self,\n",
    "        dataset: tf.data.Dataset\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the model\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        if self.model.model is None:\n",
    "            raise ValueError(\"Model not built yet\")\n",
    "            \n",
    "        results = self.model.model.evaluate(dataset, return_dict=True)\n",
    "        return results\n",
    "        \n",
    "    def get_training_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of training results\"\"\"\n",
    "        if self.history is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "            \n",
    "        return {\n",
    "            \"training_config\": self.config,\n",
    "            \"final_loss\": float(self.history.history['loss'][-1]),\n",
    "            \"final_val_loss\": float(self.history.history['val_loss'][-1]),\n",
    "            \"history\": {\n",
    "                metric: [float(val) for val in values]\n",
    "                for metric, values in self.history.history.items()\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_pipeline():\n",
    "    \"\"\"Test the complete model pipeline including factory, trainer, and registry\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting Model Pipeline Test\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"TensorFlow: {tf.__version__} (Eager: {tf.executing_eagerly()})\")\n",
    "    \n",
    "    try:\n",
    "        # Helper function for JSON serialization\n",
    "        def ensure_serializable(obj):\n",
    "            \"\"\"Recursively convert numpy types to Python native types\"\"\"\n",
    "            if isinstance(obj, dict):\n",
    "                return {key: ensure_serializable(value) for key, value in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [ensure_serializable(item) for item in obj]\n",
    "            elif isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return obj\n",
    "\n",
    "        # 1. Initialize Registry\n",
    "        print(\"Initializing model registry...\")\n",
    "        registry = ModelRegistry(\"experiments\")\n",
    "        \n",
    "        # 2. Setup Data Pipeline\n",
    "        print(\"Setting up data pipeline...\")\n",
    "        data_manager = IntegratedDatasetManager(\n",
    "            run_numbers=[\"00296939\", \"00296942\"],\n",
    "            catalog_limit=1,\n",
    "            batch_size=1000,\n",
    "            cache_size=10000,\n",
    "            shuffle_buffer=10000\n",
    "        )\n",
    "\n",
    "        # Define selections\n",
    "        track_selections = {\n",
    "            'eta': (-2.5, 2.5),\n",
    "            'chi2_per_ndof': (0.0, 3.0),\n",
    "            # 'pt': (1.0, None)  # Minimum pT of 1 GeV\n",
    "        }\n",
    "\n",
    "        event_selections = {\n",
    "            # 'mean_pt': (2.0, None)  # Mean pT > 2 GeV\n",
    "        }\n",
    "\n",
    "        # Create datasets with new selection system\n",
    "        train_dataset, val_dataset, test_dataset = data_manager.create_datasets(\n",
    "            max_tracks_per_event=30,\n",
    "            min_tracks_per_event=3,\n",
    "            track_selections=track_selections,\n",
    "            event_selections=event_selections,\n",
    "            validation_fraction=0.15,\n",
    "            test_fraction=0.15\n",
    "        )\n",
    "\n",
    "        # Update dataset config for registry\n",
    "        dataset_config = ensure_serializable({\n",
    "            \"run_numbers\": data_manager.run_numbers,\n",
    "            \"track_selections\": track_selections,\n",
    "            \"event_selections\": event_selections,\n",
    "            \"max_tracks_per_event\": 30,\n",
    "            \"min_tracks_per_event\": 3,\n",
    "            \"normalization_params\": data_manager.normalization_params,\n",
    "            \"train_fraction\": 0.7,\n",
    "            \"validation_fraction\": 0.15,\n",
    "            \"test_fraction\": 0.15,\n",
    "            \"batch_size\": data_manager.batch_size,\n",
    "            \"shuffle_buffer\": data_manager.shuffle_buffer,\n",
    "            # \"data_quality_metrics\": data_manager.validate_data_quality(train_dataset)\n",
    "        })\n",
    "\n",
    "        # The model configuration also needs to be updated to reflect the fixed input size\n",
    "        model_config_flat = {\n",
    "            \"model_type\": \"autoencoder\",\n",
    "            \"input_dim\": 30 * 6,\n",
    "            \"latent_dim\": 3,\n",
    "            \"encoder_layers\": [64, 32],\n",
    "            \"decoder_layers\": [32, 64],\n",
    "            \"quant_bits\": 8,\n",
    "            \"activation\": \"relu\",\n",
    "            \"name\": \"track_autoencoder\"\n",
    "        }\n",
    "    \n",
    "        \n",
    "        # Model config - nested version for registry\n",
    "        model_config_registry = {\n",
    "            \"model_type\": \"autoencoder\",\n",
    "            \"architecture\": {\n",
    "                \"input_dim\": model_config_flat[\"input_dim\"],\n",
    "                \"latent_dim\": model_config_flat[\"latent_dim\"],\n",
    "                \"encoder_layers\": model_config_flat[\"encoder_layers\"],\n",
    "                \"decoder_layers\": model_config_flat[\"decoder_layers\"]\n",
    "            },\n",
    "            \"hyperparameters\": {\n",
    "                \"activation\": model_config_flat[\"activation\"],\n",
    "                \"quant_bits\": model_config_flat[\"quant_bits\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Training config\n",
    "        training_config = {\n",
    "            \"batch_size\": 1000,\n",
    "            \"epochs\": 50,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"early_stopping\": {\n",
    "                \"patience\": 3,\n",
    "                \"min_delta\": 1e-4\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 4. Register Experiment\n",
    "        print(\"Registering experiment...\")\n",
    "        experiment_id = registry.register_experiment(\n",
    "            name=\"autoencoder_test\",\n",
    "            dataset_config=dataset_config,\n",
    "            model_config=model_config_registry,\n",
    "            training_config=training_config,\n",
    "            description=\"Testing autoencoder on track data with enhanced monitoring\"\n",
    "        )\n",
    "        print(f\"Created experiment: {experiment_id}\")\n",
    "        \n",
    "        # 5. Create and Build Model\n",
    "        print(\"Creating model...\")\n",
    "        try:\n",
    "            model = ModelFactory.create_model(\n",
    "                model_type=\"autoencoder\",\n",
    "                config=model_config_flat\n",
    "            )\n",
    "            model.build(input_shape=(6,))\n",
    "        except Exception as e:\n",
    "            print(f\"Model creation failed: {str(e)}\")\n",
    "            print(f\"Model config used: {json.dumps(model_config_flat, indent=2)}\")\n",
    "            raise\n",
    "        \n",
    "        # 6. Setup Training\n",
    "        print(\"Setting up training...\")\n",
    "        trainer = ModelTrainer(\n",
    "            model=model,\n",
    "            training_config=training_config\n",
    "        )\n",
    "        \n",
    "        # Setup callbacks\n",
    "        class RegistryCallback(tf.keras.callbacks.Callback):\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                logs = ensure_serializable(logs or {})\n",
    "                registry.update_training_progress(\n",
    "                    experiment_id=experiment_id,\n",
    "                    epoch=epoch,\n",
    "                    metrics=logs,\n",
    "                    # hardware_metrics=get_hardware_metrics()\n",
    "                )\n",
    "                \n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                patience=training_config[\"early_stopping\"][\"patience\"],\n",
    "                min_delta=training_config[\"early_stopping\"][\"min_delta\"],\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            RegistryCallback()\n",
    "        ]\n",
    "        \n",
    "        # 7. Train Model\n",
    "        print(\"Starting training...\")\n",
    "        training_start_time = datetime.now()  # Record start time\n",
    "        training_results = trainer.train(\n",
    "            dataset=train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        training_end_time = datetime.now()  # Record end time\n",
    "        training_duration = (training_end_time - training_start_time).total_seconds()\n",
    "        \n",
    "        # 8. Evaluate Model\n",
    "        print(\"Evaluating model...\")\n",
    "        test_results = trainer.evaluate(test_dataset)\n",
    "        \n",
    "        # Record final results\n",
    "        registry.complete_training(\n",
    "            experiment_id=experiment_id,\n",
    "            final_metrics=ensure_serializable({\n",
    "                **training_results,\n",
    "                **test_results,\n",
    "                'test_loss': test_results['loss'],\n",
    "                'training_duration': training_duration\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        # 9. Save Model\n",
    "        print(\"Saving model checkpoint...\")\n",
    "        registry.save_checkpoint(\n",
    "            experiment_id=experiment_id,\n",
    "            models={\"autoencoder\": model.model},\n",
    "            checkpoint_name=\"final\",\n",
    "            metadata=ensure_serializable({\n",
    "                \"test_loss\": test_results['loss'],\n",
    "                \"final_train_loss\": training_results['final_loss']\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        # 10. Display Results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Experiment Results\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        details = registry.get_experiment_details(experiment_id)\n",
    "        performance = registry.get_performance_summary(experiment_id)\n",
    "\n",
    "        print(f\"\\nExperiment ID: {experiment_id}\")\n",
    "        print(f\"Status: {details['experiment_info']['status']}\")\n",
    "\n",
    "        # Handle potential None values for duration\n",
    "        duration = performance.get('training_duration')\n",
    "        if duration is not None:\n",
    "            print(f\"Training Duration: {duration:.2f}s\")\n",
    "        else:\n",
    "            print(\"Training Duration: Not available\")\n",
    "\n",
    "        print(f\"Epochs Completed: {performance['epochs_completed']}\")\n",
    "\n",
    "        print(\"\\nMetrics:\")\n",
    "        def print_metrics(metrics, indent=2):\n",
    "            \"\"\"Helper function to print metrics with proper formatting\"\"\"\n",
    "            for key, value in metrics.items():\n",
    "                indent_str = \" \" * indent\n",
    "                if isinstance(value, dict):\n",
    "                    print(f\"{indent_str}{key}:\")\n",
    "                    print_metrics(value, indent + 2)\n",
    "                elif isinstance(value, (float, int)):\n",
    "                    print(f\"{indent_str}{key}: {value:.6f}\")\n",
    "                else:\n",
    "                    print(f\"{indent_str}{key}: {value}\")\n",
    "\n",
    "        # Print metrics using the helper function\n",
    "        print_metrics(performance['final_metrics'])\n",
    "        \n",
    "        # 11. Visualize Results\n",
    "        if True:  # Change to control visualization\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            history = performance['metric_progression']\n",
    "            plt.plot(history['loss'], label='Training Loss')\n",
    "            plt.plot(history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training History')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"Pipeline test completed successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline test failed: {type(e).__name__}: {str(e)}\")\n",
    "        print(f\"Error context:\")\n",
    "        raise\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    \"\"\"\n",
    "    Get GPU memory usage as a percentage.\n",
    "    Returns a dictionary with memory usage for each GPU.\n",
    "    \n",
    "    Returns:\n",
    "        Dict: GPU index to memory usage percentage\n",
    "        or None if no GPU is available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First try using TensorFlow's device API\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if not gpus:\n",
    "            return None\n",
    "            \n",
    "        memory_usage = {}\n",
    "        for gpu_id, gpu in enumerate(gpus):\n",
    "            # Get memory info for this GPU\n",
    "            memory_info = tf.config.experimental.get_memory_info(f'GPU:{gpu_id}')\n",
    "            \n",
    "            # Calculate percentage (note: this might not be available on all systems)\n",
    "            if hasattr(memory_info, 'peak') and hasattr(memory_info, 'total'):\n",
    "                memory_usage[f'gpu_{gpu_id}'] = (memory_info.peak / memory_info.total) * 100\n",
    "                \n",
    "        return memory_usage if memory_usage else None\n",
    "        \n",
    "    except:\n",
    "        try:\n",
    "            # Fallback to nvidia-smi command\n",
    "            import subprocess\n",
    "            import re\n",
    "            \n",
    "            output = subprocess.check_output(\n",
    "                ['nvidia-smi', '--query-gpu=index,memory.used,memory.total', '--format=csv,nounits,noheader'],\n",
    "                encoding='utf-8'\n",
    "            )\n",
    "            \n",
    "            memory_usage = {}\n",
    "            for line in output.strip().split('\\n'):\n",
    "                gpu_id, memory_used, memory_total = map(int, line.split(','))\n",
    "                memory_percentage = (memory_used / memory_total) * 100\n",
    "                memory_usage[f'gpu_{gpu_id}'] = memory_percentage\n",
    "                \n",
    "            return memory_usage\n",
    "            \n",
    "        except:\n",
    "            # If both methods fail, return None\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Starting Model Pipeline Test\n",
      "==================================================\n",
      "TensorFlow: 2.13.1 (Eager: True)\n",
      "Initializing model registry...\n",
      "Setting up data pipeline...\n",
      "\n",
      "Initial system state:\n",
      "\n",
      "Pre-processing: System Usage:\n",
      "Memory: 90.5GB / 376.2GB (26.7%)\n",
      "Available Memory: 275.8GB\n",
      "CPU Usage: 23.4%\n",
      "Creating base dataset...\n",
      "\n",
      "Creating streaming dataset\n",
      "Normalizing datatset\n",
      "\n",
      "Computing normalization parameters from sample...\n",
      "\n",
      "Processing run 00296939\n",
      "Processing catalog 0\n",
      "Not cleaning up catalog files for faster testing\n",
      "Processing catalog 1\n",
      "Not cleaning up catalog files for faster testing\n",
      "\n",
      "Processing run 00296942\n",
      "Processing catalog 0\n",
      "Not cleaning up catalog files for faster testing\n",
      "Processing catalog 1\n",
      "Not cleaning up catalog files for faster testing\n",
      "\n",
      "System usage after dataset creation:\n",
      "\n",
      "Post-processing: System Usage:\n",
      "Memory: 90.6GB / 376.2GB (26.7%)\n",
      "Available Memory: 275.8GB\n",
      "CPU Usage: 23.8%\n",
      "Checking base dataset...\n",
      "\n",
      "Processing run 00296939\n",
      "Processing catalog 0\n",
      "Not cleaning up catalog files for faster testing\n",
      "Processing catalog 1\n",
      "Not cleaning up catalog files for faster testing\n",
      "\n",
      "Processing run 00296942\n",
      "Processing catalog 0\n",
      "Not cleaning up catalog files for faster testing\n",
      "Processing catalog 1\n",
      "Not cleaning up catalog files for faster testing\n",
      "\n",
      "Splitting datasets...\n",
      "Created test dataset\n",
      "Created remaining dataset\n",
      "Created validation dataset\n",
      "Created training dataset\n",
      "\n",
      "Caching datasets...\n",
      "\n",
      "Forcing cache load...\n",
      "Loading train cache...\n",
      "Loading validation cache...\n",
      "Loading test cache...\n",
      "\n",
      "Final system state after caching:\n",
      "\n",
      "Post-caching: System Usage:\n",
      "Memory: 90.6GB / 376.2GB (26.7%)\n",
      "Available Memory: 275.7GB\n",
      "CPU Usage: 27.0%\n",
      "\n",
      "Dataset splits:\n",
      "Training batches: 70\n",
      "Validation batches: 15\n",
      "Test batches: 15\n",
      "Registering experiment...\n",
      "Created experiment: 7334f363-8da7-4600-a273-67ab7061512a\n",
      "Creating model...\n",
      "\n",
      "Model layer structure:\n",
      "Layer: input_layer, Type: <class 'keras.src.engine.input_layer.InputLayer'>\n",
      "Layer: encoder_dense_0, Type: <class 'qkeras.qlayers.QDense'>\n",
      "Layer: encoder_activation_0, Type: <class 'qkeras.qlayers.QActivation'>\n",
      "Layer: encoder_bn_0, Type: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: encoder_dense_1, Type: <class 'qkeras.qlayers.QDense'>\n",
      "Layer: encoder_activation_1, Type: <class 'qkeras.qlayers.QActivation'>\n",
      "Layer: encoder_bn_1, Type: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: latent_layer, Type: <class 'qkeras.qlayers.QDense'>\n",
      "Layer: decoder_dense_0, Type: <class 'qkeras.qlayers.QDense'>\n",
      "Layer: decoder_activation_0, Type: <class 'qkeras.qlayers.QActivation'>\n",
      "Layer: decoder_bn_0, Type: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: decoder_dense_1, Type: <class 'qkeras.qlayers.QDense'>\n",
      "Layer: decoder_activation_1, Type: <class 'qkeras.qlayers.QActivation'>\n",
      "Layer: decoder_bn_1, Type: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: output_layer, Type: <class 'qkeras.qlayers.QDense'>\n",
      "Setting up training...\n",
      "Starting training...\n",
      "\n",
      "Checking datasets before training:\n",
      "Checking training dataset...\n",
      "\n",
      "Checking validation dataset...\n",
      "\n",
      "Starting model.fit...\n",
      "Epoch 1/50\n",
      "Pipeline test failed: ValueError: Unexpected result of `train_function` (Empty logs). This could be due to issues in input pipeline that resulted in an empty dataset. Otherwise, please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.\n",
      "Error context:\n",
      "\n",
      "Test failed with error:\n",
      "Unexpected result of `train_function` (Empty logs). This could be due to issues in input pipeline that resulted in an empty dataset. Otherwise, please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.\n"
     ]
    }
   ],
   "source": [
    "# Modify code to specify which catalogs to run for each run\n",
    "# Put each catalog code in a try statement and if problem continue to next catalog\n",
    "# Note: processing 65 total catalogs over two runs took 106 minutes, about 2 minutes per catalog\n",
    "\n",
    "\n",
    "try:\n",
    "    success = test_model_pipeline()\n",
    "    if success:\n",
    "        print(\"\\nAll tests passed successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"\\nTest failed with error:\")\n",
    "    print(str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hep_foundation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
