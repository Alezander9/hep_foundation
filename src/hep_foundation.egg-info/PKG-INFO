Metadata-Version: 2.4
Name: hep-foundation
Version: 0.1.0
Summary: A python package for training foundation models on high energy physics data
Home-page: https://github.com/yourusername/hep_foundation
Author: Your Name
Author-email: Alexander Yue <alexyue@stanford.edu>
Project-URL: Homepage, https://github.com/Alezander9/hep-foundation
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Requires-Dist: numpy==1.24.3
Requires-Dist: psutil>=7.0.0
Requires-Dist: pyyaml>=6.0.2
Requires-Dist: qkeras>=0.9.0
Requires-Dist: requests>=2.32.3
Requires-Dist: seaborn>=0.13.2
Requires-Dist: tensorflow==2.13.1
Requires-Dist: tqdm>=4.67.1
Requires-Dist: uproot>=5.6.0
Provides-Extra: dev
Requires-Dist: ruff<0.12.0,>=0.11.6; extra == "dev"
Requires-Dist: pytest>=8.3.5; extra == "dev"
Requires-Dist: pytest-cov>=6.1.1; extra == "dev"
Dynamic: author
Dynamic: home-page
Dynamic: requires-python

# HEP Foundation

ML tools for High Energy Physics analysis, focusing on ATLAS PHYSLITE data processing and autoencoder training.

## Quick Start

```bash
# Clone repository
git clone https://github.com/Alezander9/hep_foundation
cd hep_foundation

# Create and activate virtual environment using uv
# Requires uv to be installed (e.g., pip install uv or via package manager)
uv venv --python 3.9  # Specify your desired Python version
source .venv/bin/activate # On Windows: .venv\\Scripts\\activate

# Install package in editable mode
uv pip install -e .
```

## Usage

### Basic Pipeline
```python
from hep_foundation.model_factory import ModelFactory
from hep_foundation.dataset_manager import DatasetManager

# Setup data pipeline
data_manager = DatasetManager()
train_dataset, val_dataset, test_dataset = data_manager.load_atlas_datasets(
config={
'run_numbers': ["00296939", "00296942", "00297447"],
'track_selections': {
'eta': (-2.5, 2.5),
'chi2_per_ndof': (0.0, 10.0),
},
'max_tracks_per_event': 20,
'min_tracks_per_event': 3,
'catalog_limit': 5
}
)
# Create and train model
model = ModelFactory.create_model(
model_type="autoencoder",
config={
'input_shape': (20, 6),
'latent_dim': 32,
'encoder_layers': [128, 64, 32],
'decoder_layers': [32, 64, 128],
'activation': 'relu'
}
)
```
For full training pipeline example, see scripts/model_pipeline.py
```bash
Run full pipeline test
python scripts/model_pipeline.py
```

## Project Structure
- `src/hep_foundation/`: Core package code
- `scripts/`: Example scripts and tests
- `experiments/`: Output directory for model registry and results
- `processed_datasets/`: Storage for processed datasets

## Dependencies
Core requirements are automatically handled by `uv pip install -e .` using `pyproject.toml`.

For development dependencies (like linters, testing tools), ensure they are listed under `[project.optional-dependencies]` in `pyproject.toml` (e.g., in a group named `dev`) and install them using:

```bash
uv pip install -e ".[dev]"
```

To add a new runtime dependency:
```bash
uv add <package_name>  # e.g., uv add numpy==1.24.3
```

To add a new development dependency:
```bash
uv add --dev <package_name> # e.g., uv add --dev pytest
```

## Code Quality

This project uses [Ruff](https://docs.astral.sh/ruff/) for linting and formatting.

- **Check Code**: To check for linting errors and style issues, run:
  ```bash
  uv run ruff check .
  ```
- **Format Code**: To automatically format the code according to the project's style guide, run:
  ```bash
  uv run ruff format .
  ```

It's recommended to run these commands after making significant changes and before committing code.

## Data Access
The package automatically handles ATLAS PHYSLITE data download from CERN OpenData.

## Questions?
Contact: alexyue@stanford.edu
