# Realistic Pipeline Config Configuration
# This file demonstrates the structure for _experiment_config_stack configurations
# Copy this file and modify the parameters for your specific experiments

name: "realistic_foundation_vae"
description: "Realistic configuration for foundation model pipeline"
version: "1.0"
created_by: "Alex"

# Dataset Configuration
dataset:
  run_numbers: !python get_run_numbers()[-2:]  # ATLAS run numbers to process
  signal_keys: ["zprime_tt", "wprime_taunu", "DMA", "leptoquark", "dm_4top_scalar"]  # Signal types for anomaly detection
  catalog_limit: 10  # Number of catalogs per run (null for all available)
  validation_fraction: 0.15
  test_fraction: 0.15
  shuffle_buffer: 50000
  plot_distributions: true  # Generate feature distribution plots
  include_labels: true

# Task Configuration - defines input features and labels
task:
  event_filters: {}  # Additional event-level filters

  input_features: []  # Scalar input features (empty for array-based approach)

  # Array-based input features (tracks, jets, etc.)
  input_array_aggregators:
    - input_branches:
        - "InDetTrackParticlesAuxDyn.d0"
        - "InDetTrackParticlesAuxDyn.z0"
        - "InDetTrackParticlesAuxDyn.phi"
        - "derived.InDetTrackParticlesAuxDyn.eta"
        - "derived.InDetTrackParticlesAuxDyn.pt"
        - "derived.InDetTrackParticlesAuxDyn.reducedChiSquared"
      filter_branches:
        - {"branch": "InDetTrackParticlesAuxDyn.d0", "min": -5.0, "max": 5.0}
        - {"branch": "InDetTrackParticlesAuxDyn.z0", "min": -100.0, "max": 100.0}
      sort_by_branch: {"branch": "InDetTrackParticlesAuxDyn.qOverP"}
      min_length: 10
      max_length: 30

  label_features:
    - []  # Scalar label features (empty for array-based approach)

  # Array-based label features (for regression tasks)
  label_array_aggregators:
    - - input_branches:
          - "MET_Core_AnalysisMETAuxDyn.sumet"
        filter_branches: []
        sort_by_branch: null
        min_length: 1
        max_length: 1

# Model Configurations
models:
  # Variational Autoencoder (Foundation Model)
  vae:
    model_type: "variational_autoencoder"
    architecture:
      latent_dim: 32  # Size of the latent space
      encoder_layers: [128, 128, 64, 48]  # Hidden layer sizes for encoder
      decoder_layers: [48, 64, 128, 128]  # Hidden layer sizes for decoder
      activation: "relu"
      name: "foundation_vae"
    hyperparameters:
      beta_schedule:  # Beta annealing schedule for VAE training
        start: 0.001
        warmup: 40
        cycle_low: 0.01
        cycle_high: 0.05
        cycle_period: 20

  # DNN Predictor (for regression evaluation)
  dnn:
    model_type: "dnn_predictor"
    architecture:
      hidden_layers: [16]  # Hidden layer sizes
      label_index: 0  # Which label set to use (0-indexed)
      activation: "relu"
      output_activation: "linear"
      name: "met_predictor"
    hyperparameters:
      quant_bits: 8
      dropout_rate: 0.2
      l2_regularization: 0.0001  # 1e-4

# Training Configurations
training:
  # VAE Training
  vae:
    batch_size: 1024
    learning_rate: 0.001
    epochs: 200  # Number of training epochs
    early_stopping:
      patience: 200
      min_delta: 0.0001
    plot_training: true  # Generate training plots

  # DNN Training (for regression evaluation)
  dnn:
    batch_size: 1024
    learning_rate: 0.001
    epochs: 0 # This gets overridden by the data size
    early_stopping:
      patience: 10 # This shouldnt be overridden by the data size, but doesnt seem to be working
      min_delta: 0.0001
    plot_training: true

# Evaluation Settings
evaluation:
  # Data sizes for regression data efficiency study
  regression_data_sizes: [1000, 5000, 10000, 50000]
  fixed_epochs: 50  # Fixed epochs for fair comparison in regression evaluation
  anomaly_eval_batch_size: 1024  # Batch size for anomaly detection evaluation

# Pipeline Settings
pipeline:
  delete_catalogs: false  # Delete intermediate catalog files after processing
  experiments_output_dir: "_foundation_experiments"  # Where to save results
  processed_data_parent_dir: null  # Parent dir for _processed_datasets (null = workspace root)
